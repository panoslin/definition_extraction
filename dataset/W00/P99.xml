<?xml version="1.0" encoding="UTF-8"?>
	<volume id="P99">

		<paper id="1049">
			<definition id="0">
				<sentence>The SA tags represent a speaker 's intention in an utterance , and is more or less similar to the traditional illocutionary force type ( Searle , 1969 ) .</sentence>
				<definiendum id="0">SA tags</definiendum>
				<definiens id="0">represent a speaker 's intention in an utterance , and is more or less similar to the traditional illocutionary force type</definiens>
			</definition>
			<definition id="1">
				<sentence>This probability is estimated by a probabilistic decision tree and we have P ( Bw= , Wx+ , I hi , W ) ~P ( Bw ... . +1 I eE ( hj , W ) ) , where riPE is a decision tree that categorizes hj , W into equivalent classes ( Jelinek , 1997 ) .</sentence>
				<definiendum id="0">riPE</definiendum>
				<definiens id="0">estimated by a probabilistic decision tree</definiens>
				<definiens id="1">a decision tree that categorizes hj</definiens>
			</definition>
			<definition id="2">
				<sentence>The function g ( uj ) extracts cue words for the SA tags from uj using a cue word list .</sentence>
				<definiendum id="0">function g</definiendum>
				<definiens id="0">extracts cue words for the SA tags from uj using a cue word list</definiens>
			</definition>
			<definition id="3">
				<sentence>Recall Precision A B C A B C B 2.84 B 1.25 C 2.71 0.12 C 0.83 0.44 D 2.57 0.28 0.17 D 0.74 0.39 0.01 Table 3 : Average accuracy for segmentation match .</sentence>
				<definiendum id="0">Recall Precision A B C A B</definiendum>
				<definiens id="0">Average accuracy for segmentation match</definiens>
			</definition>
			<definition id="4">
				<sentence>( I ) Representative form of the positive response ( J ) SA tag for the positive response ( K ) SA tag for the SA unit previous to the positive response ( L ) Speaker ( Hotel/Clerk ) 386 Table 7 : Representation forms and the counts .</sentence>
				<definiendum id="0">K ) SA tag</definiendum>
				<definiens id="0">Representative form of the positive response ( J ) SA tag for the positive response</definiens>
			</definition>
			<definition id="5">
				<sentence>The result gives us a strange impression in that the SA tags for the previous SA units ( K ) were far more effective than the SA tags for the positive responses themselves ( J ) .</sentence>
				<definiendum id="0">SA tags</definiendum>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>pxAqx 301 ( T is a variable over categories unique to each individual occurrence of the raised categories ( 3 ) and ( 4 ) , abbreviating a finite number of different raised types .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">a variable over categories unique to each individual occurrence of the raised categories</definiens>
			</definition>
			<definition id="1">
				<sentence>An arbitrary object is an object with which properties can be associated but whose extensional identity in terms of actual objects is unspecified .</sentence>
				<definiendum id="0">arbitrary object</definiendum>
				<definiens id="0">an object with which properties can be associated but whose extensional identity in terms of actual objects is unspecified</definiens>
			</definition>
			<definition id="2">
				<sentence>Call it SkdonkeyX in this case , where Skdonkey maps individual instantiations of x-that is , the variable bound by the generalized quantifier every farmer -- -onto objects with the property donkey in the database .</sentence>
				<definiendum id="0">Skdonkey</definiendum>
				<definiens id="0">maps individual instantiations of x-that is , the variable bound by the generalized quantifier every farmer -- -onto objects with the property donkey in the database</definiens>
			</definition>
			<definition id="3">
				<sentence>q ( arb'p ) b. some : = ( T\ ( T/NP ) ) /N : ~ , pS~q.q ( arb'p ) In this pair of categories , the constant arb ' is the function identified earlier from properties p to entities of type e with that property , such that those entities are functionally related to any universally quantified NPs that have scope over them at the level of logical form .</sentence>
				<definiendum id="0">q</definiendum>
				<definiens id="0">the function identified earlier from properties p to entities of type e with that property , such that those entities are functionally related to any universally quantified NPs that have scope over them at the level of logical form</definiens>
			</definition>
			<definition id="4">
				<sentence>The fact that relatively few readings are available and that they are so tightly related to surface structure and derivation means that the technique of incremental semantic or probabilistic disambiguation of fully specified partial logical forms mentioned earlier may be a more efficient technique for computing the contextually relevant readings .</sentence>
				<definiendum id="0">derivation</definiendum>
				<definiens id="0">the technique of incremental semantic or probabilistic disambiguation of fully specified partial logical forms mentioned earlier may be a more efficient technique for computing the contextually relevant readings</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>TAG keeps dependencies together , or local , no matter how far apart the corresponding lexicM items are .</sentence>
				<definiendum id="0">TAG</definiendum>
			</definition>
			<definition id="1">
				<sentence>The idea behind S-TAG is to take two TAGs and link them in an appropriate way so that when substitution or adjunction occurs in a tree in one grammar , then a corresponding composition operation occurs in a tree in the other grammar .</sentence>
				<definiendum id="0">S-TAG</definiendum>
				<definiens id="0">to take two TAGs and link them in an appropriate way so that when substitution or adjunction occurs in a tree in one grammar</definiens>
			</definition>
			<definition id="2">
				<sentence>The tree pair a0 is the one that captures the syntactic rearrangement in this paraphrase ; such a tree pair will be termed the STRUCTURAL MAPPING PAIR ( SMP ) .</sentence>
				<definiendum id="0">STRUCTURAL MAPPING PAIR</definiendum>
				<definiens id="0">the one that captures the syntactic rearrangement in this paraphrase</definiens>
			</definition>
			<definition id="3">
				<sentence>A formalism ~-i in the progression is defined by applying the TAG yield function to a derivation tree defined by a grammar formalism ~16 ; cmx0Axl ~NXdxN ~Vvx /~sPUs I I c~DXD aNXdxN c~NXdxN c~NXdxN $ cqT : aNXdxN I aDXD aNXdxN c~DXD ~N0nx0Vnxl ~COMPs c~NXdxN , Figure 10 : Meta-grammar for ( 5b ) 0t14 ~15 a17 ~18/ Figure 11 : Derivation tree pair for Fig 3 5~i_1 ; the generative capacity of ~i is a superset of ~'i-1Thus using a TAG meta-grammar , as described in Section 4 , would suggest that the generative capacity of the object-level formalism would necessarily have been increased over that of TAG .</sentence>
				<definiendum id="0">formalism ~-i</definiendum>
				<definiens id="0">Meta-grammar for ( 5b ) 0t14 ~15 a17 ~18/ Figure 11 : Derivation tree pair for Fig 3 5~i_1 ; the generative capacity of ~i is a superset of ~'i-1Thus using a TAG meta-grammar , as described in Section 4 , would suggest that the generative capacity of the object-level formalism would necessarily have been increased over that of TAG</definiens>
			</definition>
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>The latent variable represents the true state of the object , and is the source of the correlations among the observed variables .</sentence>
				<definiendum id="0">latent variable</definiendum>
				<definiens id="0">represents the true state of the object , and is the source of the correlations among the observed variables</definiens>
			</definition>
			<definition id="1">
				<sentence>Judges M , D , and J participate in interactive discussions centered around the differences .</sentence>
				<definiendum id="0">J</definiendum>
				<definiens id="0">participate in interactive discussions centered around the differences</definiens>
			</definition>
			<definition id="2">
				<sentence>Judge M is an undergraduate computer science student , and judge D has no background in computer science or linguistics .</sentence>
				<definiendum id="0">Judge M</definiendum>
				<definiens id="0">an undergraduate computer science student , and judge D has no background in computer science or linguistics</definiens>
			</definition>
			<definition id="3">
				<sentence>The Kappa value for the agreement between D and M considering all certainty ratings reaches .76 , which allows tentative conclusions on Krippendorf 's scale ( 1980 ) .</sentence>
				<definiendum id="0">Kappa value</definiendum>
			</definition>
			<definition id="4">
				<sentence>The data consists of the concatenation of the two corpora annotated with bias-corrected tags as described above .</sentence>
				<definiendum id="0">data</definiendum>
				<definiens id="0">consists of the concatenation of the two corpora annotated with bias-corrected tags as described above</definiens>
			</definition>
</paper>

		<paper id="1074">
			<definition id="0">
				<sentence>CMU 's Phoenix system is implemented as a recursive transition network ( RTN ) .</sentence>
				<definiendum id="0">RTN</definiendum>
				<definiens id="0">a recursive transition network</definiens>
			</definition>
			<definition id="1">
				<sentence>Phoenix performs a depth-first search over its textual input , while Abney 's `` chunking '' and `` attaching '' parsers perform best-first searches ( 1991 ) .</sentence>
				<definiendum id="0">Phoenix</definiendum>
			</definition>
			<definition id="2">
				<sentence>Indeed , the CSLU toolkit provides a generic recognizer , which accepts a set of vocabulary and word sequences defined by a regular grammar on a perstate basis .</sentence>
				<definiendum id="0">CSLU toolkit</definiendum>
				<definiens id="0">provides a generic recognizer , which accepts a set of vocabulary and word sequences defined by a regular grammar on a perstate basis</definiens>
			</definition>
			<definition id="3">
				<sentence>This suggests to us that there must be two levels of tokens : small , quickly manipulated tokens at the acoustic level ( i.e. , lexical level ) , and larger , less-frequently used tokens at the structural level ( i.e. , syntactic , semantic , pragmatic level ) .</sentence>
				<definiendum id="0">acoustic level</definiendum>
				<definiens id="0">lexical level ) , and larger , less-frequently used tokens at the structural level ( i.e. , syntactic , semantic , pragmatic level )</definiens>
			</definition>
</paper>

		<paper id="1078">
			<definition id="0">
				<sentence>Informative Selection : this process aims to confirm which of the potential topics computed by the indicative selection are actual topics ( i.e. topics the system could informatively expand according to the reader interest ) and produces a pool of `` propositions '' elaborating the topics .</sentence>
				<definiendum id="0">Informative Selection</definiendum>
				<definiens id="0">this process aims to confirm which of the potential topics computed by the indicative selection are actual topics</definiens>
			</definition>
			<definition id="1">
				<sentence>The instantiated informative templates constitute the informative data base and the potential topics appearing in the informative templates form the topics of the document .</sentence>
				<definiendum id="0">instantiated informative templates</definiendum>
				<definiens id="0">the informative data base and the potential topics appearing in the informative templates form the topics of the document</definiens>
			</definition>
			<definition id="2">
				<sentence>EC'I'~D TOPICS t INPORMATIVE ABSTRACT Figure 3 : System Architecture Templates and Instantiated Slots Topic ol the document template Entity description template Main predicate : `` describes '' : DESCRIBE Where : nil Who : `` This paper '' : PAPER What : `` the Active Telepresence System with an integrated AR system to enhance the operator 's sense of presence in hazardous environments '' `` Position : Number 1 from `` Conclusion '' Section Topic candidates : `` the Active Telepresence System '' , `` an integrated AR system '' , `` the operator 's sense '' , `` presence '' , `` hazardous environments '' Weight : ... Main predicate : `` consist of '' : CONSIST OF Topical entity : `` The Active Telepresence System '' Related entities : `` three distinct elements '' , `` the stereo head '' , `` its controller '' , `` the display device '' Position : Number 4 from `` The Active Telepresence System '' Section Weight : ... Figure 4 : Some Instantiated Templates for the article `` Augmenting reality for telerobotics : unifying real and virtual worlds '' J. Pretlove , Industrial Robot , voi.25 , issue 6 , 1998 .</sentence>
				<definiendum id="0">EC'I'~D TOPICS</definiendum>
				<definiens id="0">unifying real and virtual worlds '' J.</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>W ( , assume familiarity with theories of feature structure based unification grammars , as formulated by , e.g. , Carpenter ( 1992 ) or Shieber ( 1992 ) .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">assume familiarity with theories of feature structure based unification grammars , as formulated by</definiens>
			</definition>
			<definition id="1">
				<sentence>A multi-rooted structure ( MRS , see Sikkel ( 1997 ) ( ) r Wintner and Francez ( 1999 ) ) is a sequence of TFSs , with possible reentrancies among diffi ; rent elements in the sequence .</sentence>
				<definiendum id="0">multi-rooted structure ( MRS</definiendum>
				<definiens id="0">a sequence of TFSs , with possible reentrancies among diffi ; rent elements in the sequence</definiens>
			</definition>
			<definition id="2">
				<sentence>If a function T is monotone it has a least fixpoint ( Tarski-Knaster theorem ) ; if T is also continuous , the fixpoint can be obtained by iterative application of T to the empty set ( Kleene theorem ) : lfp ( T ) = TSw , where TI '' 0 = 0 and T t n = T ( T t ( n1 ) ) when 'n is a successor ordinal and ( _Jk &lt; n ( T i '' n ) when n is a limit ordinal. When the semantics of programming languages are concerned , a notion of observables is called for : Ob is a flmction associating a set of objects , the observables , with every program. The choice of semantics induces a natural equivalence operator on grammars : given a semantics 'H ' , G1 ~ G2 iff ~GI~ = ~G2~. An essential requirement of any semantic equivalence is that it 97 ' be correct ( observables-preserving ) : if G1 -G2 , then Ob ( G1 ) = Ob ( G2 ) . Let 'U ' be a composition operation on grammars and '• ' a combination operator on denorations. A ( correct ) semantics 'H ' is compo.s'itional ( Gaifinan and Shapiro , 1989 ) if whenever ~1~ : ~G2~ and ~G3\ ] -~G4\ ] , also ~G , U G3~ = \ [ G2 U G4\ ] . A semantics is commutative ( Brogi et al. , 1992 ) if ~G1 UG2\ ] = ~G , ~ • \ [ G2~. This is a stronger notion than ( : ompositionality : if a semantics is commutative with respect to some operator then it is compositional. As Van Emden and Kowalski ( 1976 ) note , `` to define an operational semantics for a programruing language is to define an implementational independent interpreter for it. For predicate logic the proof procedure behaves as such an interpreter. '' Shieber et al. ( 1995 ) view parsing as a. deductive process that proves claims about the grammatical status of strings from assumptions derived from the grammar. We follow their insight and notation and list a deductive system for parsing unification-based grammars. Definition 3. The deductive parsing system associated with a grammar G = ( 7~ , F. , AS } is defined over ITEMS and is characterized by : Axioms : \ [ a , i , A , i + 1\ ] i.f B E Z. ( a ) and B K A ; \ [ e , i , A , i\ ] if B is an e-rule in T~ and B K_ A Goals : \ [ w , 0 , A , \ [ w\ ] \ ] where A ~ A s Inference rules : \ [ wx , i l , A1 , ill , ... , \ [ Wk , ik , Ak , Jk \ ] \ [ Wl `` `` `` Wk , i , A , j\ ] if .'h = i1 , +1 .for 1 &lt; _ l &lt; k and i = il and J = Jk and ( A1 , ... , Ak ) = &gt; a A When an item \ [ w , i , A , j\ ] can be deduced , applying k times the inference rules associz~ted with a grammar G , we write F-~\ [ w , i , A , j\ ] .</sentence>
				<definiendum id="0">'n</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">Ob</definiendum>
				<definiendum id="3">semantics</definiendum>
				<definiens id="0">a flmction associating a set of objects</definiens>
				<definiens id="1">a natural equivalence operator on grammars : given a semantics 'H '</definiens>
				<definiens id="2">a. deductive process that proves claims about the grammatical status of strings from assumptions derived from the grammar. We follow their insight and notation</definiens>
			</definition>
			<definition id="3">
				<sentence>The only difference between GUG1 and GUG2 is the presence of the rule ( cat : up ) -+ ( cat : up ) in the former .</sentence>
				<definiendum id="0">GUG2</definiendum>
				<definiens id="0">the presence of the rule</definiens>
			</definition>
			<definition id="4">
				<sentence>Ta ( I ) U I. In other words , the semantics is Ta + Id , where Id is the identity operator .</sentence>
				<definiendum id="0">Id</definiendum>
				<definiens id="0">the identity operator</definiens>
			</definition>
			<definition id="5">
				<sentence>but Ob ( G1 ) 7 ~ Ob ( G2 ) ( due to different lexicons ) , that is , the semantics would not be correct .</sentence>
				<definiendum id="0">Ob</definiendum>
				<definiens id="0">due to different lexicons</definiens>
			</definition>
</paper>

		<paper id="1067">
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>Our model , MBMA ( Memory-Based Morphological Analysis ) , is a memory-based learning system ( Stanfill and Waltz , 1986 ; Daelemans et al. , 1997 ) .</sentence>
				<definiendum id="0">MBMA</definiendum>
			</definition>
			<definition id="1">
				<sentence>Memory-based learning is a class of inductive , supervised machine learning algorithms that learn by storing examples of a task in memory .</sentence>
				<definiendum id="0">Memory-based learning</definiendum>
				<definiens id="0">a class of inductive , supervised machine learning algorithms that learn by storing examples of a task in memory</definiens>
			</definition>
			<definition id="2">
				<sentence>Previous approaches to Dutch morphological analysis have been based on finite-state transducers ( e.g. , XEROX'es morphological analyzer ) , or on parsing with context-free word grammars interleaved with exploration of possible spelling changes ( e.g. Heemskerk and van Heuven ( 1993 ) ; or see Heemskerk ( 1993 ) for a probabilistic variant ) .</sentence>
				<definiendum id="0">probabilistic variant</definiendum>
				<definiens id="0">XEROX'es morphological analyzer ) , or on parsing with context-free word grammars interleaved with exploration of possible spelling changes</definiens>
			</definition>
			<definition id="3">
				<sentence>The ( most frequently occurring ) classification of the memory instance Y with the smallest A ( X , Y ) is then taken as the classification of X. The weighting function W ( fi ) computes for each feature , over the full instance base , its information gain , a function from information theory ; cf. Quinlan ( 1986 ) .</sentence>
				<definiendum id="0">cf. Quinlan</definiendum>
				<definiens id="0">a function from information theory</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision is the percentage of morphemes predicted by MBMA that is actually a morpheme in the target analysis ; recall is the percentage of morphemes in the target analysis that are also predicted by MBMA .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">the percentage of morphemes predicted by MBMA that is actually a morpheme in the target analysis ;</definiens>
				<definiens id="1">the percentage of morphemes in the target analysis that are also predicted by MBMA</definiens>
			</definition>
</paper>

		<paper id="1047">
			<definition id="0">
				<sentence>The cluster named EVALUATIONINTERPRETATION contained the rhetorical relations of EVALUATION and INTERPRETATION .</sentence>
				<definiendum id="0">EVALUATIONINTERPRETATION</definiendum>
				<definiens id="0">contained the rhetorical relations of EVALUATION and INTERPRETATION</definiens>
			</definition>
			<definition id="1">
				<sentence>The local context consists of a window of size 5 that enumerates the Part-Of-Speech ( POS ) tags of the lexeme under scrutiny and the two lexemes found immediately before and after it .</sentence>
				<definiendum id="0">local context</definiendum>
				<definiens id="0">consists of a window of size 5 that enumerates the Part-Of-Speech ( POS ) tags of the lexeme under scrutiny and the two lexemes found immediately before and after it</definiens>
			</definition>
			<definition id="2">
				<sentence>Ace Action ( a ) ( b ) ( c ) ( d ) ( e ) sentence-break ( a ) 272 4 edu-break ( b ) 133 3 84 start-parcH ( c ) 4 26 end-paten ( d ) 20 6 none ( e ) 2 38 1 4 7555 Table 2 : Confusion matrix for the decision-tree , non-binary classifier ( the Brown corpus ) .</sentence>
				<definiendum id="0">Ace Action</definiendum>
				<definiens id="0">Confusion matrix for the decision-tree , non-binary classifier ( the Brown corpus )</definiens>
			</definition>
			<definition id="3">
				<sentence>The shift-reduce action identifier uses the C4.5 program in order to learn decision trees and rules that specify how discourse segments should be assembled into trees .</sentence>
				<definiendum id="0">shift-reduce action identifier</definiendum>
			</definition>
			<definition id="4">
				<sentence>The parser has two components : a discourse segmenter , which identifies the elementary discourse units in a text ; and a shift-reduce action identifier , which determines how these units should be assembled into rhetorical structure trees .</sentence>
				<definiendum id="0">discourse segmenter</definiendum>
				<definiens id="0">identifies the elementary discourse units in a text</definiens>
			</definition>
</paper>

		<paper id="1068">
			<definition id="0">
				<sentence>STRAND ( Resnik , 1998 ) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web .</sentence>
				<definiendum id="0">STRAND</definiendum>
			</definition>
			<definition id="1">
				<sentence>Precision is estimated as the proportion of pages judged GOOD by STRAND that were also judged to be good ( i.e. `` yes '' ) by both judges -this figure is 92.1 % Recall is estimated as the number of pairs that should have been judged GOOD by STRAND ( i.e. that recieved a `` yes '' from both judges ) that STRAND indeed marked GOOD -this figure is 47.3 % .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">STRAND</definiendum>
				<definiens id="0">the proportion of pages judged GOOD by STRAND that were also judged to be good ( i.e. `` yes ''</definiens>
			</definition>
</paper>

		<paper id="1061">
			<definition id="0">
				<sentence>A positive restrictor is an automaton describing the paths in a feature structure that will remain after restriction ( the deletion operation ) , 3There are refinements of the technique which we have implemented and which in practice produce additional benefits ; we will report these in a subsequent paper .</sentence>
				<definiendum id="0">positive restrictor</definiendum>
				<definiens id="0">an automaton describing the paths in a feature structure that will remain after restriction ( the deletion operation ) , 3There are refinements of the technique which we have implemented and which in practice produce additional benefits</definiens>
			</definition>
			<definition id="1">
				<sentence>DISCO -- an HPSG-based NLP system and its application for appointment scheduling .</sentence>
				<definiendum id="0">DISCO</definiendum>
				<definiens id="0">an HPSG-based NLP system and its application for appointment scheduling</definiens>
			</definition>
</paper>

		<paper id="1056">
			<definition id="0">
				<sentence>The grapheme frequency and grapheme entropy estimates for pseudowords were computed by averaging respectively grapheme frequency or grapheme entropy across all graphemes in the letter string .</sentence>
				<definiendum id="0">grapheme entropy</definiendum>
				<definiens id="0">pseudowords were computed by averaging respectively grapheme frequency or grapheme entropy across all graphemes in the letter string</definiens>
			</definition>
			<definition id="1">
				<sentence>Marielle Lange is a research assistant at the Belgian National Fund for Scientific Research ( FNRS ) .</sentence>
				<definiendum id="0">Marielle Lange</definiendum>
			</definition>
</paper>

		<paper id="1059">
			<definition id="0">
				<sentence>A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles .</sentence>
				<definiendum id="0">bilexical grammar</definiendum>
				<definiens id="0">makes many stipulations about the compatibility of particular pairs of words in particular roles</definiens>
			</definition>
			<definition id="1">
				<sentence>A context-free grammar ( CFG ) is a tuple G = ( VN , VT , P , S ) , where VN and VT are finite , disjoint sets of nonterminal and terminal symbols , respectively , and S E VN is the start symbol .</sentence>
				<definiendum id="0">context-free grammar ( CFG )</definiendum>
				<definiendum id="1">S E VN</definiendum>
				<definiens id="0">a tuple G = ( VN , VT , P , S ) , where VN and VT are finite , disjoint sets of nonterminal and terminal symbols , respectively , and</definiens>
				<definiens id="1">the start symbol</definiens>
			</definition>
			<definition id="2">
				<sentence>459 ( a ) A i4 , A A h z j ( i g h &lt; j , A E VD ) ( i &lt; j &lt; h , A , C E VD ) ( h &lt; i &lt; j , A , C E VD ) is derived iff A\ [ dh\ ] ~* wi , j is derived iff A\ [ dh\ ] ~ B\ [ dh , \ ] C\ [ dh\ ] ~* wi , jC\ [ dh\ ] for some B , h ' is derived iff A\ [ dh\ ] ~ C\ [ dh\ ] B\ [ dh , \ ] ~* C\ [ dh\ ] wi , j for some B , h ' ( b ) STAaT : ~ A\ [ dh\ ] ~ dh h @ h ATTACH-LEFT : B A ./Q '' . c ~ 3 h ATTACH-RIGHT : B .4 h ~ 3 A\ [ dh\ ] -~ B\ [ dh , \ ] C\ [ dh\ ] A\ [ dh\ ] -~ C\ [ dh\ ] B\ [ dh , \ ] COMPLETE-RIGHT : COMPLETE-LEFT : A C 3 h j A iz k C A A iz @ k Figure 1 : An O ( n 4 ) recognition algorithm for CNF bilexical CFG. ( a ) Types of items in the parse table ( chart ) . The first is syntactic sugar for the tuple \ [ A , A , i , h , j\ ] , and so on. The stated conditions assume that dl , ... dn are all distinct. ( b ) Inference rules. The algorithm derives the item below -if the items above -have already been derived and any condition to the right of is met. It accepts input w just if item I/k , T , 1 , h , n\ ] is derived for some h such that dh -= $ . ( a ) A A i//\ ] h ( i &lt; _ h , A e VD ) A h~ ( h &lt; j , A E VD ) , ~. ~C ( i _ &lt; j &lt; h , A , C E VD ) 3 h A A C~. ( h &lt; i &lt; j , A , C E VD ) h ~ 3 ( i &lt; h _ &lt; j , A E VD ) is derived iff A\ [ dh\ ] ~* wi , j is derived iff A\ [ dh\ ] ~* wi , j for some j _ &gt; h is derived iff A\ [ dh\ ] ~* w~ , j for some i _ &lt; h is derived iff A\ [ dh\ ] ~ B\ [ dh , \ ] C\ [ dh\ ] ~* wi , jC\ [ dh\ ] ~* wi , k for some B , h ~ , k is derived iff A\ [ dh\ ] ~ C\ [ dh\ ] B\ [ dh , \ ] ~* C\ [ dh\ ] wi , j ~* Wk , j for some B , h ~ , k ( b ) As in Figure l ( b ) above , but add HALVE and change ATTACH-LEFT and ATTACH-RIGHT as shown .</sentence>
				<definiendum id="0">A h z j</definiendum>
				<definiendum id="1">E VD</definiendum>
				<definiendum id="2">E VD</definiendum>
				<definiendum id="3">C~.</definiendum>
				<definiendum id="4">E VD</definiendum>
				<definiens id="0">B .4 h ~ 3 A\ [ dh\ ] -~ B\ [ dh , \ ] C\ [ dh\ ] A\ [ dh\ ] -~ C\ [ dh\ ] B\ [ dh , \ ] COMPLETE-RIGHT : COMPLETE-LEFT : A C 3 h j A iz k C A A iz @ k Figure 1 : An O ( n 4 ) recognition algorithm for CNF bilexical CFG. ( a ) Types of items in the parse table</definiens>
			</definition>
			<definition id="3">
				<sentence>A head automaton grammar ( HAG ) is a function H : a ~ Ha that defines a head automaton ( HA ) for each element of its ( finite ) domain .</sentence>
				<definiendum id="0">head automaton grammar</definiendum>
				<definiendum id="1">HAG</definiendum>
				<definiendum id="2">H</definiendum>
				<definiens id="0">a function</definiens>
			</definition>
			<definition id="4">
				<sentence>For each a E VT , Ha is a tuple ( Qa , VT , ( ~a , In , Fa ) , where • Qa is a finite set of states ; 4Translation in the other direction is possible if the HAG formalism is extended to allow multiple senses per word ( see §6 ) .</sentence>
				<definiendum id="0">E VT , Ha</definiendum>
				<definiendum id="1">Qa</definiendum>
				<definiens id="0">a tuple</definiens>
			</definition>
			<definition id="5">
				<sentence>A single head automaton is an acceptor for a language of string pairs ( z~ , Zr ) E V~ x V~ .</sentence>
				<definiendum id="0">single head automaton</definiendum>
				<definiens id="0">an acceptor for a language of string pairs ( z~ , Zr ) E V~ x V~</definiens>
			</definition>
			<definition id="6">
				<sentence>Grammatical trigrams : A probabilistic model of link grammar .</sentence>
				<definiendum id="0">Grammatical trigrams</definiendum>
				<definiens id="0">A probabilistic model of link grammar</definiens>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>The semantically smoothed probability of a pair ( v , n ) is defined to be : p ( v , n ) = ~~p ( c , v , n ) = ~-'\ ] p ( c ) p ( vJc ) p ( nJc ) cEC cEC The joint distribution p ( c , v , n ) is defined by p ( c , v , n ) = p ( c ) p ( vlc ) p ( n\ [ c ) .</sentence>
				<definiendum id="0">semantically smoothed probability</definiendum>
				<definiens id="0">of a pair ( v , n ) is defined to be : p ( v , n ) = ~~p ( c , v , n ) = ~-'\ ] p ( c ) p ( vJc ) p ( nJc ) cEC cEC The joint distribution p ( c , v , n ) is defined by p ( c , v , n ) = p ( c ) p ( vlc ) p ( n\ [ c )</definiens>
			</definition>
			<definition id="1">
				<sentence>We are given : ( i ) a sample space y of observed , incomplete data , corre17 : scalar change sponding to pairs from VxN , ( ii ) a sample space X of unobserved , complete data , corresponding to triples from CxYxg , ( iii ) a set X ( y ) = { x E X \ [ x = ( c , y ) , c E C } of complete data related to the observation y , ( iv ) a complete-data specification pe ( x ) , corresponding to the joint probability p ( c , v , n ) over C x V x N , with parametervector 0 : ( 0c , Ovc , OncJc E C , v e V , n E N ) , ( v ) an incomplete data specification Po ( Y ) which is related to the complete-data specification as the marginal probability Po ( Y ) -~~X ( y ) po ( x ) . ``</sentence>
				<definiendum id="0">c E</definiendum>
				<definiens id="0">a sample space y of observed , incomplete data , corre17 : scalar change sponding to pairs from VxN , ( ii ) a sample space X of unobserved , complete data , corresponding to triples from CxYxg , ( iii ) a set X ( y ) = { x E X \ [ x = ( c , y ) ,</definiens>
				<definiens id="1">OncJc E C , v e V , n E N ) , ( v ) an incomplete data specification Po ( Y )</definiens>
			</definition>
			<definition id="2">
				<sentence>This auxiliary function is iteratively maximized as a function of O ( M-step ) , where each iteration is defined by the map O ( t+l ) = M ( O ( t ) = argmax Q ( O ; 0 ( t ) ) EM-algorithm for context-free models ( Baum et 105 Class 5 PROB 0.0412 ~g ?</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">( t ) = argmax Q ( O ; 0 ( t ) ) EM-algorithm for context-free models</definiens>
			</definition>
			<definition id="3">
				<sentence>The V x Nspace for the above clustering models included about 425 million ( v , n ) combinations ; we approximated the smoothing size of a model by randomly sampling 1000 pairs from V x N and returning the percentage of positively assigned pairs in the random sample .</sentence>
				<definiendum id="0">V x Nspace</definiendum>
				<definiens id="0">the smoothing size of a model by randomly sampling 1000 pairs from V x N and returning the percentage of positively assigned pairs in the random sample</definiens>
			</definition>
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>A formal concept is defined in FCA as an extension-intension pair ( A , B ) , where the extension is a subset A of the set of objects and the intension is a subset B of the set of attributes .</sentence>
				<definiendum id="0">extension-intension pair</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">intension</definiendum>
				<definiens id="0">a subset A of the set of objects and the</definiens>
				<definiens id="1">a subset B of the set of attributes</definiens>
			</definition>
			<definition id="1">
				<sentence>A 'subconcept ' relation , ' &lt; FCA ' , is defined over the set of formal concepts thus : ( A , B ) &lt; FCA ( A* , B* ) iff A C A* ~=~ B* C B The main theorem of FCA then shows that &lt; FCA induces a complete lattice structure over the set of formal concepts. The resulting lattice for the present example is shown in Figure 1. Each node is shown labeled with two pieces of information : the intension and the extension. The intensions consist simply of the sets of properties involved. The representations of the extensions emphasize the function of the nodes in the lattice -- i.e. , that the indicated objects ( e.g. , ol and o2 for the leftmost node ) are equal with respect to all the attributes contained in the intension ( e.g. , type for the leftmost node ) . 128 { TYPE } m ( ol ) =m ( o2 ) C &gt; { COLOR , SIZE } m ( ol ) =m ( o3 ) Figure 1 : Simple aggregation lattice This lattice may be construed as an aggregation lattice because the functional redundancies that are captured are precisely those redundances that indicate opportunities for structurally-induced aggregation .</sentence>
				<definiendum id="0">'subconcept ' relation</definiendum>
				<definiens id="0">the set of formal concepts thus : ( A , B ) &lt; FCA ( A* , B* ) iff A C A* ~=~ B* C B The main theorem of FCA then shows that &lt; FCA induces a complete lattice structure over the set of formal concepts. The resulting lattice for the present example is shown in Figure 1. Each node is shown labeled with two pieces of information : the intension and the extension. The intensions consist simply of the sets of properties involved. The representations of the extensions emphasize the function of the nodes in the lattice -- i.e. , that the indicated objects ( e.g. , ol and o2 for the leftmost node ) are equal with respect to all the attributes contained in the intension ( e.g. , type for the leftmost node )</definiens>
				<definiens id="1">an aggregation lattice because the functional redundancies that are captured are precisely those redundances that indicate opportunities for structurally-induced aggregation</definiens>
			</definition>
			<definition id="2">
				<sentence>The first variant they consider relies on a 'Greedy Heuristic ' ( Dale , 1989 ; Johnson , 1974 ) ; they illustrate that this algorithm sacrifices minimality by constructing an RE for object ol in the context of the following properties concerning a set of seven cups of varying size ( large , small ) , color ( red , green , blue ) and material ( paper , plastic ) : ( oi ( size large ) ( color red ) ( material plastic ) ) ( 02 ( size small ) ( color red ) ( material plastic ) ) ( 03 ( size small ) ( color red ) ( material paper ) ) ( 04 ( size medium ) ( color red ) ( material paper ) ) ( 05 ( size large ) ( color green ) ( material paper ) ) ( 06 ( size large ) ( color blue ) ( material paper ) ) ( 07 ( size large ) ( color blue ) ( material plastic ) ) The greedy algorithm produces 'the large red plastic cup ' although the true minimum description is 'the large red cup ' .</sentence>
				<definiendum id="0">color red )</definiendum>
				<definiendum id="1">material paper ) ) ( 06 ( size large )</definiendum>
				<definiens id="0">object ol in the context of the following properties concerning a set of seven cups of varying size ( large , small ) , color ( red , green , blue ) and material</definiens>
				<definiens id="1">material plastic ) ) ( 02 ( size small )</definiens>
				<definiens id="2">size large ) ( color blue ) ( material plastic ) ) The greedy algorithm produces 'the large red plastic cup ' although the true minimum description is 'the large red cup '</definiens>
			</definition>
			<definition id="3">
				<sentence>Partially to handle such situations , Horacek provides a further related algorithm that is intended to improve on the original and which he illustrates in action with reference to a rather more complex situation involving two tables with a variety of cups and bottles on them .</sentence>
				<definiendum id="0">Horacek</definiendum>
			</definition>
			<definition id="4">
				<sentence>To form an aggregation lattice for this fact set , we extend our data representation to deal with relations as well as attributes .</sentence>
				<definiendum id="0">aggregation lattice</definiendum>
			</definition>
</paper>

		<paper id="1070">
			<definition id="0">
				<sentence>Both probabilistic context-free grammars ( PCFGs ) and shift-reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .</sentence>
				<definiendum id="0">PPDAs</definiendum>
				<definiens id="0">Both probabilistic context-free grammars ( PCFGs ) and shift-reduce probabilistic pushdown automata</definiens>
			</definition>
			<definition id="1">
				<sentence>In general each stack entry consists of a syntactic category and a head word .</sentence>
				<definiendum id="0">stack entry</definiendum>
				<definiens id="0">consists of a syntactic category and a head word</definiens>
			</definition>
			<definition id="2">
				<sentence>A PCFG is a context-free grammar in which each production is associated with a weight in the interval \ [ 0 , 1\ ] and such that the weights of the productions from any given nonterminal sum to 1 .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">a context-free grammar in which each production is associated with a weight in the interval \ [ 0 , 1\ ] and such that the weights of the productions from any given nonterminal sum to 1</definiens>
			</definition>
			<definition id="3">
				<sentence>This contrast between PPDAs and PCFGs is formalized in theorem 1 , which exhibits a PCFG for which no stochastic parameterization of the corresponding shift-reduce parser yields the same probability distribution over strings .</sentence>
				<definiendum id="0">PCFGs</definiendum>
				<definiens id="0">exhibits a PCFG for which no stochastic parameterization of the corresponding shift-reduce parser yields the same probability distribution over strings</definiens>
			</definition>
			<definition id="4">
				<sentence>A weighted context-free grammar ( WCFG ) consists of a distinguished start symbol S E N plus a finite set of weighted productions of the form X -~ a , ( alternately , u : X -- ~ a ) , where X E N , a E ( Nt2E ) * and the weight u is a nonnegative real number .</sentence>
				<definiendum id="0">weighted context-free grammar ( WCFG</definiendum>
				<definiendum id="1">E</definiendum>
				<definiendum id="2">weight u</definiendum>
				<definiens id="0">a nonnegative real number</definiens>
			</definition>
			<definition id="5">
				<sentence>A PCFG defines a stochastic process with sentential forms as states , and leftmost rewriting steps as transitions .</sentence>
				<definiendum id="0">PCFG</definiendum>
				<definiens id="0">defines a stochastic process with sentential forms as states</definiens>
			</definition>
			<definition id="6">
				<sentence>We say a WCFG G admits a tree d just in case all nodes of d are well-labeled , and all labels are productions of G. Note that no requirement is placed on the nonterminal of the root node of d ; in particular , it need not be S. We define the weight of a tree d , denoted Wa ( d ) , or W ( d ) if G is clear from context , to be the product of weights of its nodes .</sentence>
				<definiendum id="0">WCFG G</definiendum>
				<definiendum id="1">W</definiendum>
				<definiens id="0">admits a tree d just in case all nodes of d are well-labeled , and all labels are productions of G. Note that no requirement is placed on the nonterminal of the root node of d ; in particular , it need not be S. We define the weight of a tree d , denoted Wa ( d ) , or</definiens>
			</definition>
			<definition id="7">
				<sentence>We write , for example , { p = X } as an abbreviation for { dip ( d ) = X } ; and WG ( p = X ) represents the sum of weights of such trees .</sentence>
				<definiendum id="0">X )</definiendum>
			</definition>
			<definition id="8">
				<sentence>A weighted push-down automaton ( WPDA ) consists of a distinguished start state q0 , a distinguished start stack symbol X0 and a finite set of transitions of the following form where p and q are states , a E E L.J { e } , X and Z1 , ... , Zn are stack symbols , and w is a nonnegative real weight : x , pa~ Zl ... Zn , q A WPDA is a probabilistic push-down automaton ( PPDA ) if all weights are in the interval \ [ 0 , 1\ ] and for each pair of a stack symbol X and a state q the sum of the weights of all transitions of the form X , p ~ Z1 ... Z= , q equals 1 .</sentence>
				<definiendum id="0">weighted push-down automaton</definiendum>
				<definiendum id="1">WPDA</definiendum>
				<definiendum id="2">w</definiendum>
				<definiendum id="3">WPDA</definiendum>
				<definiens id="0">a nonnegative real weight : x</definiens>
				<definiens id="1">a probabilistic push-down automaton ( PPDA ) if all weights are in the interval \ [ 0 , 1\ ] and for each pair of a stack symbol X and a state q the sum of the weights of all transitions of the form X</definiens>
			</definition>
			<definition id="9">
				<sentence>form with underlying CFG G ' such that no consistent weighting M of the PDA SR ( G ~ ) has the property that PM ( Y ) = Pa ( u ) for all U e To prove the theorem take G to be the following grammar .</sentence>
				<definiendum id="0">PDA SR</definiendum>
				<definiens id="0">the property that PM ( Y ) = Pa ( u ) for all U e To prove the theorem take G to be the following grammar</definiens>
			</definition>
			<definition id="10">
				<sentence>A reweighting of G is any WCFG derived from G by changing the weights of the productions of G. Lemma 5 For any convergent proper WCFG G , there exists a reweighting G t of G such that G ~ is a consistent PCFG such that for all terminal strings y we have PG ' ( Y ) = Pa ( Y ) .</sentence>
				<definiendum id="0">reweighting of G</definiendum>
				<definiens id="0">any WCFG derived from G by changing the weights of the productions of G. Lemma 5 For any convergent proper WCFG G , there exists a reweighting G t of G such that G ~ is a consistent PCFG such that for all terminal strings</definiens>
			</definition>
			<definition id="11">
				<sentence>547 The H-derivations from X/Y will be in one to one correspondence with the left-corner Gderivations from X to Y. For each production in G of the form X ~ a we include the following in H where S is the start symbol of G : S -- ~ a S/X We also include in H all productions of the following form where X is any nonterminal in G : x/x If G consists only of productions of the form S -~ a these productions suffice .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">the start symbol of G</definiens>
			</definition>
			<definition id="12">
				<sentence>We now define the final grammar G ' to consist of all productions of the following form where X , Y , and Z are nontrivial nonterminals appearing in J and a is a terminal symbol appearing in J. X Pj ( a=a I__~=X , `` y¢¢ ) a X pj ( a=aY~_~=X , `` yCe ) aY X PJ ( a=aYZl-~ p=X ' ~¢ ) aYZ As in section 4 , for every nontrivial nonterminal X in K and terminal string ( ~ we have PK ( a = ( ~ I P= X ) = Pj ( a= a I P= X , a ~ e ) .</sentence>
				<definiendum id="0">X pj</definiendum>
				<definiendum id="1">yCe ) aY X PJ</definiendum>
				<definiens id="0">the final grammar G ' to consist of all productions of the following form where X , Y , and Z are nontrivial nonterminals appearing in J</definiens>
			</definition>
			<definition id="13">
				<sentence>The stack symbols of M are of the form W~ where ce E N* is a proper suffix of the right hand side of some production in G. For example , if G contains the production X -~ aYZ then the symbols of M include Wyz , Wy , and We .</sentence>
				<definiendum id="0">ce E N*</definiendum>
				<definiens id="0">the production X -~ aYZ then the symbols of M include Wyz , Wy , and We</definiens>
			</definition>
</paper>

		<paper id="1055">
			<definition id="0">
				<sentence>Notice that the penalty probability , essentially a fitness measure of individual grammars , is an intrinsic property of a UG-defined grammar relative to a particular linguistic environment E , determined by the distributional patterns of linguistic expressions in E. It is not explicitly computed , as in ( Clark , 1992 ) which uses the Genetic Algorithm ( GA ) .</sentence>
				<definiendum id="0">Genetic Algorithm</definiendum>
				<definiens id="0">an intrinsic property of a UG-defined grammar relative to a particular linguistic environment E , determined by the distributional patterns of linguistic expressions in E. It is not explicitly computed</definiens>
			</definition>
			<definition id="1">
				<sentence>Consider the acquisition of the target , a German V2 grammar , in a population of grammars below : We have used X to denote non-argument categories such as adverbs , adjuncts , etc. , which can quite freely appear in sentence-initial positions .</sentence>
				<definiendum id="0">German V2 grammar</definiendum>
				<definiens id="0">adjuncts , etc. , which can quite freely appear in sentence-initial positions</definiens>
			</definition>
			<definition id="2">
				<sentence>As noted in section 2.5 , there appears to no unambiguous evidence for the \ [ +\ ] value of the V2 parameter : SVO , VSO , and OVS grammars , members of the \ [ -V2\ ] class , are each compatible with certain proportions of expressions produced .</sentence>
				<definiendum id="0">OVS grammars</definiendum>
				<definiens id="0">each compatible with certain proportions of expressions produced</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>People deal with polysemy so easily that potential abiguities are overlooked , whereas computers must work hard to do far less well .</sentence>
				<definiendum id="0">People</definiendum>
				<definiens id="0">deal with polysemy so easily that potential abiguities are overlooked</definiens>
			</definition>
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Among the machine learning algorithms studied , rule based systems have proven effective on many natural language processing tasks , including part-of-speech tagging ( Brill , 1995 ; Ramshaw and Marcus , 1994 ) , spelling correction ( Mangu and Brill , 1997 ) , word-sense disambiguation ( Gale et al. , 1992 ) , message understanding ( Day et al. , 1997 ) , discourse tagging ( Samuel et al. , 1998 ) , accent restoration ( Yarowsky , 1994 ) , prepositional-phrase attachment ( Brill and Resnik , 1994 ) and base noun phrase identification ( Ramshaw and Marcus , In Press ; Cardie and Pierce , 1998 ; Veenstra , 1998 ; Argamon et al. , 1998 ) .</sentence>
				<definiendum id="0">prepositional-phrase attachment</definiendum>
				<definiens id="0">spelling correction ( Mangu and Brill , 1997 ) , word-sense disambiguation ( Gale et al. , 1992 ) , message understanding ( Day et al. , 1997 ) , discourse tagging ( Samuel et al. , 1998 ) , accent restoration</definiens>
			</definition>
			<definition id="1">
				<sentence>1 To train their system , R &amp; M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal ( Marcus et al. , 1993 ) tagged using a transformation-based tagger ( Brill , 1995 ) and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics ( like treating the possessive marker as the first word of a new base noun phrase ) to flatten the recursive structure of the parse .</sentence>
				<definiendum id="0">transformation-based tagger</definiendum>
				<definiendum id="1">possessive marker</definiendum>
			</definition>
			<definition id="2">
				<sentence>67 TRAINING SET ( 25K Words ) Precision Recall 87.8 % 88.6 % 88.1 % 88.2 % 88.6 % 87.6 % 88.0 % 87.2 % 86.2 % 86.8 % 86.0 % 87.1 % 84.9 % 86.7 % 83.6 % 86.0 % 83.9 % 85.0 % 82.8 % 84.5 % 84.8 % 78.8 % Student 1 Student 2 Student 3 Student 4 Student 5 Student 6 Student 7 Student 8 Student 9 Student 10 Student 11 F-Measure P+n Precision 2 88.2 88.2 88.0 % 88.2 88.2 88.2 % 88.1 88.2 88.3 % 87.6 87.6 86.9 % 86.5 86.5 85.8 % 86.6 86.6 85.8 % 85.8 85.8 85.3 % 84.8 84.8 83.1 % 84.4 84.5 83.5 % 83.6 83.7 83.3 % 81.7 81.8 84.0 % TEST SET Recall F-Measure 88.8 % 88.4 87.9 % 88.0 87.8 % 88.0 85.9 % 86.4 85.8 % 85.8 87.1 % 86.4 87.3 % 86.3 85.7 % 84.4 84.8 % 84.1 84.4 % 83.8 77.4 % 80.6 2 88.4 88.1 88.1 86.4 85.8 86.5 86.3 84.4 84.2 83.8 80.7 Figure 1 : P/R results of test subjects on training and test corpora In the beginning , we believed that the students would be able to match or better the R &amp; M system 's results , which are shown in figure 2 .</sentence>
				<definiendum id="0">25K Words ) Precision Recall</definiendum>
				<definiens id="0">P/R results of test subjects on training and test corpora In the beginning</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Corpus : Our corpus consists of a set of 544 dialogues ( over 40 hours of speech ) between humans and one of three dialogue systems : ANNIE ( Kamm et al. , 1998 ) , an agent for voice dialing and messaging ; ELVIS ( Walker et al. , 1998b ) , an agent for accessing email ; and TOOT ( Litman and Pan , 1999 ) , an agent for accessing online train schedules .</sentence>
				<definiendum id="0">Corpus</definiendum>
				<definiendum id="1">TOOT</definiendum>
				<definiens id="0">a set of 544 dialogues ( over 40 hours of speech ) between humans and one of three dialogue systems : ANNIE ( Kamm et al. , 1998 ) , an agent for voice dialing and messaging ; ELVIS ( Walker et al. , 1998b ) , an agent for accessing email</definiens>
			</definition>
			<definition id="1">
				<sentence>Rejections represents the number of times that the system plays special rejection prompts , e.g. , utterances A2 and A3 in dialogue D1 .</sentence>
				<definiendum id="0">Rejections</definiendum>
			</definition>
			<definition id="2">
				<sentence>In this case , RIPPER learns some rules that are specific to the TOOT system .</sentence>
				<definiendum id="0">RIPPER</definiendum>
				<definiens id="0">learns some rules that are specific to the TOOT system</definiens>
			</definition>
</paper>

		<paper id="1060">
			<definition id="0">
				<sentence>a \E ( which ) ( mary ) ( ate ) rel/ ( s/np ) np ( np\s ) /np \ [ np\ ] /E np\s \E S rel The above proof illustrates 'hypothetical reasoning ' , i.e. the presence of additional assumptions ( 'hypotheticals ' ) in proofs that are subsequently discharged .</sentence>
				<definiendum id="0">\E</definiendum>
				<definiens id="0">the presence of additional assumptions ( 'hypotheticals ' ) in proofs that are subsequently discharged</definiens>
			</definition>
			<definition id="1">
				<sentence>The labelled formula is computed from ( Xi : ( h-i ) ) + using the polar translation functions shown in Figure 1 ( where /~ denotes the complementary polarity to p ) .3 As an example , Figure 1 also shows the results of converting the antededents of X/ ( Y/Z ) , W , ( W\Y ) /Z =~ X ( where k is a constant and i , j variables ) .</sentence>
				<definiendum id="0">W\Y ) /Z =~ X</definiendum>
				<definiendum id="1">k</definiendum>
				<definiens id="0">a constant and i , j variables )</definiens>
			</definition>
			<definition id="2">
				<sentence>The higher-order X/ ( Y/Z ) yields two output formulae : the main residue X/Y and the hypothetical Z , with the dependency between the two indicated by the common index 1 in the argument index set of the former and the principal index set of the latter .</sentence>
				<definiendum id="0">higher-order X/</definiendum>
				<definiens id="0">the main residue X/Y and the hypothetical Z , with the dependency between the two indicated by the common index 1 in the argument index set of the former and the principal index set of the latter</definiens>
			</definition>
			<definition id="3">
				<sentence>s ) + F where Y atomic , T ( ( m , X1 , ( tv ) ) ) = ( re , X2 , s ) + F , v a fresh variable as for ( T2a ) modulo directionality of connective v ( ( m , X/ ( ( Y/Z ) : rni ) , t ) ) = A + ( B U F U A ) where w , v fresh variables , i a fresh multiset index , m2 = i U rnl v ( ( m , X/ ( Y : m2 ) , Aw .</sentence>
				<definiendum id="0">Y atomic</definiendum>
				<definiens id="0">re , X2 , s ) + F</definiens>
			</definition>
			<definition id="4">
				<sentence>The general form of edges is : ( ( ml , m2 ) , 9 , r , ( A ~ F * A ) ) where ( ~4 ~ F , A ) E R , 0 is a substitution over span variables , r is a restrictor set identifying span variables whose values are required non-locally ( explained below ) , and ml , m2 are multisets .</sentence>
				<definiendum id="0">r</definiendum>
				<definiens id="0">a substitution over span variables</definiens>
				<definiens id="1">a restrictor set identifying span variables whose values are required non-locally ( explained below ) , and ml</definiens>
			</definition>
			<definition id="5">
				<sentence>FA ) was introduced , and m2 is the current multiset for passing onto the daughters in A. We call ml the initial multiset and m2 the current multiset .</sentence>
				<definiendum id="0">m2</definiendum>
				<definiens id="0">the current multiset for passing onto the daughters in A. We call ml the initial multiset and m2 the current multiset</definiens>
			</definition>
			<definition id="6">
				<sentence>B\ [ m4\ ] ( i-h ) , A ) ) E E and ( ( m2 , ms ) , 02 , r2 , ( B\ [ m6\ ] ( i-j ) -4 A* ) ) E E then ( ( ml , ms ) , 03 , rl , ( A\ [ m3\ ] ( f - ( gO ) ) -~ F , B\ [ m4\ ] ( i-j ) * ( A0 ) ) ) E E where O=01+02+MGU ( h , j ) ; mhCrn2 ; m6C_m2Um4 ; 03 = O/ ( rl U dauglnlv ( A ) ) Figure 4 : Chart rules m5 is non-empty ) , or to the addition of indices from the predicting edge 's next rhs unit ( i.e. if ma is non-empty ) .</sentence>
				<definiendum id="0">O=01+02+MGU</definiendum>
				<definiens id="0">non-empty ) , or to the addition of indices from the predicting edge 's next rhs unit</definiens>
			</definition>
			<definition id="7">
				<sentence>The function call dauglnlv ( A ) returns the set of non-local variables associated with the multiset indices of the next daughter in A ( or the empty set if A is empty ) .</sentence>
				<definiendum id="0">function call dauglnlv</definiendum>
				<definiens id="0">A ) returns the set of non-local variables associated with the multiset indices of the next daughter in A ( or the empty set if A is empty )</definiens>
			</definition>
</paper>

		<paper id="1073">
			<definition id="0">
				<sentence>A first attempt would be to simply multiply the score of the dialogue by the average subgoal complexity per main goal per speaker turn in the dialogue , where Nmg is the number of main goals in a speaker turn and Nsg is the number of subgoals .</sentence>
				<definiendum id="0">Nmg</definiendum>
				<definiendum id="1">Nsg</definiendum>
				<definiens id="0">the number of subgoals</definiens>
			</definition>
</paper>

		<paper id="1054">
			<definition id="0">
				<sentence>The FOM for an analysis is the product of the probabilities of all PCFG rules used in its derivation and what we call its look-ahead probability ( LAP ) .</sentence>
				<definiendum id="0">FOM for an analysis</definiendum>
				<definiendum id="1">look-ahead probability</definiendum>
				<definiens id="0">the product of the probabilities of all PCFG rules used in its derivation</definiens>
			</definition>
			<definition id="1">
				<sentence>Grammar binaxization is one way to do this , by allowing the parser to use a rule like NP -- + DT NP-DT , where the new non-terminal NP-DT can expand into anything that follows a DT in an NP .</sentence>
				<definiendum id="0">Grammar binaxization</definiendum>
				<definiens id="0">a rule like NP -- + DT NP-DT , where the new non-terminal NP-DT can expand into anything that follows a DT in an NP</definiens>
			</definition>
			<definition id="2">
				<sentence>RB0 pushes the lookahead out to the first item in the string after the constituent being expanded , which can be useful in deciding between rules of unequal length , e.g. NP -- -+ DT NN and NP ~ DT NN NN .</sentence>
				<definiendum id="0">RB0</definiendum>
				<definiens id="0">pushes the lookahead out to the first item in the string after the constituent being expanded</definiens>
			</definition>
			<definition id="3">
				<sentence>Left-corner ( LC ) parsing ( Rosenkrantz and Lewis II , 1970 ) is a well-known strategy that uses both bottom-up evidence ( from the left corner of a rule ) and top-down prediction ( of the rest of the rule ) .</sentence>
				<definiendum id="0">Left-corner ( LC ) parsing</definiendum>
				<definiens id="0">a well-known strategy that uses both bottom-up evidence ( from the left corner of a rule ) and top-down prediction ( of the rest of the rule )</definiens>
			</definition>
			<definition id="4">
				<sentence>As a baseline , their parser considered an average of 2216 edges per sentence in section 22 of the WSJ corpus ( p.c. ) .</sentence>
				<definiendum id="0">WSJ corpus</definiendum>
				<definiens id="0">their parser considered an average of 2216 edges per sentence in section 22 of the</definiens>
			</definition>
			<definition id="5">
				<sentence>Given our two binarization orientations ( LB and RB ) , there are four possible compositions of binarization and LC transforms : ( a ) LB o LC ( b ) RB o LC ( c ) LC o LB ( d ) LC o RB Table 2 shows left-corner results over various conditions 6 .</sentence>
				<definiendum id="0">binarization orientations</definiendum>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>LM2 is the reading-dependent language model , and LM3 is a modification of LM2 by filled-pause modeling .</sentence>
				<definiendum id="0">LM2</definiendum>
				<definiendum id="1">LM3</definiendum>
				<definiens id="0">the reading-dependent language model , and</definiens>
			</definition>
			<definition id="1">
				<sentence>The system consists of three components : ( 1 ) a speaker-independent largevocabulary speech recognition engine which ( Satellite receiver ) ~ Video ( MPEG-coder ) MPEO-video ~ MPEG-audio C Segm nter ) ~ MPEG-audio , Segment boundaries ~peech recognizer ) MPEO-auaio Text Segment boundaries I Result output \ ] -- ~ \ [ ( Thesaurus ) Video query server ) .</sentence>
				<definiendum id="0">system</definiendum>
				<definiendum id="1">Video</definiendum>
				<definiendum id="2">Segment boundaries ~peech recognizer ) MPEO-auaio Text Segment</definiendum>
				<definiens id="0">consists of three components : ( 1 ) a speaker-independent largevocabulary speech recognition engine which ( Satellite receiver ) ~</definiens>
				<definiens id="1">boundaries I Result output \ ] -- ~ \ [ ( Thesaurus ) Video query server )</definiens>
			</definition>
			<definition id="2">
				<sentence>The speech recognition component of SCAN includes an intonational phrase boundary detection module and a classification module , These subcomponents preprocess the speech data before passing the speech to the recognizer itself .</sentence>
				<definiendum id="0">speech recognition component of SCAN</definiendum>
				<definiens id="0">includes an intonational phrase boundary detection module and a classification module , These subcomponents preprocess the speech data before passing the speech to the recognizer itself</definiens>
			</definition>
			<definition id="3">
				<sentence>MODELING speech recognition One of the most important issues for speech recognition is how to create language models ( rules ) for spontaneous speech .</sentence>
				<definiendum id="0">speech recognition</definiendum>
				<definiens id="0">how to create language models ( rules ) for spontaneous speech</definiens>
			</definition>
			<definition id="4">
				<sentence>18 State-of-the-art automatic speech recognition systems employ the criterion of maximizing P ( /4 , qX ) , where W is a word sequence , and X is an acoustic observation sequence .</sentence>
				<definiendum id="0">State-of-the-art automatic speech recognition systems</definiendum>
				<definiendum id="1">W</definiendum>
				<definiendum id="2">X</definiendum>
				<definiens id="0">a word sequence</definiens>
				<definiens id="1">an acoustic observation sequence</definiens>
			</definition>
			<definition id="5">
				<sentence>9 \ [ 18\ ] , where M is the message ( content ) that a speaker intended to convey .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the message ( content ) that a speaker intended to convey</definiens>
			</definition>
</paper>

		<paper id="1057">
			<definition id="0">
				<sentence>A table consists of one or more hlines .</sentence>
				<definiendum id="0">table</definiendum>
			</definition>
			<definition id="1">
				<sentence>• Special character : any character that is not a space character and not an alphanumeric character .</sentence>
				<definiendum id="0">Special character</definiendum>
				<definiens id="0">any character that is not a space character and not an alphanumeric character</definiens>
			</definition>
			<definition id="2">
				<sentence>446 Feature Description F1 F2 F3 Whether H consists of only space characters .</sentence>
				<definiendum id="0">F3 Whether H</definiendum>
				<definiens id="0">consists of only space characters</definiens>
			</definition>
			<definition id="3">
				<sentence>c~ , # , and Vj+I = Czj+l ... cij+l ... cm , j+z where m is the number of hlines that constitute a table .</sentence>
				<definiendum id="0">m</definiendum>
				<definiens id="0">the number of hlines that constitute a table</definiens>
			</definition>
			<definition id="4">
				<sentence>Similarity is measured by character type transitions , as in the case of table column recognition .</sentence>
				<definiendum id="0">Similarity</definiendum>
				<definiens id="0">measured by character type transitions</definiens>
			</definition>
			<definition id="5">
				<sentence>Ci ' , n , where n is the number of vlines of the table .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of vlines of the table</definiens>
			</definition>
			<definition id="6">
				<sentence>Let B 448 Feature Description F1 cl , j is a space character and ci , j is a space character F2 F3 F4 ci , ,j is an alphanumeric character or a special character , and ci , j is a space character ci , ,j is a space character , and ci , j is an alphanumeric character or a special character kin Table 3 : Feature values for table row be the number of Mines identified by the program as being part of some table .</sentence>
				<definiendum id="0">,j</definiendum>
				<definiens id="0">an alphanumeric character or a special character , and ci</definiens>
				<definiens id="1">a space character ci</definiens>
				<definiens id="2">a space character , and ci</definiens>
				<definiens id="3">an alphanumeric character or a special character kin Table 3 : Feature values for table row be the number of Mines identified by the program as being part of some table</definiens>
			</definition>
			<definition id="7">
				<sentence>A Mine is considered part of a table if at least one character of Mine is not a space character and if any of the following conditions is met : * The ratio of the position of the first non-space character in hline to the length of hline exceeds some pre-determined threshold ( 0.25 ) • Hline consists entirely of one special character .</sentence>
				<definiendum id="0">Mine</definiendum>
				<definiens id="0">considered part of a table if at least one character of Mine is not a space character and if any of the following conditions is met : * The ratio of the position of the first non-space character in hline to the length of hline exceeds some pre-determined threshold ( 0.25 ) • Hline consists entirely of one special character</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>MAGIC utilizes co-ordinated text , speech , and graphics to convey information about a patient 's status after coronary bypass surgery ; it generates concise but complex descriptions that frequently involve four or more premodifiers in the same noun phrase .</sentence>
				<definiendum id="0">MAGIC</definiendum>
				<definiens id="0">utilizes co-ordinated text , speech , and graphics to convey information about a patient 's status after coronary bypass surgery ; it generates concise but complex descriptions that frequently involve four or more premodifiers in the same noun phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>A simplex NP is a maximal noun phrase that includes premodifiers such as determiners and possessives but not post-nominal constituents such as prepositional phrases or relative clauses .</sentence>
				<definiendum id="0">simplex NP</definiendum>
				<definiens id="0">a maximal noun phrase that includes premodifiers such as determiners and possessives but not post-nominal constituents such as prepositional phrases or relative clauses</definiens>
			</definition>
			<definition id="2">
				<sentence>This data sparseness problem is exacerbated by the inevitable occurrence of errors during the data extraction process , which will introduce some spurious pairs ( and orderings ) of premodifiers .</sentence>
				<definiendum id="0">extraction process</definiendum>
				<definiens id="0">will introduce some spurious pairs ( and orderings ) of premodifiers</definiens>
			</definition>
			<definition id="3">
				<sentence>We assign to an edge ( A , B ) the negative logarithm of the probability that A precedes B ; probabilities are estimated as in the previous paragraph .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the negative logarithm of the probability that A precedes B ; probabilities are estimated as in the previous paragraph</definiens>
			</definition>
			<definition id="4">
				<sentence>Note that if A and B occur sometimes as A -~ B and some139 Corpus Test pairs Medical/ adjectives 27,670 Financial/ adjectives 9,925 Medical/ adjectives 74,664 and nouns Financial/ adjectives 62,383 and nouns Direct evidence Transitivity Transitivity ( maxomin ) ( min-plus ) 92.67 % ( 88.20 % -98.47 % ) 89.60 % ( 94.94 % -91.79 % ) 94.93 % ( 97.20 % -96.16 % ) 75.41 % ( 53.85 % -98.37 % ) 79.92 % ( 72.76 % -90.79 % ) 80.77 % ( 76.36 % -90.18 % ) 88.79 % ( 80.38 % -98.35 % ) 87.69 % ( 90.86 % -91.50 % ) 90.67 % ( 91.90 % -94.27 % ) 65.93 % ( 35.76 % -95.27 % ) 69.61 % ( 56.63 % -84.51 % ) 71.04 % ( 62.48 % -83.55 % ) Table 1 : Accuracy of direct-evidence and transitivity methods on different data strata of our test corpora .</sentence>
				<definiendum id="0">Transitivity Transitivity</definiendum>
				<definiens id="0">A -~ B and some139 Corpus Test pairs Medical/ adjectives 27,670 Financial/ adjectives 9,925 Medical/ adjectives 74,664 and nouns Financial/ adjectives 62,383 and nouns Direct evidence</definiens>
				<definiens id="1">Accuracy of direct-evidence and transitivity methods on different data strata of our test corpora</definiens>
			</definition>
			<definition id="5">
				<sentence>Introduction to WordNet : An OnLine LexicM Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1082">
			<definition id="0">
				<sentence>The visibility of configurable options ( different levels of detail ) is adjustable along a simple gradient via the automatically generated user interface ( Edwards , Forthcoming ) .</sentence>
				<definiendum id="0">visibility of configurable options</definiendum>
			</definition>
			<definition id="1">
				<sentence>in a framebased data model 615 This type of architecture has been implemented , classically , as a 'blackboard ' system such as Hearsay-II ( Erman , 1980 ) , where intermodule communication takes place through a shared knowledge structure ; or as a 'messagepassing ' system where the modules communicate directly .</sentence>
				<definiendum id="0">intermodule communication</definiendum>
				<definiens id="0">takes place through a shared knowledge structure</definiens>
			</definition>
			<definition id="2">
				<sentence>For telegraphic text compression , we estimate E ( w ) , the information value of a word , based on a wide range of different information sources ( Fig.2.1 shows a subset of our working system ) .</sentence>
				<definiendum id="0">E ( w )</definiendum>
			</definition>
			<definition id="3">
				<sentence>Modules communicate by reading and writing information to and from a central database .</sentence>
				<definiendum id="0">Modules</definiendum>
				<definiens id="0">communicate by reading and writing information to and from a central database</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>Rambow , Wier and Vijay-Shanker ( Rainbow et al. , 1995 ) point out the differences between TAG derivation structures and semantic or predicateargument dependencies , and Joshi and VijayShanker ( Joshi and Vijay-Shanker , 1999 ) describe a monotonic compositional semantics based on attachment order that represents the desired dependencies of a derivation without underspecifying predicate-argument relationships at any stage .</sentence>
				<definiendum id="0">Rambow , Wier</definiendum>
				<definiendum id="1">Vijay-Shanker</definiendum>
			</definition>
			<definition id="1">
				<sentence>Since the number of possible values for the additional predicate variable field is bounded by n , where n is the number of lexical items in the input sentence , and none of the productions combine more than one predicate variable , the complexity of the dependency transducing algorithm is O ( nT ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of lexical items in the input sentence , and none of the productions combine more than one predicate variable , the complexity of the dependency</definiens>
			</definition>
			<definition id="2">
				<sentence>The resulting dependencies are represented graphiCally in the dependency structure below : ¢0 : supposed-to I ¢\ ] : be-able-to ( 0 ) I ¢2 : fly ( 0 ) This example is relatively straightforward , simply reversing the direction of adjunction dependencies as described in ( Candito and Kahane , 1998a ) , but this algorithm can transduce 91 the correct isomorphic dependency structure for the Portuguese derivation as well , similar to the distributed derivation tree in Candito and Kahane 's example 5b , `` Paul claims Mary seems to adore hot dogs , '' ( Rambow et al. , 1995 ) , where there is no edge corresponding to the dependency between the raising and bridge verbs : c~ : voar 81 : ~-capaz-de ( VP ) ~2 : fi-pressuposto-que ( S ) We begin by adjoining ~1 : g-capaz-de at node VP of c~ : voar , which produces the dependency ( ~-capaz-de , 0 , voar ) , just as before .</sentence>
				<definiendum id="0">voar</definiendum>
				<definiens id="0">produces the dependency ( ~-capaz-de , 0 , voar )</definiens>
			</definition>
			<definition id="3">
				<sentence>Here , as in a parsing algorithm , we define forest items as tuples of ( ~/¢ , 'q , _1_ , i , j , X ) where a , ~ , and 7 are elementary trees with node'O , ¢ and ¢ are predicates , X and w be predicate variables , and T and _1_ are delimiters tbr opening and closing adjunction , but now let i , j , and k refer to the indices on the scoping enumeration described above , instead of on an input string .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">elementary trees with node'O , ¢ and ¢ are predicates ,</definiens>
			</definition>
</paper>

		<paper id="1072">
			<definition id="0">
				<sentence>Revision here applies to generic and topic-related informative summaries , intended for publishing and dissemination .</sentence>
				<definiendum id="0">Revision</definiendum>
				<definiens id="0">applies to generic and topic-related informative summaries , intended for publishing and dissemination</definiens>
			</definition>
			<definition id="1">
				<sentence>mary is the match score of s 's best-matching summary sentence , where the match score is the percentage of content word occurrences in s that are also found in the summary sentence .</sentence>
				<definiendum id="0">mary</definiendum>
				<definiendum id="1">score</definiendum>
				<definiens id="0">the match score of s 's best-matching summary sentence , where the match</definiens>
				<definiens id="1">the percentage of content word occurrences in s that are also found in the summary sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>Informativeness using V7 is measured by V71° normalized for compression as : sl nV7 = V7 * ( 1 ~-~ ) ( 1 ) where sl is summary length and sO is the source length .</sentence>
				<definiendum id="0">sl</definiendum>
				<definiendum id="1">sO</definiendum>
				<definiens id="0">summary length and</definiens>
			</definition>
			<definition id="3">
				<sentence>Coherence disfiuencies do occur ; for example , since we do n't resolve possessive pronouns or plural definites , we can get infelicitous revisions like `` A computer virus , which entered , their computers through ARPANET , infected systems from MIT . ''</sentence>
				<definiendum id="0">Coherence disfiuencies</definiendum>
				<definiendum id="1">computer virus</definiendum>
				<definiens id="0">entered , their computers through ARPANET</definiens>
			</definition>
			<definition id="4">
				<sentence>The FOG index sums the average sentence length with the percentage of words over 3 syllables , with a `` grade '' level over 12 indicating difficulty for the average reader .</sentence>
				<definiendum id="0">FOG index</definiendum>
				<definiens id="0">sums the average sentence length with the percentage of words over 3 syllables</definiens>
			</definition>
			<definition id="5">
				<sentence>`` SRA : Description of the SRA System as Used for MUC-6 '' , Proceedings of the Sixth Message Understanding Conference ( MUC-6 ) , Columbia , Maryland , November 1995 .</sentence>
				<definiendum id="0">SRA</definiendum>
				<definiens id="0">Description of the SRA System as Used for MUC-6 ''</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>Reading comprehension tests can serve as a testbed , providing an impetus for research in a number of areas : • Machine learning of lexical information , including subcategorization frames , semantic relations between words , and pragmatic import of particular words .</sentence>
				<definiendum id="0">Reading comprehension tests</definiendum>
				<definiens id="0">semantic relations between words , and pragmatic import of particular words</definiens>
			</definition>
			<definition id="1">
				<sentence>Pro is automatic name and personal pronoun coreference .</sentence>
				<definiendum id="0">Pro</definiendum>
				<definiens id="0">automatic name and personal pronoun coreference</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally , reading comprehension is a task that is sufficiently close to information extraction applications such as ad hoc question answering , fact verification , situation tracking , and document summarization , that improvements on the reading comprehension evaluations will result in improved systems for these applications .</sentence>
				<definiendum id="0">reading comprehension</definiendum>
				<definiens id="0">a task that is sufficiently close to information extraction applications such as ad hoc question answering , fact verification , situation tracking , and document summarization</definiens>
			</definition>
			<definition id="3">
				<sentence>Computer Intensive methods for Testing Hypotheses .</sentence>
				<definiendum id="0">Computer Intensive</definiendum>
			</definition>
</paper>

		<paper id="1052">
			<definition id="0">
				<sentence>A parsing task represents the combination of a passive chart edge and an active chart edge or a rule .</sentence>
				<definiendum id="0">parsing task</definiendum>
				<definiens id="0">the combination of a passive chart edge and an active chart edge or a rule</definiens>
			</definition>
			<definition id="1">
				<sentence>E.g. , a prepositional phrase can be a complete utterance expressing an answer to a question ( On Monday . )</sentence>
				<definiendum id="0">E.g.</definiendum>
				<definiens id="0">a prepositional phrase can be a complete utterance expressing an answer to a question ( On Monday</definiens>
			</definition>
			<definition id="2">
				<sentence>Finally , Adj consists of all vertices adjacent to a given vertex ( we use an adjacency-list representation ) .</sentence>
				<definiendum id="0">Adj</definiendum>
				<definiens id="0">consists of all vertices adjacent to a given vertex ( we use an adjacency-list representation )</definiens>
			</definition>
			<definition id="3">
				<sentence>The scoring function for edges takes into account their length , the coverage of the edge , the number of component edges it consists of , and the confidence value for the operation which created it .</sentence>
				<definiendum id="0">scoring function</definiendum>
			</definition>
			<definition id="4">
				<sentence>Word graphs : An efficient interface between continuous-speech recognition and language understanding .</sentence>
				<definiendum id="0">Word graphs</definiendum>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>CommandTalk consists of independent , cooperating agents interacting through SRI 's Open Agent Architecture ( OAA ) ( Martin et al. , 1998 ) .</sentence>
				<definiendum id="0">CommandTalk</definiendum>
			</definition>
			<definition id="1">
				<sentence>Nuance accepts language models written in a Grammar Specification Language ( GSL ) format that allows context-free , as well as the more commonly used finite-state , models .</sentence>
				<definiendum id="0">Nuance</definiendum>
				<definiens id="0">accepts language models written in a Grammar Specification Language ( GSL ) format that allows context-free</definiens>
			</definition>
			<definition id="2">
				<sentence>CommandTalk uses a dialogue stack to keep track of the current discourse context .</sentence>
				<definiendum id="0">CommandTalk</definiendum>
			</definition>
			<definition id="3">
				<sentence>Also , CommandTalk provides the same language capabilities for user and system utterances .</sentence>
				<definiendum id="0">CommandTalk</definiendum>
				<definiens id="0">provides the same language capabilities for user and system utterances</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>The Construct Algebra provides the building blocks needed to create new dialog motivators and analyze them .</sentence>
				<definiendum id="0">Construct Algebra</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Construct Algebra allows a designer to add new motivators in a principled way .</sentence>
				<definiendum id="0">Construct Algebra</definiendum>
				<definiens id="0">allows a designer to add new motivators in a principled way</definiens>
			</definition>
			<definition id="2">
				<sentence>The SLU module extracts the meaning of the user 's utterance and produces a list of possible objects with associated confidence scores that is interpreted by the dialog manager .</sentence>
				<definiendum id="0">SLU module</definiendum>
				<definiens id="0">extracts the meaning of the user 's utterance and produces a list of possible objects with associated confidence scores that is interpreted by the dialog manager</definiens>
			</definition>
			<definition id="3">
				<sentence>The DIAL_FOR_ME construct is the head and it has two constructs for its body , FORWARD_NUMBER and BILLING .</sentence>
				<definiendum id="0">DIAL_FOR_ME construct</definiendum>
				<definiens id="0">the head and it has two constructs for its body</definiens>
			</definition>
			<definition id="4">
				<sentence>The Construct Algebra defines six relations in the set of constructs .</sentence>
				<definiendum id="0">Construct Algebra</definiendum>
				<definiens id="0">defines six relations in the set of constructs</definiens>
			</definition>
			<definition id="5">
				<sentence>Definition 4 Restriction Cl is a restriction of c2 , denoted cl C c~ , when head ( c1 ) = head ( c2 ) and ( 3f : body ( c1 ) -- + body ( c2 ) ) ( fis 1 to 1 A ( Vbl • body ( cl ) ) ( bl C_ f ( bl ) ) Intuitively , cl can be obtained by `` pruning '' elements of c2 .</sentence>
				<definiendum id="0">Restriction Cl</definiendum>
				<definiens id="0">a restriction of c2 , denoted cl C c~ , when head ( c1 ) = head ( c2 ) and ( 3f : body ( c1 ) -- + body ( c2 ) ) ( fis 1 to 1 A ( Vbl • body ( cl ) ) ( bl C_ f ( bl ) ) Intuitively , cl can be obtained by `` pruning '' elements of c2</definiens>
			</definition>
			<definition id="6">
				<sentence>Intuitively , c2 is an ancestor of Cl or in object-oriented C ~ .</sentence>
				<definiendum id="0">c2</definiendum>
				<definiens id="0">an ancestor of Cl or in object-oriented C ~</definiens>
			</definition>
			<definition id="7">
				<sentence>BILLING is a generalization of CALLING_CARD , or in other words CALLING_CARD is-a BILLING .</sentence>
				<definiendum id="0">BILLING</definiendum>
			</definition>
			<definition id="8">
				<sentence>Definition 7 Symmetric Generalization Cl is a symmetric generalization of c2 , denoted cl ~ c2 , when C1¢ -- - &gt; C2 or c2¢ -- -~Cl This definition simply removes the directionality of __¢ -- -~ .</sentence>
				<definiendum id="0">Symmetric Generalization Cl</definiendum>
				<definiens id="0">a symmetric generalization of c2</definiens>
			</definition>
			<definition id="9">
				<sentence>CARD_NUMBER 8485417 BILLING Cl c2 Figure 5 : cl ¢ -- &gt; c2 or ; ; c2 is-a c1 '' Definition 8 Containment Generalization Cl is a containment generalization of c2 , denoted ci ¢ -- - &gt; c2 , when b2 is contained in c2 and cl is a symmetric generalization of b2 .</sentence>
				<definiendum id="0">Containment Generalization Cl</definiendum>
				<definiendum id="1">cl</definiendum>
				<definiens id="0">a containment generalization of c2 , denoted ci ¢ -- - &gt; c2 , when b2 is contained in c2 and</definiens>
				<definiens id="1">a symmetric generalization of b2</definiens>
			</definition>
			<definition id="10">
				<sentence>BILLING is contained in DIAL_FOR_ME and is a symmetric generalization of CALLING_CARD .</sentence>
				<definiendum id="0">BILLING</definiendum>
				<definiens id="0">a symmetric generalization of CALLING_CARD</definiens>
			</definition>
			<definition id="11">
				<sentence>The Construct Algebra consists of two operations union , U and projection , \ .</sentence>
				<definiendum id="0">Construct Algebra</definiendum>
				<definiens id="0">consists of two operations union , U and projection , \</definiens>
			</definition>
			<definition id="12">
				<sentence>Definition 9.1 Union of values ( vl U v2 ) V 1 U V 2 = Vl , Vl = v2 and vl # NULL v2 , Vl = v2 and Vl = NULL not defined , Vl # v2 Recall that by definition , NULL is equal to any other value .</sentence>
				<definiendum id="0">NULL</definiendum>
				<definiens id="0">equal to any other value</definiens>
			</definition>
			<definition id="13">
				<sentence>The first part is a set of unions ( denoted f ( b2 ) U b2 in the definition above ) where b2 spans the body of the second operand c2 and f is a mapping from Definition 6 .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">a set of unions ( denoted f ( b2 ) U b2 in the definition above ) where b2 spans the body of the second operand</definiens>
			</definition>
			<definition id="14">
				<sentence>J Cl , ( ( REP , NULL ) , { cl , c2 } ) , C1 ~ C2 e2 ~ el Cl ~ C2 and C2 ~ Cl In this definition REP is a construct used to represent the union of those constructs that do not satisfy any of the aforementioned conditions .</sentence>
				<definiendum id="0">REP</definiendum>
			</definition>
			<definition id="15">
				<sentence>VPQ uses two additional motivators , they are continuation and 196 co : Construct used for disambiguation , cQ Ec CA : User response Dk ( c , cigK ) = c , c ~ AMBIGUITY Dk+l ( c , CIDK ) , CA ~__~_ERROR Dk+l ( C , CID g ( .</sentence>
				<definiendum id="0">VPQ</definiendum>
				<definiens id="0">uses two additional motivators , they are continuation and 196 co : Construct used for disambiguation</definiens>
			</definition>
			<definition id="16">
				<sentence>If however , CA is in the containment generalization relation with c then the projection operation is applied and the result is returned .</sentence>
				<definiendum id="0">CA</definiendum>
				<definiens id="0">in the containment generalization relation with c then the projection operation is applied</definiens>
			</definition>
</paper>

		<paper id="1043">
			<definition id="0">
				<sentence>A mixed language query consists of words in a primary language and a secondary language .</sentence>
				<definiendum id="0">mixed language query</definiendum>
				<definiens id="0">consists of words in a primary language and a secondary language</definiens>
			</definition>
			<definition id="1">
				<sentence>Mutual information is a good measure of the co-occurrence relationship between two words ( Gale and Church , 1993 ) .</sentence>
				<definiendum id="0">Mutual information</definiendum>
			</definition>
			<definition id="2">
				<sentence>We first compute the mutual information between any word pair from a monolingual corpus in the primary language 2 1In actual experiments , each sentence can contain multiple secondary language words 2This corpus does not need to be in the same domain as the testing data using the following formula , where E is a word and f ( E ) is the frequency of word E. MI ( Ei , Ej ) = log f ( Ei , Ej ) f ( Ei ) * f ( Sj ) ( 1 ) Ei and Ej can be either neighboring words or any two words in the sentence .</sentence>
				<definiendum id="0">E</definiendum>
				<definiendum id="1">f ( E</definiendum>
				<definiens id="0">a word and</definiens>
				<definiens id="1">log f ( Ei , Ej ) f ( Ei ) * f ( Sj ) ( 1 ) Ei and Ej can be either neighboring words or any two words in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Suppose there are n primary language words in S = E1 , E2 , ... , C , ... , En , as shown in Figure 2 , we compute mutual information scores between all Ec~ and all Ej where Eci is one of the translation candidates for C and Ej is one of all n words in S. A mutual information score matrix is shown in Table 1 .</sentence>
				<definiendum id="0">Eci</definiendum>
				<definiens id="0">one of the translation candidates for C and Ej is one of all n words in S. A mutual information</definiens>
			</definition>
			<definition id="4">
				<sentence>whereMIjc~ is the mutual information score between contextual word Ej and translation candidate Eel .</sentence>
				<definiendum id="0">whereMIjc~</definiendum>
			</definition>
			<definition id="5">
				<sentence>Referring again to Table 1 , Mljci is the mutual information score between contextual word Ej and translation candidate Ec~ .</sentence>
				<definiendum id="0">Mljci</definiendum>
				<definiens id="0">the mutual information score between contextual word Ej and translation candidate Ec~</definiens>
			</definition>
			<definition id="6">
				<sentence>Translation accuracy is the ratio of the number of secondary language ( Chinese ) words disambiguated correctly over the number of all 337 secondary language ( Chinese ) words present in the testing sentences .</sentence>
				<definiendum id="0">Translation accuracy</definiendum>
				<definiens id="0">the ratio of the number of secondary language ( Chinese ) words disambiguated correctly over the number of all 337 secondary language ( Chinese ) words present in the testing sentences</definiens>
			</definition>
			<definition id="7">
				<sentence>A mixed-language sentence consists of words mostly in a primary language and some in a secondary language .</sentence>
				<definiendum id="0">mixed-language sentence</definiendum>
				<definiens id="0">consists of words mostly in a primary language and some in a secondary language</definiens>
			</definition>
</paper>

		<paper id="1034">
			<definition id="0">
				<sentence>The implemented morphological analyzer provides the user with more detailed category information ( lexical , morpho-syntactic , semantic , etc. ) according to the case illustrated by Example 4 ( see next page ) .</sentence>
				<definiendum id="0">morphological analyzer</definiendum>
				<definiens id="0">provides the user with more detailed category information ( lexical , morpho-syntactic , semantic</definiens>
			</definition>
			<definition id="1">
				<sentence>Thus stemming is a combination of the morphological analysis and a post-processing phase where the actual stems ( lexical forms ) are extracted from the analysis resuits .</sentence>
				<definiendum id="0">Thus stemming</definiendum>
				<definiens id="0">a combination of the morphological analysis and a post-processing phase where the actual stems ( lexical forms ) are extracted from the analysis resuits</definiens>
			</definition>
			<definition id="2">
				<sentence>Choice is a task of either the end-user or a disambiguator module that is based on the context of the word .</sentence>
				<definiendum id="0">Choice</definiendum>
				<definiens id="0">a task of either the end-user or a disambiguator module that is based on the context of the word</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>Cross language information retrieval ( CLIR ) ( Oard and Dorr , 1996 ; Oard , 1997 ) deals with the use of queries in one language to access documents in another .</sentence>
				<definiendum id="0">Cross language information retrieval</definiendum>
				<definiendum id="1">CLIR</definiendum>
				<definiens id="0">deals with the use of queries in one language to access documents in another</definiens>
			</definition>
			<definition id="1">
				<sentence>Hybrid approaches ( Ballesteros and Croft , 1998 ; Bian and Chen , 1998 ; Davis 1997 ) integrate both lexical and corpus knowledge .</sentence>
				<definiendum id="0">Hybrid approaches</definiendum>
				<definiens id="0">integrate both lexical and corpus knowledge</definiens>
			</definition>
			<definition id="2">
				<sentence>Translation ambiguity results from the source language , and target polysemy occurs in target language .</sentence>
				<definiendum id="0">Translation ambiguity</definiendum>
				<definiens id="0">results from the source language , and target polysemy occurs in target language</definiens>
			</definition>
			<definition id="3">
				<sentence>They are determined by the following formula , where n is number of words in Q and mk is the number of words in a restriction for Ek .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">mk</definiendum>
				<definiens id="0">the number of words in a restriction for Ek</definiens>
			</definition>
			<definition id="4">
				<sentence>~- ( income ) : I~g~_N ( N : quota ) ~ ( tax ) : i/~_V ( N : evasion ) , I~_N ( N : surtax ) , ~t ~ , _N ( N : surplus ) , , g'~_N ( N : sales tax ) Augmented translation restrictions ( poundage , scot , stay ) , ( quota ) , and ( evasion , surtax , surplus , sales tax ) are added to `` evasion '' , `` income '' , and `` tax '' , respectively .</sentence>
				<definiendum id="0">surtax</definiendum>
				<definiens id="0">sales tax ) Augmented translation restrictions ( poundage , scot , stay )</definiens>
			</definition>
</paper>

		<paper id="1076">
			<definition id="0">
				<sentence>1 , Brief presentation of LTAGs A LTAG consists of a finite set of elementary trees of finite depth .</sentence>
				<definiendum id="0">LTAG</definiendum>
				<definiens id="0">consists of a finite set of elementary trees of finite depth</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>In an effort to develop measures of discourse level management strategies , this study examines a measure of the degree to which decisionmaking interactions consist of sequences of utterance functions that are linked in a decisionmaking routine .</sentence>
				<definiendum id="0">discourse level management strategies</definiendum>
				<definiens id="0">a measure of the degree to which decisionmaking interactions consist of sequences of utterance functions that are linked in a decisionmaking routine</definiens>
			</definition>
			<definition id="1">
				<sentence>Default principles associated with routines can determine the encoding of these routine functions in sequences of utterances .</sentence>
				<definiendum id="0">Default principles</definiendum>
				<definiens id="0">associated with routines can determine the encoding of these routine functions in sequences of utterances</definiens>
			</definition>
			<definition id="2">
				<sentence>Scores for infrequent move and response functions , In the initial study , the 16 face-to-face interactions produced a corpus of 4141 utterances ( ave. 259 per discourse ) , while the 16 computer-mediated interactions consisted of 918 utterances ( ave. 57 ) .</sentence>
				<definiendum id="0">In</definiendum>
				<definiens id="0">the initial study , the 16 face-to-face interactions produced a corpus of 4141 utterances ( ave. 259 per discourse ) , while the 16 computer-mediated interactions consisted of 918 utterances</definiens>
			</definition>
			<definition id="3">
				<sentence>Orientation ~ Suggestion~Agre_ement Figure 3 : A More Complex Decision Routine Based on Frequency Analyses Examination of the 2ha-order analyses in the original study revealed that all of the 7 most frequent sequences of 3 utterances trace a path in the model in Figure 3 .</sentence>
				<definiendum id="0">Orientation</definiendum>
				<definiens id="0">A More Complex Decision Routine Based on Frequency Analyses Examination of the 2ha-order analyses in the original study revealed that all of</definiens>
			</definition>
</paper>

		<paper id="1077">
			<definition id="0">
				<sentence>Much of the relevant linguistic literature is indebted to Halliday and Hasan ( 1976 ) , where cohesion is defined as a network of relationships between locations in the text , arising from ( i ) grammatical factors ( co-reference , use of pro-forms , ellipsis and sentential connectives ) , and ( ii ) lexical factors ( reiteration and collocation ) .</sentence>
				<definiendum id="0">cohesion</definiendum>
				<definiendum id="1">collocation</definiendum>
				<definiens id="0">a network of relationships between locations in the text , arising from ( i ) grammatical factors ( co-reference , use of pro-forms , ellipsis and sentential connectives ) , and ( ii ) lexical factors ( reiteration and</definiens>
			</definition>
			<definition id="1">
				<sentence>Topic parsing ( Hahn , 1990 ) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge More general approaches assess word mmllanty based on thesauri ( Morris and Hirst , 1991 ) or dictionary definitions ( Kozima , 1994 ) .</sentence>
				<definiendum id="0">Topic parsing</definiendum>
				<definiens id="0">utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge More general approaches assess word mmllanty based on thesauri ( Morris and Hirst , 1991 ) or dictionary definitions</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>We use LH R M L to denote the 1 available at http : //www.cs.umanitoba.ca/-lindek/minipar.htm/ 2available at http : //www.cs.umanitob &amp; .ca/-lindek/nlldemo.htm/ 3available at http : //www.cs.umanitoba.ca/-lindek/nlldemo.htm/ 317 frequency count of all the collocations that match the pattern ( H R M ) , where H and M are either words or the wild card ( * ) and R is either a dependency type or the wild card .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">//www.cs.umanitoba.ca/-lindek/minipar.htm/ 2available at http : //www.cs.umanitob &amp; .ca/-lindek/nlldemo.htm/ 3available at http : //www.cs.umanitoba.ca/-lindek/nlldemo.htm/ 317 frequency count of all the collocations that match the pattern ( H R M ) , where H and M are either words or the wild card ( *</definiens>
			</definition>
			<definition id="1">
				<sentence>To compute the mutual information in a collocation , we treat a collocation ( head type modifier ) as the conjunction of three events : A : ( * type * ) B : ( head * * ) C : ( * * modifier ) The mutual information of a collocation is the logarithm of the ratio between the probability of the collocation and the probability of events A , B , and C co-occur if we assume B and C are conditionally independent given A : ( 2 ) mutualInfo ( head , type , modifier ) P ( A , B , c ) = log P ( B\ [ A ) P ( C\ [ A ) P ( A ) \ [ head type modifier\ [ * * *\ ] = log ( \ [ , type *\ [ \ [ head type *\ [ \ [ * t~Te modifier\ [ ) \ [ * * *\ [ \ [ * type *1 \ [ *type *1 • , \ ] head type modifier\ [ x * type * -- -log , \ ] head type * x * type modifier / Collocations In this section , we use several examples to demonstrate the basic idea behind our algorithm .</sentence>
				<definiendum id="0">C co-occur</definiendum>
				<definiendum id="1">P ( B\ [ A ) P</definiendum>
				<definiens id="0">A : ( * type * ) B : ( head * * ) C : ( * * modifier ) The mutual information of a collocation is the logarithm of the ratio between the probability of the collocation and the probability of events A , B , and</definiens>
			</definition>
			<definition id="2">
				<sentence>Our evaluation sample consists of 5 most frequent open class words in the our parsed corpus : { have , company , make , do , take } and 5 words whose frequencies are ranked from 2000 to 2004 : { path , lock , resort , column , gulf } .</sentence>
				<definiendum id="0">evaluation sample</definiendum>
				<definiens id="0">consists of 5 most frequent open class words in the our parsed corpus : { have , company , make , do , take } and 5 words whose frequencies are ranked from 2000 to 2004 : { path , lock , resort , column , gulf }</definiens>
			</definition>
			<definition id="3">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>This paper presents incremental significantutterance-sequence search ( ISSS ) , a method that 200 enables incremental understanding of user utterances word by word by finding plausible sequences of utterances that play crucial roles in the task execution of dialogues .</sentence>
				<definiendum id="0">ISSS</definiendum>
				<definiens id="0">a method that 200 enables incremental understanding of user utterances word by word by finding plausible sequences of utterances that play crucial roles in the task execution of dialogues</definiens>
			</definition>
			<definition id="1">
				<sentence>For incremental understanding , we propose incremental significant-utterance-sequence search ( ISSS ) , which is an integrated parsing and discourse processing method .</sentence>
				<definiendum id="0">ISSS</definiendum>
				<definiens id="0">an integrated parsing and discourse processing method</definiens>
			</definition>
			<definition id="2">
				<sentence>A significant utterance ( SU ) in the user 's speech is a phrase that plays a crucial role in performing the task in the dialogue .</sentence>
				<definiendum id="0">significant utterance ( SU</definiendum>
				<definiens id="0">a phrase that plays a crucial role in performing the task in the dialogue</definiens>
			</definition>
			<definition id="3">
				<sentence>SU is defined as a syntactic category by the grammar for linguistic processing , which includes semantic inference rules .</sentence>
				<definiendum id="0">SU</definiendum>
				<definiens id="0">a syntactic category by the grammar for linguistic processing , which includes semantic inference rules</definiens>
			</definition>
			<definition id="4">
				<sentence>As an example of SUs across pauses , `` gozen-jftji kara gozen-jaichiji made ( from 10 a.m. to 11 a.m. ) '' in U5 and U7 IA bunsetsu phrase is a phrase that consists of one content word and a number ( possibly zero ) of function words .</sentence>
				<definiendum id="0">IA bunsetsu phrase</definiendum>
				<definiens id="0">a phrase that consists of one content word and a number ( possibly zero ) of function words</definiens>
			</definition>
			<definition id="5">
				<sentence>S means a system utterance and U a user utterance .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">means a system utterance</definiens>
			</definition>
			<definition id="6">
				<sentence>The word error rate is defined as 100 * ( substitutions + deletions + insertions ) / ( correct + substitutions + deletions ) ( Zechner and Waibel , 1998 ) .</sentence>
				<definiendum id="0">word error rate</definiendum>
			</definition>
</paper>

		<paper id="1069">
			<definition id="0">
				<sentence>Each feature f maps a syntactic analysis w E ~ to a real value f ( w ) .</sentence>
				<definiendum id="0">feature f</definiendum>
				<definiens id="0">maps a syntactic analysis w E ~ to a real value f ( w )</definiens>
			</definition>
			<definition id="1">
				<sentence>Log-linear models are models in which the log probability is a linear combination of feature values ( plus a constant ) .</sentence>
				<definiendum id="0">Log-linear models</definiendum>
				<definiens id="0">a linear combination of feature values ( plus a constant )</definiens>
			</definition>
			<definition id="2">
				<sentence>ojlj ( ~o ) Zo Zo -- -- Z eZJ=l ... ...</sentence>
				<definiendum id="0">ojlj</definiendum>
			</definition>
			<definition id="3">
				<sentence>- , Wn is a training corpus of n syntactic analyses .</sentence>
				<definiendum id="0">Wn</definiendum>
			</definition>
			<definition id="4">
				<sentence>Letting fj ( ~ ) = ~i=l , ... , n fJ ( wi ) , the log likelihood of the corpus and its derivatives are : logL0 ( ~ ) = ~ Ojfj ( ~ ) -nlogZo ( 2 ) j=l , ... , m 0 log L0 ( ~ ) nEd/j ) ( 3 ) ooj where Eo ( fj ) is the expected value of fj under the distribution determined by the parameters which maximize log Lo ( ~ ) .</sentence>
				<definiendum id="0">... , n fJ</definiendum>
				<definiendum id="1">Eo ( fj )</definiendum>
			</definition>
			<definition id="5">
				<sentence>This motivates an alternative strategy involving a data-based estimate of E0 ( fj ) : Ee ( fj ) = Ee ( Ee ( fj ( w ) ly ( w ) ) ) ( 4 ) 1 = ~ Ea ( fj ( w ) ly ( w ) =yd ( 5 ) 72 i=l , ... , n where y ( w ) is the yield belonging to the syntactic analysis w , and Yi = y ( wi ) is the yield belonging to the i'th sample in the training corpus .</sentence>
				<definiendum id="0">y</definiendum>
				<definiendum id="1">wi )</definiendum>
				<definiens id="0">the yield belonging to the syntactic analysis w , and Yi = y (</definiens>
				<definiens id="1">the yield belonging to the i'th sample in the training corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>In fact , if f~ ( y ) is the set of well-formed syntactic structures that have yield y ( i.e. , the set of possible parses of the string y ) , then Eo ( fj ( o ) ly ( , ) = = Ew'Ef~ ( yi ) f J ( w ' ) e~-~k=x ... ...</sentence>
				<definiendum id="0">y )</definiendum>
				<definiens id="0">the set of well-formed syntactic structures that have yield y ( i.e. , the set of possible parses of the string y )</definiens>
			</definition>
			<definition id="7">
				<sentence>Many applications in spatial statistics , involving Markov random fields ( MRF ) , are of this nature as well .</sentence>
				<definiendum id="0">MRF</definiendum>
				<definiens id="0">Many applications in spatial statistics , involving Markov random fields</definiens>
			</definition>
			<definition id="8">
				<sentence>Thus instead of maximizing the log pseudo-likelihood , we choose 0 to maximize /3z 2 log PL0 ( ~ ) ~ 2avJ2 ( 9 ) j=l , ... , m J log linear models The pseudo-likelihood estimator described in the last section finds parameter values which maximize the conditional probabilities of the observed parses ( syntactic analyses ) given the observed sentences ( yields ) in the training corpus .</sentence>
				<definiendum id="0">pseudo-likelihood estimator</definiendum>
				<definiens id="0">described in the last section finds parameter values which maximize the conditional probabilities of the observed parses ( syntactic analyses ) given the observed sentences ( yields ) in the training corpus</definiens>
			</definition>
			<definition id="9">
				<sentence>Co ( ~ ) is a highly discontinuous function of 0 , and most conventional optimization algorithms perform poorly on it .</sentence>
				<definiendum id="0">Co</definiendum>
			</definition>
			<definition id="10">
				<sentence>C ( ~test ) is the number of maximum likelihood parses of the test corpus that were the correct parses , and -log PL ( wtest ) is the negative logarithm of the pseudo-likelihood of the test corpus .</sentence>
				<definiendum id="0">C ( ~test )</definiendum>
				<definiendum id="1">-log PL ( wtest )</definiendum>
				<definiens id="0">the number of maximum likelihood parses of the test corpus that were the correct parses</definiens>
				<definiens id="1">the negative logarithm of the pseudo-likelihood of the test corpus</definiens>
			</definition>
</paper>

		<paper id="1083">
			<definition id="0">
				<sentence>She finds that when a FP occurs at a major phrase or discourse boundary , the FP itself is the best predictor of the following lexical material ; conversely , in a non-boundary context , FP 's are predictable from the preceding words .</sentence>
				<definiendum id="0">FP itself</definiendum>
				<definiens id="0">a major phrase or discourse boundary , the</definiens>
			</definition>
			<definition id="1">
				<sentence>Derived CONTROLLED-FP-CORPUS is a version of the finished transcriptions corpus populated stochastically with 2,665,000 FP 's based on the BIGRAMFP-LM .</sentence>
				<definiendum id="0">Derived CONTROLLED-FP-CORPUS</definiendum>
				<definiens id="0">a version of the finished transcriptions corpus populated stochastically with 2,665,000 FP 's based on the BIGRAMFP-LM</definiens>
			</definition>
			<definition id="2">
				<sentence>Condition B used a subset of the corpus that contained high frequency FP users ( FPs/Words ratio above 1.0 ) .</sentence>
				<definiendum id="0">Condition B</definiendum>
			</definition>
</paper>

		<paper id="1079">
			<definition id="0">
				<sentence>The advantages of this new model , the Left-Right Centering Algorithm ( LRC ) , lie in its incremental processing of utterances and in its low computational overhead .</sentence>
				<definiendum id="0">Left-Right Centering Algorithm</definiendum>
				<definiens id="0">lie in its incremental processing of utterances and in its low computational overhead</definiens>
			</definition>
			<definition id="1">
				<sentence>LRC favors entities near the head of the sentence under the assumption they are more salient .</sentence>
				<definiendum id="0">LRC</definiendum>
				<definiens id="0">favors entities near the head of the sentence under the assumption they are more salient</definiens>
			</definition>
			<definition id="2">
				<sentence>Coding all the algorithms allows one to quickly test them all on a large corpus and eliminates human error , both shortcomings of hand evaluation .</sentence>
				<definiendum id="0">Coding all the algorithms</definiendum>
				<definiens id="0">allows one to quickly test them all on a large corpus and eliminates human error</definiens>
			</definition>
</paper>

		<paper id="1010">
			<definition id="0">
				<sentence>For inducing grammars from sparsely labeled training data ( e.g. , only higher-level constituent labels ) , we propose an adaptation strategy , which produces grammars that parse almost as well as grammars induced from fully labeled corpora .</sentence>
				<definiendum id="0">adaptation strategy</definiendum>
				<definiens id="0">produces grammars that parse almost as well as grammars induced from fully labeled corpora</definiens>
			</definition>
			<definition id="1">
				<sentence>Induction • Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language .</sentence>
				<definiendum id="0">Induction • Grammar induction</definiendum>
				<definiens id="0">the process of inferring the structure of a language by learning from example sentences drawn from the language</definiens>
			</definition>
			<definition id="2">
				<sentence>NotBaseP is the complement of BaseP .</sentence>
				<definiendum id="0">NotBaseP</definiendum>
				<definiens id="0">the complement of BaseP</definiens>
			</definition>
			<definition id="3">
				<sentence>A typical HighP is a sentential clause or a complex noun phrase .</sentence>
				<definiendum id="0">typical HighP</definiendum>
				<definiens id="0">a sentential clause or a complex noun phrase</definiens>
			</definition>
			<definition id="4">
				<sentence>The ATIS corpus consists of 577 short sentences with simple structures , and the vocabulary set is made up of 32 • POS tags , a subset of the 47 tags used for the WSJ .</sentence>
				<definiendum id="0">ATIS corpus</definiendum>
				<definiens id="0">consists of 577 short sentences with simple structures , and the vocabulary set is made up of 32 • POS tags , a subset of the 47 tags used for the WSJ</definiens>
			</definition>
			<definition id="5">
				<sentence>Each training set consists of 3600 sentences , and 1780 sentences are used as held-out data .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">consists of 3600 sentences , and 1780 sentences are used as held-out data</definiens>
			</definition>
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>As a consequence of this versatility , the general decision of apostrophe vs. of is not trivial : Quirk claims that the higher on the gender scale , i.e. , the more animate the noun , the more the possessor realization tends to be realized as an inflected genitive : • Person 's name : Segovia 's pupil • Person 's nouns : the boy 's new shirt • Collective nouns : the nation 's social security • Higher Animals : the horse 's neck • Geographical names : Europe 's future • Locative nouns : the school 's history • Temporal nouns : the decade 's event This decision also interacts with other realization decisions : if several modifiers must be attached to the same head , they can compete for the same slot in the syntactic structure .</sentence>
				<definiendum id="0">Geographical names</definiendum>
				<definiens id="0">an inflected genitive : • Person 's name : Segovia 's pupil • Person 's nouns : the boy 's new shirt • Collective nouns : the nation 's social security • Higher Animals : the horse 's neck •</definiens>
			</definition>
			<definition id="1">
				<sentence>The partitive relation denotes a subset of the thing to which the head of a noun phrase refers .</sentence>
				<definiendum id="0">partitive relation</definiendum>
				<definiens id="0">a subset of the thing to which the head of a noun phrase refers</definiens>
			</definition>
			<definition id="2">
				<sentence>A partitive relation can be realized in two main ways : as part of the pre-determiner sequence ( Halliday , 1994 ) , ( Winograd , 1983 ) using quantifiers that have a partitive meaning ( e.g. , some/most/many/one-third ( of the ) children ) or using a construction of the form a measure/X of Y. There are three subtypes of the partitive construction ( ( Quirk et al. , 1985 ) \ [ p.130\ ] , ( Halliday , 1994 ) ) : measure a mile of cable , typical partitives a loaf of bread , a slice of cake , and general partitives : a piece/bit/of an item of X. In the syntactic structure of a partitive structure , the part is the head of the phrase ( and determines agreement ) , but the Thing is what is being measured .</sentence>
				<definiendum id="0">partitive relation</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Possessive Construction in Modern Hebrew : A Sociolinguistic Approach .</sentence>
				<definiendum id="0">Possessive Construction</definiendum>
			</definition>
</paper>

		<paper id="1006">
			<definition id="0">
				<sentence>Because the entity that licences ( or `` discharges '' ) a given presupposition usually has a source within the discourse , the presupposition seems to link the clause containing the presupposition-bearing ( p-bearing ) element to that source .</sentence>
				<definiendum id="0">presupposition-bearing</definiendum>
				<definiens id="0">a source within the discourse , the presupposition seems to link the clause containing the</definiens>
			</definition>
			<definition id="1">
				<sentence>We can represent both generalisations using the meta-level predicate , evidence ( rt , C ) , which holds iff a premise rc is evidence for a conclusion C. In Example 6d , the relevant generalisation involves possible worlds associated jointly with the modality of the first clause and `` then '' ( Webber et al. , 1999 ) .</sentence>
				<definiendum id="0">relevant generalisation</definiendum>
				<definiens id="0">involves possible worlds associated jointly with the modality of the first clause</definiens>
			</definition>
			<definition id="2">
				<sentence>7There is another sense of `` otherwise '' corresponding to `` in other respects '' , which appears either as an adjective phrase modifier ( e.g. `` He 's an otherwise happy boy . '' )</sentence>
				<definiendum id="0">7There</definiendum>
				<definiens id="0">appears either as an adjective phrase modifier</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>Introduction Cross-language information retrieval ( IR ) enables a user to retrieve documents written in diverse languages using queries expressed in his or her own language .</sentence>
				<definiendum id="0">Introduction Cross-language information retrieval</definiendum>
				<definiendum id="1">IR</definiendum>
				<definiens id="0">enables a user to retrieve documents written in diverse languages using queries expressed in his or her own language</definiens>
			</definition>
			<definition id="1">
				<sentence>Joint probabilities , p ( x , y ) , are estimated by counting the number of times , f , ( x , y ) , that x is followed by y in a window of w words and normalizing it by N. In our application of query translation , the joint cooccurrence frequency f , ( x , y ) has 6-word window size which seems to allow semantic relations of query as well as fixed expressions ( idioms such 225 as bread and butter ) .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">estimated by counting the number of times , f , ( x , y )</definiens>
			</definition>
			<definition id="2">
				<sentence>Once the value for W b is calculated , the weight for the rest of the candidates are calculated as follows : Wr _ 1 W h ( 3 ) n-1 where n is the number of candidates .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the weight for the rest of the candidates are calculated as follows</definiens>
				<definiens id="1">the number of candidates</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>Cache language models ( Kuhn and de Mori ( 1992 ) , Rosenfeld ( 1994 ) ) try to overcome this limitation by boosting the probability of the words already seen in the history ; trigger models ( Lau et al. ( 1993 ) ) , even more general , try to capture the interrelationships between words .</sentence>
				<definiendum id="0">Cache language models</definiendum>
				<definiendum id="1">trigger models</definiendum>
				<definiens id="0">try to overcome this limitation by boosting the probability of the words already seen in the history</definiens>
			</definition>
			<definition id="1">
				<sentence>The data used in this research is the Broadcast News ( BN94 ) corpus , consisting of radio and TV news transcripts form the year 1994 .</sentence>
				<definiendum id="0">BN94 )</definiendum>
			</definition>
			<definition id="2">
				<sentence>Let this evaluation function be g ( n ) , where n is a node of the tree , and suppose that we want to minimize it .</sentence>
				<definiendum id="0">n</definiendum>
			</definition>
			<definition id="3">
				<sentence>, ( where ( n # ) j_ 1 kare the children of node n ) , g ( root ) can be coml ) uted efficiently using dynamic programming 2 : where N ( Ck ) is the number of vectors ( documents ) in cluster Ck and c ( Ci ) is the centroid of the i th cluster .</sentence>
				<definiendum id="0">N ( Ck )</definiendum>
				<definiendum id="1">c ( Ci )</definiendum>
				<definiens id="0">the number of vectors ( documents ) in cluster</definiens>
				<definiens id="1">the centroid of the i th cluster</definiens>
			</definition>
			<definition id="4">
				<sentence>-°**° I I I 0. , Figure 4 : Conditional entropy for different a , cluster sizes and linkage methods where c ( w , Ci ) is the TF-IDF factor of word w in class Ci and T is the size of the corpus .</sentence>
				<definiendum id="0">c ( w</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the size of the corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>In our case , based on the assumptions made in the GoodTuring formulation , we considered that the ratio of the probability mass that goes to the unseen events and the one that goes to seen , free events should be Model fixed fixed free free Bigrsm-type Exsmple p ( FWIFW ) p ( thel~ ) p ( FWICW ) ~ , ( o.t'i.e. , ~a , 'io ) p ( CWICW ) p ( airlco/d ) n ( CWlFW ) n ( oi , .</sentence>
				<definiendum id="0">CWlFW</definiendum>
				<definiens id="0">goes to seen , free events should be Model fixed fixed free free Bigrsm-type Exsmple p ( FWIFW ) p ( thel~ ) p ( FWICW ) ~</definiens>
			</definition>
</paper>

		<paper id="1071">
			<definition id="0">
				<sentence>Sentence generation begins with phrases .</sentence>
				<definiendum id="0">Sentence generation</definiendum>
			</definition>
			<definition id="1">
				<sentence>A variety of approaches exist for determining the salient sentences in the text : statistical techniques based on word distribution ( Salton et al. , 1991 ) , symbolic techniques based on discourse structure ( Marcu , 1997 ) , and semantic relations between words ( Barzilay and Elhadad , 1997 ) .</sentence>
				<definiendum id="0">variety of approaches</definiendum>
				<definiens id="0">exist for determining the salient sentences in the text : statistical techniques based on word distribution ( Salton et al. , 1991 ) , symbolic techniques based on discourse structure ( Marcu , 1997 ) , and semantic relations between words</definiens>
			</definition>
			<definition id="2">
				<sentence>Paraphrasing is defined as alternative ways a human speaker can choose to `` say the same thing '' by using linguistic knowledge ( as opposed to world knowledge ) ( Iordanskaja et al. , 1991 ) .</sentence>
				<definiendum id="0">Paraphrasing</definiendum>
			</definition>
			<definition id="3">
				<sentence>First is the need to use learning techniques to identify paraphrasing patterns in corpus data .</sentence>
				<definiendum id="0">First</definiendum>
				<definiens id="0">the need to use learning techniques to identify paraphrasing patterns in corpus data</definiens>
			</definition>
</paper>

		<paper id="1075">
			<definition id="0">
				<sentence>LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient parsers have already been realized using this system .</sentence>
				<definiendum id="0">LiLFeS</definiendum>
				<definiens id="0">one of the fastest inference engines for processing feature structure logic , and efficient parsers have already been realized using this system</definiens>
			</definition>
			<definition id="1">
				<sentence>NODE_UNIFY shows the number of nodes for which unification of types is computed .</sentence>
				<definiendum id="0">NODE_UNIFY</definiendum>
				<definiens id="0">the number of nodes for which unification of types is computed</definiens>
			</definition>
</paper>

		<paper id="1051">
			<definition id="0">
				<sentence>Diathesis alternations are changes in the realization of the argument structure of a verb that are sometimes accompanied by changes in meaning ( Levin , 1993 ) .</sentence>
				<definiendum id="0">Diathesis alternations</definiendum>
			</definition>
			<definition id="1">
				<sentence>I Here MOD represents any prenominal modifier ( e.g. , articles , pronouns , adjectives , quantifiers , ordinals ) .</sentence>
				<definiendum id="0">MOD</definiendum>
				<definiens id="0">any prenominal modifier ( e.g. , articles , pronouns , adjectives , quantifiers , ordinals )</definiens>
			</definition>
			<definition id="2">
				<sentence>The Kappa coefficient of agreement ( K ) is the ratio of the proportion of times , P ( A ) , that k raters agree to the proportion of times , P ( E ) , that we would expect the raters to agree by chance ( cf. ( 4 ) ) .</sentence>
				<definiendum id="0">Kappa coefficient of agreement ( K )</definiendum>
				<definiens id="0">the ratio of the proportion of times , P ( A ) , that k raters agree to the proportion of times , P ( E ) , that we would expect the raters to agree by chance</definiens>
			</definition>
			<definition id="3">
				<sentence>Hindle and Rooth ( 1993 ) used a partial parser to extract ( v , n , p ) tuples from a corpus , where p is the preposition whose attachment is ambiguous between the verb v and the noun n. We used a variant of the method described in Hindle and Rooth ( 1993 ) , the main difference being that we applied their lexical association score ( a log-likelihood ratio which compares the probability of noun versus verb attachment ) in an unsupervised non-iterative manner .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">a log-likelihood ratio which compares the probability of noun versus verb attachment</definiens>
			</definition>
			<definition id="4">
				<sentence>Note that the corpus and Levin did not agree with respect to the most popular classes licensing the dative and benefactive alternations : THROWING ( e.g. , toss ) and BUILD verbs ( e.g. , carve ) are the biggest classes in Levin allowing the dative and benefactive alternations respectively , in contrast to FUTURE HAVING and GET verbs in the corpus .</sentence>
				<definiendum id="0">THROWING</definiendum>
				<definiens id="0">the biggest classes in Levin allowing the dative and benefactive alternations respectively , in contrast to FUTURE HAVING and GET verbs in the corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>Thus the typicality of a verb can be defined as the conditional probability of the frame given the verb : f ( framei , verb ) ( 6 ) P ( frameilverb ) = y~ f fframe n , verb ) n We calculate Pfframeilverb ) by dividing f ( frame i , verb ) , the number of times the verb was attested in the corpus with frame i , by ~-~ .</sentence>
				<definiendum id="0">typicality of a verb</definiendum>
			</definition>
			<definition id="6">
				<sentence>Introduction to WordNet : An on-line lexical database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="7">
				<sentence>Learnability and Cognition : The Acquisition of Argument Structure .</sentence>
				<definiendum id="0">Learnability</definiendum>
				<definiendum id="1">Cognition</definiendum>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>The parser produces outputs that encode a labeled dependency tree representation of the syntactic relations between the words in the sentence .</sentence>
				<definiendum id="0">parser</definiendum>
				<definiens id="0">produces outputs that encode a labeled dependency tree representation of the syntactic relations between the words in the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root ( Oflazer , 1993 ) .</sentence>
				<definiendum id="0">Turkish</definiendum>
			</definition>
			<definition id="2">
				<sentence>For instance , LI~ is the pattern `` © ... . ) ... . 0 '' \ [ `` 0 '' I 1\ ] * `` &gt; '' which checks that ( i ) this is a word-final IG ( has a `` © '' ) , ( ii ) the right side `` topmost '' channel is empty ( channel symbol nearest to `` ) '' is `` 0 '' ) , and ( iii ) the IG is not linked to any other in any of the lower channels ( the only symbols on the right side are 0s and ls . )</sentence>
				<definiendum id="0">LI~</definiendum>
				<definiens id="0">empty ( channel symbol nearest to `` ) '' is `` 0 '' ) , and ( iii ) the IG is not linked to any other in any of the lower channels</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , more than one subject or one object may attach to a verb , or more that one determiner or possessor may attach to a nominal , an object may attach to a passive verb ( conjunctions are handled in the manner described in J£rvinen and Tapanainen ( 1998 ) ) , or a nominative pronoun may be linked as a direct object ( which is not possible in Turkish ) , etc .</sentence>
				<definiendum id="0">direct object</definiendum>
				<definiens id="0">a verb , or more that one determiner or possessor may attach to a nominal , an object may attach to a passive verb ( conjunctions are handled in the manner described in J£rvinen</definiens>
			</definition>
			<definition id="4">
				<sentence>AtMost0neDet = \ [ `` &lt; `` \ [ ~ \ [ \ [ $ '' D '' \ ] 'I\ ] &amp; LeftCharmelSymbols* \ ] `` ( `` AnyIG ( `` @ '' ) `` ) '' RightChannelSymbols* `` &gt; '' \ ] * ; The FST for this regular expression makes sure that all configurations that are produced have at most one D symbol among the left channel symbols , n Many other syntactic constraints ( e.g. , only one object to a verb ) can be formulated similar to above .</sentence>
				<definiendum id="0">FST</definiendum>
				<definiens id="0">Many other syntactic constraints ( e.g. , only one object to a verb</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>With this set of resources , we performed the two different sets of CLIR experiments , denoted EqFd ( English queries retrieving French documents ) , and FqBd ( French queries retrieving English documents . )</sentence>
				<definiendum id="0">EqFd</definiendum>
				<definiens id="0">queries retrieving French documents</definiens>
			</definition>
			<definition id="1">
				<sentence>Note also that the results presented are not the TREC-7 CLIR task , which involved both cross-language information retrieval and the merging of documents retrieved from sources in different languages .</sentence>
				<definiendum id="0">TREC-7 CLIR task</definiendum>
				<definiens id="0">information retrieval and the merging of documents retrieved from sources in different languages</definiens>
			</definition>
			<definition id="2">
				<sentence>Our information retrieval systems consists of first pass scoring with the Okapi formula ( Robertson et al. , 1995 ) on unigrams and symmetrized bigrams ( with 210 en , des , de , and allowed as connectors ) followed by a second pass re-scoring using local context analysis ( LCA ) as a query expansion technique ( Xu and Croft , 1996 ) .</sentence>
				<definiendum id="0">information retrieval systems</definiendum>
				<definiendum id="1">LCA</definiendum>
				<definiens id="0">consists of first pass scoring with the Okapi formula ( Robertson et al. , 1995 ) on unigrams and symmetrized bigrams ( with 210 en , des , de , and allowed as connectors ) followed by a second pass re-scoring using local context analysis</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>The first indicator , frequency , is simply the frequency with which each verb occurs over the entire corpus .</sentence>
				<definiendum id="0">frequency</definiendum>
				<definiens id="0">the frequency with which each verb occurs over the entire corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>A pilot study showed no further improvement in accuracy or recall tradeoff by additional learning algorithms : Naive Bayes ( Duda and 114 stativity completedness corpus : 3,224 med reports 10 novels size : 1,159,891 846,913 parsed clauses : 97,973 training : 739 ( 634 events ) testing : 739 ( 619 events ) verbs in test set : 222 204 clauses excluded : be and have stative 75,289 307 ( 196 culm ) 308 ( 195 culm ) Table 4 : Two classification problems on different data sets .</sentence>
				<definiendum id="0">Naive Bayes</definiendum>
			</definition>
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>Word Sense Disambiguation ( WSD ) is an open problem in Natural Language Processing .</sentence>
				<definiendum id="0">Word Sense Disambiguation ( WSD )</definiendum>
			</definition>
			<definition id="1">
				<sentence>WSD methods can be broadly classified into three types : provided by machine readable dictionaries ( Cowie et al. , 1992 ) , ( Miller et al. , 1994 ) , ( Agirre and Rigau , 1995 ) , ( Li et al. , 1995 ) , ( McRoy , 1992 ) ; training on a corpus that has already been semantically disambiguated ( supervised training methods ) ( Gale et al. , 1992 ) , ( Ng and Lee , 1996 ) ; raw corpora ( unsupervised training methods ) ( Yarowsky , 1995 ) ( Resnik , 1997 ) .</sentence>
				<definiendum id="0">WSD methods</definiendum>
				<definiendum id="1">raw corpora</definiendum>
				<definiens id="0">provided by machine readable dictionaries</definiens>
				<definiens id="1">McRoy , 1992 ) ; training on a corpus that has already been semantically disambiguated ( supervised training methods</definiens>
			</definition>
			<definition id="2">
				<sentence>A metric is introduced in this sense which when applied to all possible combinations of the senses of two or more words it ranks them .</sentence>
				<definiendum id="0">metric</definiendum>
			</definition>
			<definition id="3">
				<sentence>INPUT : semantically untagged word1 word2 pair ( W1 W2 ) OUTPUT : ranking the senses of one word PROCEDURE : STEP 1 .</sentence>
				<definiendum id="0">INPUT</definiendum>
				<definiens id="0">semantically untagged word1 word2 pair ( W1 W2 ) OUTPUT : ranking the senses of one word PROCEDURE : STEP 1</definiens>
			</definition>
			<definition id="4">
				<sentence>Consider , for example , that W2 has m senses , thus W2 appears in m similarity lists : ... , ( wL ( ' , ... , where W 1 , Wff , ... , W~ n are the senses of W2 , and W2 ( s ) represents the synonym number s of the sense W~ as defined in WordNet .</sentence>
				<definiendum id="0">wL</definiendum>
				<definiens id="0">' , ... , where W 1 , Wff , ... , W~ n are the senses of W2 , and W2 ( s ) represents the synonym number s of the sense W~ as defined in WordNet</definiens>
			</definition>
			<definition id="5">
				<sentence>As indicated in ( Resnik and Yarowsky , 1997 ) , it is difficult to compare the WSD methods , as long as distinctions reside in the approach considered ( MRD based methods , supervised or unsupervised statistical methods ) , and in the words that are disambiguated .</sentence>
				<definiendum id="0">WSD methods</definiendum>
				<definiens id="0">MRD based methods , supervised or unsupervised statistical methods ) , and in the words that are disambiguated</definiens>
			</definition>
			<definition id="6">
				<sentence>bomb ( # 1/3 ) cause ( # 1/2 ) damage ( # 1~5 ) iuju , ( # e/4 ) X-Y cause-bomb cause-damage cause-injury SCORE c # I 12.83 12.63 30.62 C # 2 X Y C # 1 damage-bomb 5.60 damage-cause 1.73 damage-injury 9.87 SCORE 17.20 c # 2 C # 3 C # 4 C # 5 Note that the senses for word injury differ from la. to lb. ; the one determined by our method ( # 2/4 ) is described in WordNet as `` an accident that results in physical damage or hurt '' ( hypernym : accident ) , and the sense provided in SemCor ( # 1/4 ) is defined as `` any physical damage ' ( hypernym : health problem ) .</sentence>
				<definiendum id="0">health problem</definiendum>
				<definiens id="0">the senses for word injury differ from la. to lb. ; the one determined by our method ( # 2/4 ) is described in WordNet as `` an accident that results in physical damage or hurt ''</definiens>
			</definition>
</paper>

		<paper id="1046">
			<definition id="0">
				<sentence>P ... .. is the probability that they discuss different subjects and are separated by a topic boundary .</sentence>
				<definiendum id="0">P ... ..</definiendum>
				<definiens id="0">the probability that they discuss different subjects and are separated by a topic boundary</definiens>
			</definition>
			<definition id="1">
				<sentence>M is a normalizing term required to make the conditional probabilities sum to 1 .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">a normalizing term required to make the conditional probabilities sum to 1</definiens>
			</definition>
			<definition id="2">
				<sentence>Occurrences in region 1 0 0 2+ Occurrences in region 2 0 2+ 1+ 0+ Conditional probability ( x ( l-y ) ~-y 1 ( 1 ) ~-2 B-I B-I l-y y 1 ( 1 ) ~-2 B-I B-I 1 1 - ( 1 ~ ) k-2 M ( B 1 ) B 1 Table 2 : Conditional probabilities used to compute P nn~ '' Our second algorithm is a maximum entropy model that uses these features : • Did our word frequency algorithm suggest a topic boundary ?</sentence>
				<definiendum id="0">Conditional probability</definiendum>
				<definiens id="0">a maximum entropy model that uses these features</definiens>
			</definition>
			<definition id="3">
				<sentence>To address this , we discarded the or , ~ , and B parameters particular to each word and instead used the same parameter values for each word-namely , those assigned to unknown words through our smoothing process .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">parameters particular to each word and instead used the same parameter values for each word-namely , those assigned to unknown words through our smoothing process</definiens>
			</definition>
</paper>

		<paper id="1081">
			<definition id="0">
				<sentence>The coordinate phrase ( CP ) is a source of structural ambiguity in natural language .</sentence>
				<definiendum id="0">phrase</definiendum>
				<definiendum id="1">CP</definiendum>
				<definiens id="0">a source of structural ambiguity in natural language</definiens>
			</definition>
			<definition id="1">
				<sentence>• w , ~_~ is the leftmost noun ( nl ) if : I5 = ( a l nl , p , n2 , cc , n3 ) The parts of the CP are analogous to those of the prepositional phrase ( PP ) such that { nl , n2 } { n , v } and n3 p. JAR98\ ] determines the probability p ( v , n , p , a ) .</sentence>
				<definiendum id="0">~_~</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">a l nl , p , n2 , cc</definiens>
				<definiens id="1">analogous to those of the prepositional phrase ( PP ) such that { nl , n2 } { n , v } and n3 p. JAR98\ ] determines the probability p ( v , n ,</definiens>
			</definition>
			<definition id="2">
				<sentence>First , we can factor p ( a , nl , n2 , n3 ) as follows : p ( a , nl , n2 , n3 ) = p ( nl ) p ( n2 ) , p ( alnl , n2 ) , p ( n3 I a , nl , n2 ) The terms p ( nl ) and p ( n2 ) are independent of the attachment and need not be computed .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">independent of the attachment and need not be computed</definiens>
			</definition>
			<definition id="3">
				<sentence>~1 , true ) iff ( nl , true ) &gt; 0 f ( nl ) p ( truelnl ) = .5 otherwise { / ( n2 , ~r~ , e ) if f ( n2 , true ) &gt; 0 / ( n2 ) p ( true\ [ n2 ) = .5 otherwise where f ( n2 , true ) is the number of times n2 appears in an unambiguously attached CP in the training data and f ( n2 ) is the number of times this noun has appeared as either nl , n3 , or ncc in the training data .</sentence>
				<definiendum id="0">f ( n2</definiendum>
				<definiens id="0">the number of times n2 appears in an unambiguously attached CP in the training data and f</definiens>
			</definition>
			<definition id="4">
				<sentence>Bigram counts axe used to compute these as follows : f ( nl , n3 , true ) p ( n3 \ [ true , nl ) = l\ ] ( nl , TM ) if I ( nl , n3 , true ) &gt; O otherwise f ( n2 , n3 , true ) p ( n3 l true , n2 ) = 11 ( n2 , TM ) if f ( n2 , n3 , true ) &gt; O otherwise where N is the set of all n3s and nets that occur in the training data .</sentence>
				<definiendum id="0">Bigram</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">counts axe used to compute these as follows : f ( nl , n3 , true ) p ( n3 \ [ true , nl ) = l\ ] ( nl , TM ) if I ( nl , n3 , true ) &gt; O otherwise f ( n2 , n3 , true ) p ( n3 l true</definiens>
				<definiens id="1">the set of all n3s and nets that occur in the training data</definiens>
			</definition>
</paper>

		<paper id="1062">
			<definition id="0">
				<sentence>For example , the case grammar theory is a semantic valence theory that describes the logical form of a sentence in terms of a predicate and a series of case-labeled arguments such as agent , object , location , source , goal ( Fillmore , 1968 ) .</sentence>
				<definiendum id="0">case grammar theory</definiendum>
				<definiens id="0">a semantic valence theory that describes the logical form of a sentence in terms of a predicate and a series of case-labeled arguments such as agent , object , location</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , coach is a coach of some sport ; virus is a virus causing some disease .</sentence>
				<definiendum id="0">coach</definiendum>
				<definiendum id="1">virus</definiendum>
				<definiens id="0">a virus causing some disease</definiens>
			</definition>
			<definition id="2">
				<sentence>The semantic-role relation is a relation that N1 fills in an semantic role of N2 .</sentence>
				<definiendum id="0">semantic-role relation</definiendum>
				<definiens id="0">a relation that N1 fills in an semantic role of N2</definiens>
			</definition>
			<definition id="3">
				<sentence>RSK ( Reikai Shougaku Kokugojiten ) , a Japanese dictionary for children , is used to find semantic roles of nouns in DBA .</sentence>
				<definiendum id="0">RSK</definiendum>
				<definiens id="0">used to find semantic roles of nouns in DBA</definiens>
			</definition>
			<definition id="4">
				<sentence>SBA uses the dictionary to specify conditions of rules .</sentence>
				<definiendum id="0">SBA</definiendum>
				<definiens id="0">uses the dictionary to specify conditions of rules</definiens>
			</definition>
			<definition id="5">
				<sentence>Dictionary based-Analysis ( DBA ) tries to find a correspondence between N1 and a semantic role of N2 by utilizing RSK , by the following process : tion sentences of N2 .</sentence>
				<definiendum id="0">Dictionary based-Analysis ( DBA )</definiendum>
				<definiens id="0">tries to find a correspondence between N1 and a semantic role of N2 by utilizing RSK , by the following process : tion sentences of N2</definiens>
			</definition>
			<definition id="6">
				<sentence>Relation ( R ) Semantic-role ( DBA ) Semantic-role ( SBA ) Agent Possession Belonging Time Place Modification Correct R is correct , but the R was detected , detected corresponbut incorrect dence was incorrect R was not detected , though R is possibly correct 137 19 21 19 15 -2 0 10 -1 2 32 -7 0 12 -1 2 20 -1 0 23 -7 2 20 -3 21 ( 4 ) rojin 'old person ' no shozo 'portrait ' DBA : portrait a painting0.17 or photograph0.17 of a face0.1s or figure0.0 of real person 0 .</sentence>
				<definiendum id="0">Relation</definiendum>
				<definiens id="0">R ) Semantic-role ( DBA ) Semantic-role ( SBA ) Agent Possession Belonging Time Place Modification Correct R is correct , but the R was detected</definiens>
			</definition>
			<definition id="7">
				<sentence>We have collected 300 test N1 no N2 phrases from EDR dictionary ( Japan Electronic Dictionary Research Institute Ltd. , 1995 ) , IPA dictionary ( Information-Technology Promotion Agency , Japan , 1996 ) , and literatures on N1 no N2 phrases , paying attention so that they had enough diversity in their relations .</sentence>
				<definiendum id="0">IPA dictionary</definiendum>
				<definiens id="0">collected 300 test N1 no N2 phrases from EDR dictionary ( Japan Electronic Dictionary Research Institute Ltd.</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>The Single Word Assignment strategy uses the stop list which includes the most frequent common words .</sentence>
				<definiendum id="0">Single Word Assignment strategy</definiendum>
				<definiens id="0">uses the stop list which includes the most frequent common words</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Using this insight , we developed an information-theoretic metric , the skew divergence , which incorporates the support-intersection data in an asymmetric fashion .</sentence>
				<definiendum id="0">information-theoretic metric</definiendum>
				<definiendum id="1">skew divergence</definiendum>
				<definiens id="0">incorporates the support-intersection data in an asymmetric fashion</definiens>
			</definition>
			<definition id="1">
				<sentence>Previously , we found the Jensen-Shannon divergence ( Rao , 1982 ; J. Lin , 1991 ) to be a useful measure of the distance between distributions : JS ( q , r ) =-~l \ [ D ( q aVgq , r ) +D ( r aVgq , r ) \ ] The function D is the KL divergence , which measures the ( always nonnegative ) average inefficiency in using one distribution to code for another ( Cover and Thomas , 1991 ) : ( v ) D ( pl ( V ) IIp2 ( V ) ) = EPl ( V ) log Pl p2 ( v ) `` V The function avga , r denotes the average distribution avgq , r ( V ) -- = ( q ( v ) +r ( v ) ) /2 ; observe that its use ensures that the Jensen-Shannon divergence is always defined .</sentence>
				<definiendum id="0">Jensen-Shannon divergence</definiendum>
				<definiens id="0">1982 ; J. Lin , 1991 ) to be a useful measure of the distance between distributions : JS ( q , r ) =-~l \ [ D ( q aVgq</definiens>
				<definiens id="1">measures the ( always nonnegative ) average inefficiency in using one distribution to code for another</definiens>
			</definition>
			<definition id="2">
				<sentence>2Strictly speaking , some of these functions are dissimilarity measures , but each such function f can be recast as a similarity function via the simple transformation C f , where C is an appropriate constant .</sentence>
				<definiendum id="0">C</definiendum>
				<definiens id="0">an appropriate constant</definiens>
			</definition>
			<definition id="3">
				<sentence>Euclidean distance L1 norm cosine Jaccard 's coefficient L2 ( q , r ) = Ll ( q , r ) = cos ( q , r ) = Jac ( q , r ) = ~v ( q ( v ) r ( v ) ) 2 Iq ( v ) r ( v ) l V ~-~v q ( v ) r ( v ) X/~-~v q ( v ) 2 V/Y~-v r ( v ) 2 I { v : q ( v ) &gt; 0 and r ( v ) &gt; 0 } l I { v I q ( v ) &gt; 0 or r ( v ) &gt; O } l Figure 1 : Well-known functions The confusion probability has been used by several authors to smooth word cooccurrence probabilities ( Sugawara et al. , 1985 ; Essen and Steinbiss , 1992 ; Grishman and Sterling , 1993 ) ; it measures the degree to which word m can be substituted into the contexts in which n appears .</sentence>
				<definiendum id="0">confusion probability</definiendum>
			</definition>
			<definition id="4">
				<sentence>Test-set performance was measured by the error rate , defined as T ( # of incorrect choices + ( # of ties ) /2 ) , where T is the number of test triple tokens in the set , and a tie results when both alternatives are deemed equally likely by the language model in question .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">the number of test triple tokens in the set</definiens>
			</definition>
			<definition id="5">
				<sentence>For a given similarity measure f and neighborhood size k , let 3f , k ( n ) denote the k most similar words to n according to f. We define the evidence according to f for the cooccurrence ( n , v~ ) as Ef , k ( n , vi ) = \ [ ( m E SLk ( n ) : P ( vilm ) &gt; l } l • Then , the decision rule was to choose the alternative with the greatest evidence .</sentence>
				<definiendum id="0">m E SLk ( n )</definiendum>
				<definiens id="0">For a given similarity measure f and neighborhood size k , let 3f , k ( n ) denote the k most similar words to n according to f. We define the evidence according to f for the cooccurrence</definiens>
			</definition>
			<definition id="6">
				<sentence>vq~ v~ = 2 IVq~l IV \ ( vq u V~ ) l 2 IVq \ Vail Iv~ \Vq~l + E E sign\ [ ( q ( vl ) q ( v2 ) ) ( r ( vl ) r ( v2 ) ) \ ] Vl E ( VqA Vr ) v2EYq~ , + E E sign\ [ ( q ( vl ) -q ( v2 ) ) ( r ( vl ) -r ( v2 ) ) \ ] Vl eVqr v2EVqUVr conf ( q , r , P ( m ) ) cos ( q , r ) = P ( ra ) Y\ ] q ( v ) r ( v ) /P ( v ) v e Vq~ = E q ( v ) r ( v ) ( E q ( v ) 2 E r ( v ) 2 ) -1/2 v~ Vqr ve Vq v~ Vr Ll ( q , r ) JS ( q , r ) Jac ( q , r ) = 2-E ( Iq ( v ) -r ( v ) l-q ( v ) -r ( v ) ) vE Vqr = log2 + 1 E ( h ( q ( v ) + r ( v ) ) h ( q ( v ) ) h ( r ( v ) ) ) , v ~ Vq~ = IV~l/IV~ u v~l h ( x ) = -x log x Table 1 : Similarity functions , written in terms of sums over supports and grouped by average performance .</sentence>
				<definiendum id="0">P ( m ) ) cos</definiendum>
				<definiens id="0">-r ( v ) ) vE Vqr = log2 + 1 E ( h ( q ( v ) + r ( v ) ) h</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>Each utterance in dialogues is manually annotated with discourse knowledge such as speaker ( SP ) , syntactic pattern ( ST ) , speech acts ( SA ) and discourse structure ( DS ) information .</sentence>
				<definiendum id="0">discourse structure ( DS</definiendum>
				<definiens id="0">annotated with discourse knowledge such as speaker ( SP ) , syntactic pattern ( ST ) , speech acts ( SA</definiens>
			</definition>
			<definition id="1">
				<sentence>In this paper , we classified the 17 types of speech acts that appear in the dialogue KS represents the Korean sentence and EN represents the translated English sentence .</sentence>
				<definiendum id="0">dialogue KS</definiendum>
				<definiendum id="1">EN</definiendum>
				<definiens id="0">represents the Korean sentence</definiens>
				<definiens id="1">the translated English sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>A DSB represents the relationship between two consecutive utterances in a dialogue .</sentence>
				<definiendum id="0">DSB</definiendum>
				<definiens id="0">the relationship between two consecutive utterances in a dialogue</definiens>
			</definition>
			<definition id="3">
				<sentence>With this notation , P ( GilU\ ] , O means the probability that G/ becomes the DSB of utterance U~ given a sequence of utterances U~ , U 2 ... .. Ui .</sentence>
				<definiendum id="0">O</definiendum>
				<definiens id="0">the probability that G/ becomes the DSB of utterance U~ given a sequence of utterances U~</definiens>
			</definition>
			<definition id="4">
				<sentence>, P ( Si , Gi IUl , i ) means the probability that S~ and G i will be , respectively , the speech act and the DSB of an utterance U/ given a sequence of utterances Ut , U2 ... .. U~ .</sentence>
				<definiendum id="0">P ( Si</definiendum>
				<definiens id="0">the probability that S~ and G i will be</definiens>
			</definition>
			<definition id="5">
				<sentence>P ( a , b ) = lrI '' I Ot\ [ ' ( ''b ) i=1 where 0 &lt; c~ i &lt; oo , i = { 1,2 ... .. k } ( 13 ) In equation ( 13 ) , a is either a speech act or a DSB depending on the term , b is the context ( or history ) of a , 7r is a normalization constant , and is the model parameter corresponding to each feature functionf .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">b</definiendum>
				<definiendum id="2">7r</definiendum>
				<definiens id="0">a normalization constant , and is the model parameter corresponding to each feature functionf</definiens>
			</definition>
			<definition id="6">
				<sentence>10 iff a = response and f ( a , b ) = b = User : request , Agent : ask ref otherwise where b is the information of Ujand Uk defined in equation ( 3 ) ( 16 ) 10 iff a = response and f ( a , b ) = b_ t = Agent : ask ref otherwise where b_~ is the information of Uk defined in equation ( 3 ) ( 17 ) f ( a'b ) = { lo iffa=resp°nseandb-2otherwise=USer : request where b_ 2 is the information of Ujdefined in equation ( 3 ) ( 18 ) Similarly , we can construct feature functions for the discourse structure analysis model .</sentence>
				<definiendum id="0">b</definiendum>
				<definiendum id="1">b_~</definiendum>
				<definiens id="0">the information of</definiens>
			</definition>
			<definition id="7">
				<sentence>Accuracy ( Closed test ) Accuracy ( Open test ) Candidates Top-1 Top-3 Top-1 Top-3 Lee ( 1997 ) 78.59 % 97.88 % Samuel ( 1998 ) 73.17 % Reithinger ( 1997 ) 74.70 % Model I 90.65 % 99.66 % 81.61 % 93.18 % Model II 90.65 % 99.66 % 83,37 % 95.35 % Table 4 .</sentence>
				<definiendum id="0">Accuracy</definiendum>
				<definiens id="0">Closed test ) Accuracy ( Open test</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>Part-of-speech tagging is the act of assigning each word in a sentence a tag that describes how that word is used in the sentence .</sentence>
				<definiendum id="0">Part-of-speech tagging</definiendum>
				<definiens id="0">the act of assigning each word in a sentence a tag that describes how that word is used in the sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>Part-of-speech tagging is an important research topic in Natural Language Processing ( NLP ) .</sentence>
				<definiendum id="0">Part-of-speech tagging</definiendum>
			</definition>
			<definition id="2">
				<sentence>The output of the HMM is a sequence of output symbols .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiens id="0">a sequence of output symbols</definiens>
			</definition>
			<definition id="3">
				<sentence>For part-of-speech tagging , N is the number of tags that can be used by the system .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">the number of tags that can be used by the system</definiens>
			</definition>
			<definition id="4">
				<sentence>For part-ofspeech tagging , M is the number of words in the lexicon of the system .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the number of words in the lexicon of the system</definiens>
			</definition>
			<definition id="5">
				<sentence>The probability bj ( k ) is the probability that the k-th output symbol will be emitted when the model is in state j. For part-of-speech tagging , this is the probability that the word Wk will be emitted when the system is at tag tj ( i.e. , P ( wkltj ) ) .</sentence>
				<definiendum id="0">probability bj ( k )</definiendum>
				<definiens id="0">the probability that the k-th output symbol</definiens>
			</definition>
			<definition id="6">
				<sentence>Most of the best statistical taggers use a trigram model , which replaces the bigram transition probability aij = P ( rp = tjITp_ 1 -~ ti ) with a trigram probability aijk : P ( 7 '' p = tklrp_l = tj , rp-2 = ti ) .</sentence>
				<definiendum id="0">trigram model</definiendum>
				<definiens id="0">replaces the bigram transition probability aij</definiens>
			</definition>
			<definition id="7">
				<sentence>Distributions The full second-order HMM uses a notation similar to a standard first-order model for the probability distributions .</sentence>
				<definiendum id="0">full second-order HMM</definiendum>
				<definiens id="0">uses a notation similar to a standard first-order model for the probability distributions</definiens>
			</definition>
			<definition id="8">
				<sentence>As in the trigram model , instead of limiting the context to a first-order approximation , the A matrix is defined as follows : A = { aijk ) , where '' aija= P ( rp = tklrp_l = tj , rp-2 = tl ) , 1 &lt; p &lt; P Thus , the transition matrix is now three dimensional , and the probability of transitioning to a new state depends not only on the current state , but also on the previous state .</sentence>
				<definiendum id="0">A matrix</definiendum>
				<definiendum id="1">Thus</definiendum>
				<definiens id="0">A = { aijk )</definiens>
			</definition>
			<definition id="9">
				<sentence>N : c , Yoo which depends on the following numbers : gl = N2 -- ~ N3 = Co = C : -Co = number of times tk occurs number of times sequence tjta occurs number of times sequence titjtk occurs total number of tags that appear number of times tj occurs number of times sequence titj occurs where : log ( N2 + 1 ) + 1 k~ .</sentence>
				<definiendum id="0">c , Yoo</definiendum>
				<definiens id="0">depends on the following numbers : gl = N2 -- ~ N3 = Co = C : -Co = number of times tk occurs number of times sequence tjta occurs number of times sequence titjtk occurs total number of tags that appear number of times tj occurs number of times</definiens>
			</definition>
			<definition id="10">
				<sentence>Otherwise , suffixes up to length n 2 are used , where n is the length of the word .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the length of the word</definiens>
			</definition>
			<definition id="11">
				<sentence>The running time of this algorithm is O ( NT3 ) , where N is the length of the sentence , and T is the number of tags .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">T</definiendum>
				<definiens id="0">the length of the sentence</definiens>
				<definiens id="1">the number of tags</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>In particular , a drawback of the NANC is the occurrence of repeated articles ; since the corpus consists of all of the articles that come over the wire , some days include multiple , updated versions of the same story , containing identical paragraphs or sentences .</sentence>
				<definiendum id="0">NANC</definiendum>
				<definiens id="0">the occurrence of repeated articles</definiens>
			</definition>
</paper>

		<paper id="1064">
			<definition id="0">
				<sentence>The following entries for eat and throw illustrate such an encoding for internal arguments ( again , external arguments are left aside for the sake of simplicity ) : throw : ARGSTR = EVENTSTR = QUALIA = eat ARGSTR = EVENTSTR = QUALIA = ~GI = x-'ind G2 y : Ind , i-inc ( y , el ) ~ i = e~ : throw_a~t 2 e2 : Binary_RStag~ AGENTIVE = throw_act ( ez , x , y ) ~A~ G1 = x=ind G2 y : ind , m-inc ( y , ex ) 2 e2 : binary-RStage~ AGENTIVE = eat act ( ex , x , y ) =/i-inc ( x , e ) indicates that the internal structures of subevent e and argument x are related by an homorphic mapping .</sentence>
				<definiendum id="0">eat act</definiendum>
				<definiendum id="1">e )</definiendum>
				<definiens id="0">indicates that the internal structures of subevent e and argument x are related by an homorphic mapping</definiens>
			</definition>
</paper>

		<paper id="1044">
			<definition id="0">
				<sentence>Terms are described in a two-tier framework composed of a paradigmatic level and a syntagmatic level that account for the three linguistic dimensions of term variability ( morphology , syntax , and semantics ) .</sentence>
				<definiendum id="0">syntagmatic level</definiendum>
				<definiens id="0">dimensions of term variability ( morphology , syntax , and semantics )</definiens>
			</definition>
			<definition id="1">
				<sentence>There are basically two structures of binary terms : X1 N2 compounds in which X1 is a noun , an adjective or a participle , and N1 Prep N~ terms .</sentence>
				<definiendum id="0">X1 N2</definiendum>
				<definiens id="0">compounds in which X1 is a noun , an adjective or a participle</definiens>
			</definition>
			<definition id="2">
				<sentence># Term Variant 1 cell differentiation 2 primary response 3 pressure decline 4 adipose tissue 5 extensive resection 6 clinical test 7 adipic acid 8 morphological change 9 clinical test 10 electrical property 12 hypothesis test 16 acidic protein 17 absorbed dose 18 cylindrical shape 19 assisted ventilation 20 genetic disease 21 early pregnancy 22 intertrochanteric fracture 25 arteriovenous fistula 27 pressure measurement 28 identification test 29 electrical stimulus 31 combined treatment 32 genetic disease 33 increased dose 34 acrylonitrile copolymer 35 development area 36 cell death cell growth and differentiation basal secretory activity and response pressure rise and fall adipose or fibroadipose tissue wide or radical resection clinical and histologic examinations adipie , suberic and sebacic acids morphologic , ultrastrucrural and immunologic changes clinical , radiographic , and arthroscopic examination electrical , mechanical , thermal and spectroscopic properties hypothesis , comparability , randomized and non-randomized trials acidic epidermal protein ingested human doses cylindrical fiberglass cast assisted modes of mechanical ventilation hereditary transmission of the disease early stage of gestation intertrochanteric ) femoral fractures arteriovenous ( A V ) fistulas pressure ( SBP ) measure identification , sensory tests electric , acoustic stimuli treatments were combined disease is familial dosage was increased copolymer of aerylonitrile areas of growth destruction of the virusinfected cell Table 3 : Sample variants from \ [ MEDIC\ ] using the variations from Table 1 ( # 37 to # 62 ) .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">cell death cell growth and differentiation basal secretory activity and response pressure rise and fall adipose or fibroadipose tissue wide or radical resection clinical and histologic examinations adipie , suberic and sebacic acids morphologic , ultrastrucrural and immunologic changes clinical , radiographic , and arthroscopic examination electrical , mechanical , thermal and spectroscopic properties hypothesis , comparability</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>I suspect this has happened because people assume TDM is a natural extension of the slightly less nascent field of data mining ( DM ) , also known as knowledge discovery in databases ( Fayyad and Uthurusamy , 1999 ) , and information archeology ( Brachman et al. , 1993 ) .</sentence>
				<definiendum id="0">TDM</definiendum>
			</definition>
			<definition id="1">
				<sentence>Text categorization is a boiling down of the specific content of a document into one ( or more ) of a set of pre-defined labels .</sentence>
				<definiendum id="0">Text categorization</definiendum>
				<definiens id="0">a boiling down of the specific content of a document into one ( or more ) of a set of pre-defined labels</definiens>
			</definition>
			<definition id="2">
				<sentence>WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="3">
				<sentence>In Christiane Fellbaum , editor , WordNet : An Electronic Lexical Database .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
</paper>

		<paper id="1053">
			<definition id="0">
				<sentence>We define an utterance here as a sentence , phrasal answer ( to a question ) , editing term , or acknowledgment .</sentence>
				<definiendum id="0">utterance</definiendum>
				<definiens id="0">a sentence , phrasal answer ( to a question ) , editing term , or acknowledgment</definiens>
			</definition>
</paper>

		<paper id="1045">
			<definition id="0">
				<sentence>An RST analysis is a dependency analysis of the structure of a text , whose leaf nodes are the propositions encoded in clauses .</sentence>
				<definiendum id="0">RST analysis</definiendum>
				<definiens id="0">a dependency analysis of the structure of a text , whose leaf nodes are the propositions encoded in clauses</definiens>
			</definition>
			<definition id="1">
				<sentence>From the perspective of discourse analysis , the task of information retrieval can be viewed as attempting to identify the `` aboutness , '' or global topicality , of a document in order to determine the relevance of the document as a response to a user 's query .</sentence>
				<definiendum id="0">aboutness</definiendum>
				<definiens id="0">a response to a user 's query</definiens>
			</definition>
			<definition id="2">
				<sentence>LFs normalize certain syntactic alternations ( e.g. active/passive ) and resolve both intrasentential anaphora and long-distance dependencies .</sentence>
				<definiendum id="0">LFs</definiendum>
				<definiens id="0">normalize certain syntactic alternations ( e.g. active/passive ) and resolve both intrasentential anaphora and long-distance dependencies</definiens>
			</definition>
</paper>

		<paper id="1065">
			<definition id="0">
				<sentence>Thus instead of `` nonterminal '' symbols used at the non-leaves of the tree , the PDT uses so-called analytical functions capturing the type of relation between a dependent and its governing node .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">uses so-called analytical functions capturing the type of relation between a dependent and its governing node</definiens>
			</definition>
			<definition id="1">
				<sentence>H is the head-child of the phrase , which inherits the head-word h from its parent P. L1 ... Ln and R1 ... Rm are left and right modifiers of H. Either n or m may be zero , and n = m = 0 for unary rules .</sentence>
				<definiendum id="0">H</definiendum>
				<definiens id="0">the head-child of the phrase , which inherits the head-word h from its parent P. L1 ... Ln and R1 ... Rm are left and right modifiers of H. Either n</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , in S ( bought , VBD ) -+ NP ( yesterday , NN ) NP ( IBM , NNP ) VP ( bought , VBD ) : n=2 m=0 P=S H=VP LI = NP L2 = NP l I = &lt; IBM , NNP &gt; 12 = &lt; yesterday , NN &gt; h = &lt; bought , VBD ) The model can be considered to be a variant of Probabilistic Context-Free Grammar ( PCFG ) . In PCFGs each role cr -- + fl in the CFG underlying the PCFG has an associated probability P ( /3la ) . In ( Collins 97 ) , P ( /~lo~ ) is defined as a product of terms , by assuming that the right-hand-side of the rule is generated in three steps : phrase , with probability 79H ( H I P , h ) . probability Hi=X..n+l 79L ( Li ( li ) \ [ P , h , H ) , where Ln+l ( ln+l ) = STOP. The STOP symbol is added to the vocabulary of nonterminals , and the model stops generating left modifiers when it is generated. probability Hi=l..m+l PR ( Ri ( ri ) \ [ P , h , H ) . Rm+l ( rm+l ) is defined as STOP. For example , the probability of s ( bought , VBD ) - &gt; NP ( yesterday , NN ) NP ( IBM , NNP ) VP ( bought , VBD ) is defined as /oh ( VP I S , bought , VBD ) × Pt ( NP ( IBM , NNP ) I S , VP , bought , VBD ) x Pt ( NP ( yesterday , NN ) I S , VP , bought , VBD ) × e~ ( STOP I s , vP , bought , VBD ) × Pr ( STOP I S , VP , bought .</sentence>
				<definiendum id="0">VBD ) -+ NP</definiendum>
				<definiendum id="1">NN ) NP ( IBM , NNP ) VP</definiendum>
				<definiendum id="2">Collins 97 ) , P ( /~lo~</definiendum>
				<definiendum id="3">probability Hi=X..n+l 79L</definiendum>
				<definiendum id="4">Li</definiendum>
				<definiendum id="5">NNP ) VP</definiendum>
				<definiendum id="6">VBD</definiendum>
				<definiendum id="7">VBD ) × Pt ( NP</definiendum>
				<definiendum id="8">VBD ) x Pt</definiendum>
				<definiens id="0">a variant of Probabilistic Context-Free Grammar ( PCFG ) . In PCFGs each role cr -- + fl in the CFG underlying the PCFG has an associated probability P ( /3la )</definiens>
				<definiens id="1">( li ) \ [ P , h , H ) , where Ln+l ( ln+l ) = STOP. The STOP symbol is added to the vocabulary of nonterminals , and the model stops generating left modifiers when it is generated. probability Hi=l..m+l PR ( Ri ( ri ) \ [ P , h</definiens>
				<definiens id="2">bought , VBD ) × e~ ( STOP I s , vP , bought , VBD ) × Pr ( STOP I S , VP , bought</definiens>
			</definition>
			<definition id="3">
				<sentence>( Collins 97 ) describes a series of refinements to this basic model : the addition of `` distance '' ( a conditioning feature indicating whether or not a modifier is adjacent to the head ) ; the addition of subcategorization parameters ( Model 2 ) , and parameters that model wh-movement ( Model 3 ) ; estimation 506 TOP I S ( bought , VBD ) NP ( yesterday , NN ) NP ( IBM , NNP ) I I NN NNP I I yesterday IBM TOP S ( bought , VBD ) NP ( yesterday , NN ) NP ( IBM , NNP ) VP ( bought , VBD ) NP ( Lotus , NNP ) - &gt; S ( bought , VBD ) - &gt; NP ( yesterday , NN ) - &gt; NN ( yesterday ) - &gt; NNP ( IBM ) - &gt; VBD ( bought ) - &gt; NNP ( Lotus ) VP ( bought , VBD ) VBD NP ( Lotus , NNP ) I I bought NNP I Lotus NP ( IBM , NNP ) VP ( bought , VBD ) NP ( Lotus , NNP ) Figure 1 : A lexicalized parse tree , and a list of the rules it contains .</sentence>
				<definiendum id="0">NN ) NP</definiendum>
				<definiendum id="1">yesterday IBM TOP S</definiendum>
				<definiendum id="2">NN ) NP</definiendum>
				<definiendum id="3">VBD ) NP</definiendum>
				<definiens id="0">A lexicalized parse tree</definiens>
			</definition>
			<definition id="4">
				<sentence>507 Input : sentence with part of speech tags : UN saw/V the/D man/N ( N=noun , V=verb , D=determiner ) dependencies ( word ~ Parent ) : ( I =~ saw ) , ( saw = : ~ START ) , ( the =~ man ) , ( man =¢ , saw &gt; Output : a lexicalized tree ( a ) X ( saw ) ( b ) X ( saw ) ( c ) N X ( saw ) X ( I ) V X ( man ) I \ [ I ~ I V X ( man ) N saw D N \ [ \ [ I I saw D N I the man \ [ \ [ the man X ( saw ) X ( saw ) X ( man ) N V D N I I I I I saw the man Figure 2 : Converting dependency structures to lexicalized trees with equivalent dependencies .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">c ) N X</definiendum>
				<definiens id="0">a lexicalized tree ( a ) X ( saw ) ( b )</definiens>
			</definition>
			<definition id="5">
				<sentence>The PDT takes the conjunct to be the head of coordination structures ( for example , and would be the head of the NP dogs and cats ) .</sentence>
				<definiendum id="0">PDT</definiendum>
				<definiens id="0">takes the conjunct to be the head of coordination structures ( for example , and would be the head of the NP dogs and cats</definiens>
			</definition>
			<definition id="6">
				<sentence>n+l where L0 is defined as a special NULL symbol .</sentence>
				<definiendum id="0">L0</definiendum>
			</definition>
			<definition id="7">
				<sentence>Part of speech ( POS ) tags serve an important role in statistical parsing by providing the model with a level of generalization as to how classes of words tend to behave , what roles they play in sentences , and what other classes they tend to combine with .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">an important role in statistical parsing by providing the model with a level of generalization as to how classes of words tend to behave , what roles they play in sentences</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>The non-head sequences may be empty , freq is a rule frequency , which is initialized randomly and subsequently estimated by the inside outsidealgorithm .</sentence>
				<definiendum id="0">rule frequency</definiendum>
				<definiens id="0">is initialized randomly and subsequently estimated by the inside outsidealgorithm</definiens>
			</definition>
			<definition id="1">
				<sentence>The noun chunk ( NC ) is an approximately non-recursive projection that excludes post-head complements and ( adverbial ) adjuncts introduced higher than pre-head modifiers and determiners but includes participial pre-modifiers with their complements .</sentence>
				<definiendum id="0">NC</definiendum>
			</definition>
			<definition id="2">
				<sentence>The training of our probabilistic CFG proceeds in three steps : ( i ) unlexicalized training with the supar parser , ( ii ) bootstrapping a lexicalized model from the trained unlexicalized one with the ultra parser , and finally ( iii ) lexicalized training with the hypar parser ( Carroll , 1997b ) .</sentence>
				<definiendum id="0">hypar parser</definiendum>
				<definiens id="0">probabilistic CFG proceeds in three steps : ( i ) unlexicalized training with the supar parser , ( ii ) bootstrapping a lexicalized model from the trained unlexicalized one with the ultra parser</definiens>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>By using this theory as the basis of underspecification , we can say : • underspecification is to be captured by allowing different possible relative scope assignments around the predicates , and • partial scopes between arbitrary quantitiers in the sentence will be translated into the equivalent scoping of quantifiers around their predicates .</sentence>
				<definiendum id="0">underspecification</definiendum>
				<definiens id="0">to be captured by allowing different possible relative scope assignments around the predicates , and • partial scopes between arbitrary quantitiers in the sentence will be translated into the equivalent scoping of quantifiers around their predicates</definiens>
			</definition>
			<definition id="1">
				<sentence>A strict partial order , ~- , is defined over the set which states that the relation love must be outscoped by both quantifiers : ( { every , a , love } , ( every ~love , a ~love ) ) The partial order states that both quantifiers outscope the verb , but says nothing about their scopes relative to each other .</sentence>
				<definiendum id="0">love</definiendum>
				<definiens id="0">the set which states that the relation</definiens>
			</definition>
			<definition id="2">
				<sentence>We can now extend the definition of - &gt; by saying that : 296 if ( P , ~- ) is a node in the tree , and x , y E P and x ~y , then x. &gt; y and x. &gt; z where z is any term that y dominates .</sentence>
				<definiendum id="0">z</definiendum>
				<definiens id="0">a node in the tree</definiens>
			</definition>
			<definition id="3">
				<sentence>A scope structure is defined as a triple ( P , ~ , : D ) , where P is a set of elements , ~is a strict total order over P and 7 : ) is the set of daughters .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">)</definiendum>
				<definiens id="0">a set of elements , ~is a strict total order over P</definiens>
				<definiens id="1">the set of daughters</definiens>
			</definition>
			<definition id="4">
				<sentence>If ( 9 is a ( countable ) set of elements , then scope structures can be recursively defined as : • If S = ( Ps , &gt; -s , { } ) , where Ps is a finite , non-empty subset of ( 9 and &gt; -s is a strict total order on Ps , then S is a scope structure , where : • If R and S are scope structures such that R = ( PR , ~R , DR ) and S = ( Ps , ~-s , : DS ) , where no element occurs in both R and S , and there is some element a such that a E Pn , then if T = ( PT , N'T , ~T ) , where PT = { a } t2 Ps , T~T = { R } U : Ds and ~-T is a strict total order on PT then T is a scope structure , where : or S then x occurs in T a , then a dominates x in T y in R then x dominates y in T y in S then x dominates y in T If S is a scope structure , then a node in S is defined as : • If S is a scope structure such that S - ( Ps , &gt; -s , T~S ) , then : ( Ps , &gt; '-s ) is a node in S if di E : Ds , then any node in di is a node in S. Having defined scope structures , we now define a scope representation , which is a pair iS , `` &gt; s ) , where S is a scope structure and `` &gt; s is a relation between pairs of elements which occur in S. `` &gt; s represents outscoping between any 297 pair of elements in the structure , rather than just between elements at a common node .</sentence>
				<definiendum id="0">Ps</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">a finite , non-empty subset of ( 9 and &gt; -s is a strict total order on Ps , then S is a scope structure , where : • If R and S are scope structures such that R = ( PR , ~R , DR ) and S = ( Ps , ~-s , : DS ) , where no element occurs in both R and S , and there is some element a such that a E Pn , then if T = ( PT , N'T , ~T ) , where PT = { a } t2 Ps , T~T = { R } U : Ds and ~-T is a strict total order on PT then T is a scope structure , where : or S then x occurs in T a , then a dominates x in T y in R then x dominates y in T y in S then x dominates y in T If S is a scope structure</definiens>
				<definiens id="1">a node in S. Having defined scope structures</definiens>
				<definiens id="2">a pair iS , `` &gt; s ) , where</definiens>
				<definiens id="3">a relation between pairs of elements which occur in S. `` &gt; s represents outscoping between any 297 pair of elements in the structure , rather than just between elements at a common node</definiens>
			</definition>
			<definition id="5">
				<sentence>Then the constraint representing the fully underspecified meaning is : eosAmosAeomAsoeAsomAmoe A eooAaooAeoaAooeAooaAaoe A e c-~ a A e ~-+ o A ei &gt; sAe~oAmi &gt; sAaDo Note that the symmetry of o is stated explicitly in the constraint .</sentence>
				<definiendum id="0">eooAaooAeoaAooeAooaAaoe A e c-~</definiendum>
				<definiens id="0">a A e ~-+ o A ei</definiens>
			</definition>
			<definition id="6">
				<sentence>The inference rules S1 , $ 2 and $ 3 operate by recursively reducing the ( arbitrary ) outscoping constraint X~ &gt; Z to XI &gt; YAYE &gt; Y~ , where Y and Y~ represent arguments to a common relation , and Y ' either dominates or is equal to Z. Repeated application of these constraints gives the set of scopes of quantifiers around their relations for the initial partial scoping .</sentence>
				<definiendum id="0">Y</definiendum>
				<definiens id="0">the set of scopes of quantifiers around their relations for the initial partial scoping</definiens>
			</definition>
			<definition id="7">
				<sentence>Then the worst case of applying the inference rules to F A X ~ &gt; Y to saturation turns out to be equivalent to completing the transitive closure of i &gt; , which is known to be soluble in better than O ( n 3 ) time ( Cormen et al. , 1990 ) , where n is the number of elements in the structure .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the number of elements in the structure</definiens>
			</definition>
			<definition id="8">
				<sentence>+ ZtX t &gt; Z where F is any conjunction of literals .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">any conjunction of literals</definiens>
			</definition>
</paper>

		<paper id="1058">
			<definition id="0">
				<sentence>( A signal is formula AXc AGb AF ( AX ( a /x b ) ) sense for all paths , at the next state c is true for all paths , globally b is true for all paths , eventually there is a state from which , for all paths , at the following state a and b are true at So true false true Table 1 : Interpretation of CTL formulas 452 O I i t t : =1 Figure 3 : Timing diagram for pulsing circuit r / .</sentence>
				<definiendum id="0">AX</definiendum>
				<definiens id="0">true at So true false true</definiens>
			</definition>
			<definition id="1">
				<sentence>By 'extended CTL ' , we mean a superset of CTL which is syntactically augmented to allow formulas such as rise ( p ) , fall ( p ) , discussed earlier , and pulse ( p , v , n ) , where p is an atom , v is a Boolean indicating a high or low value , and n is a natural number indicating duration .</sentence>
				<definiendum id="0">pulse</definiendum>
				<definiendum id="1">p</definiendum>
				<definiendum id="2">n</definiendum>
				<definiens id="0">a Boolean indicating a high or low value</definiens>
				<definiens id="1">a natural number indicating duration</definiens>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>( 1 ) n The distribution p ( VIc ) is the centroid for cluster c. It can be shown that I ( V , C ) is maximized subject to fixed I ( C , N ) and the above conditional independence assumption when p ( c ) p ( cln ) = ~ exp \ [ -/3D ( p ( Yln ) \ ] \ ] p ( Ylc ) ) \ ] , ( 2 ) where /3 is the Lagrange multiplier associated with fixed I ( C , N ) , Zn is the normalization Zn = y~ p ( c ) exp \ [ -/3D ( p ( Y\ [ n ) llp ( Ylc ) ) \ ] , c and D is the KuUback-Leiber ( KL ) divergence , which measures the distance , in an informationtheoretic sense , between two distributions q and r : • q ( v ) D ( qllr ) = ~ q ( v ) lOgr ( v ) .</sentence>
				<definiendum id="0">p ( c ) p</definiendum>
				<definiendum id="1">Zn</definiendum>
				<definiendum id="2">D</definiendum>
				<definiens id="0">the normalization Zn = y~ p ( c ) exp \ [ -/3D ( p ( Y\ [ n ) llp ( Ylc ) ) \ ] , c and</definiens>
				<definiens id="1">the KuUback-Leiber ( KL ) divergence , which measures the distance , in an informationtheoretic sense</definiens>
			</definition>
			<definition id="1">
				<sentence>Given an unseen pair ( n , v ) , we calculate an estimate 15 ( vln ) as an appropriate average of p ( vln I ) where n I is distributionally similar to n. Many distributional similarity measures can be considered ( Lee , 1999 ) .</sentence>
				<definiendum id="0">unseen pair</definiendum>
				<definiens id="0">an appropriate average of p ( vln I ) where n I is distributionally similar to n. Many distributional similarity measures</definiens>
			</definition>
			<definition id="2">
				<sentence>Test instances consist of noun-verb-verb triples ( n , vl , v2 ) , where both ( n , Vl ) and ( n , v2 ) are unseen cooccurrences , but ( n , vl ) is more likely ( how this is determined is discussed below ) .</sentence>
				<definiendum id="0">Test instances</definiendum>
			</definition>
			<definition id="3">
				<sentence>Overall performance is measured by the error rate on the entire test set , defined as 1 ~ ( # of incorrect choices + ( # of ties ) /2 ) , where T is the number of test triples , not counting multiplicities .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">measured by the error rate on the entire test set</definiens>
				<definiens id="1">the number of test triples , not counting multiplicities</definiens>
			</definition>
			<definition id="4">
				<sentence>Diversity : Its measurement , decomposition , apportionment and analysis .</sentence>
				<definiendum id="0">Diversity</definiendum>
				<definiens id="0">Its measurement , decomposition , apportionment and analysis</definiens>
			</definition>
</paper>

		<paper id="1080">
			<definition id="0">
				<sentence>- , Wn -- 1 } , where wi denotes the i-th word in the text .</sentence>
				<definiendum id="0">wi</definiendum>
				<definiens id="0">the i-th word in the text</definiens>
			</definition>
			<definition id="1">
				<sentence>We consider elementary questions of the type w-k E S , where W-k refers to the k-th position before the word to be predicted , 607 y/ n ( D n yes no Figure 1 : The structure of a pylon and S is a subset of the vocabulary .</sentence>
				<definiendum id="0">W-k</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">The structure of a pylon and</definiens>
			</definition>
</paper>

		<paper id="1050">
			<definition id="0">
				<sentence>Predicative patterns yield predicative relations such as cause or effect whereas discursive patterns yield non-predicative relations such as generic/specific or synonymy links .</sentence>
				<definiendum id="0">Predicative patterns</definiendum>
				<definiens id="0">yield predicative relations such as cause or effect whereas discursive patterns yield non-predicative relations such as generic/specific or synonymy links</definiens>
			</definition>
			<definition id="1">
				<sentence>2Ai is the ith item of the lexico-syntactic expression A , and n is the number of items in A. An item can be either a lemma , a punctuation mark , a symbol , or a tag ( N P , LIST , etc. ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">the ith item of the lexico-syntactic expression A , and</definiens>
			</definition>
			<definition id="2">
				<sentence>The similarity function is defined as follows : 3 Sim ( A , B ) = E Sim ( Wini ( A ) , Wini ( B ) ) ( 6 ) i=1 The function of similarity between lexicosyntactic patterns Sim ( Wini ( A ) , Wini ( B ) ) is defined experimentally as a function of the longest common string .</sentence>
				<definiendum id="0">similarity function</definiendum>
				<definiens id="0">follows : 3 Sim ( A , B ) = E Sim ( Wini ( A ) , Wini ( B ) ) ( 6 ) i=1 The function of similarity between lexicosyntactic patterns Sim ( Wini ( A ) , Wini ( B ) ) is defined experimentally as a function of the longest common string</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , procddd d'dlaboration ( process of elaboration ) is not a variant dlaboration d'une mdthode ( elaboration of a process ) even though procddd and mdthode are synonymous , because procddd is the head word of the first term while mdthode is the argument in the second term .</sentence>
				<definiendum id="0">procddd</definiendum>
				<definiendum id="1">mdthode</definiendum>
				<definiens id="0">a variant dlaboration d'une mdthode ( elaboration of a process</definiens>
			</definition>
			<definition id="4">
				<sentence>For instance , sdchage de la banane ( banana drying ) is a semantic variant of sdchage de fruits ( fruit drying ) which is not provided by the first step of the process .</sentence>
				<definiendum id="0">fruit drying</definiendum>
				<definiens id="0">a semantic variant of sdchage de fruits</definiens>
			</definition>
</paper>

		<paper id="1063">
			<definition id="0">
				<sentence>The instance \ [ - { ~ ~r ( keisha , slope ) J involves a unique inherent attribute , i.e. , `` the angle ( degree ) of the slope , '' therefore r @ ~-~ , ~ ( yuruyaka_na , gentle ) J is taken to be a value on the scale of the slope .</sentence>
				<definiendum id="0">J</definiendum>
				<definiens id="0">involves a unique inherent attribute</definiens>
			</definition>
			<definition id="1">
				<sentence>Example 1 yuruyaka_na keisha , gentle slope Japanese pronunciation literal translation KEISHA ( slope ) YURUYAKA_NA ( gentle ) \ ] degree I Adnominal Constituents that Express One of the Major Attributes of the Modified Noun This is the case in which the NLP system must identify the slot of the modified noun which is filled by the modifier .</sentence>
				<definiendum id="0">Modified Noun This</definiendum>
				<definiens id="0">the case in which the NLP system must identify the slot of the modified noun which is filled by the modifier</definiens>
			</definition>
			<definition id="2">
				<sentence>political flight ( copula , past ) A nominal refers to an extension of a thing with one or several intension ( s ) .</sentence>
				<definiendum id="0">political flight</definiendum>
				<definiendum id="1">nominal</definiendum>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>I Representation Equation I S A R ( T1 , • • • , Tn ) R ( S1 , ... , Sn ) = S I S is the semantic representation of the source , $ 1 , ... , Sn and T1 , ... , Tn are the semantic representations of the parallel elements in the source and target respectively and R represents the relation to be recovered .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">represents the relation to be recovered</definiens>
			</definition>
			<definition id="1">
				<sentence>The equation is solved using Higher-Order Unification ( HOU ) : Given any solvable equation M = N , HOU yields a substitution of terms for free variables that makes M and N equal in the theory of a/~v-identity .</sentence>
				<definiendum id="0">Higher-Order Unification</definiendum>
				<definiendum id="1">HOU</definiendum>
				<definiens id="0">yields a substitution of terms for free variables that makes M and N equal in the theory of a/~v-identity</definiens>
			</definition>
			<definition id="2">
				<sentence>For instance , in ( Gaxdent and Kohlhase , 1996a ) , the focus value of ( 8a ) is defined with the help of the equation : I Focus Value Equation I Sere = X ( F ) I where Sern is the semantic of the sentence without the focus operator ( e.g. intro ( j , m , s ) for ( 8 ) ) , F represents the focus and X helps determine the value of the focus variable ( written X ) as follows : Definition 3.1 ( Focus value ) Let X = Ax .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">the help of the equation : I Focus Value Equation I Sere = X ( F ) I where Sern is the semantic of the sentence without the focus operator ( e.g. intro ( j , m , s ) for ( 8 ) )</definiens>
			</definition>
			<definition id="3">
				<sentence>Such an approach is in line with ( Krifka , 1992 ) which argues that the repeated material in an SOE is an anaphor resolving to its source counterpart .</sentence>
				<definiendum id="0">SOE</definiendum>
				<definiens id="0">an anaphor resolving to its source counterpart</definiens>
			</definition>
</paper>

		<paper id="1048">
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>WordNet has been an important research tool , but it is insufficient for domainspecific text , such as that encountered in the MUCs ( Message Understanding Conferences ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="1">
				<sentence>sim ( A , B ) = Ev , wCOS ( v , w ) size ( A ) size ( B ) where v ranges over all vectors for nouns 120 in group A , w ranges over the vectors for group B , and size ( x ) represents the number of nouns which are descendants of node x. We want to create a tree of all of the nouns in this data using standard bottom-up clustering techniques as follows : Put each noun into its own node .</sentence>
				<definiendum id="0">sim</definiendum>
				<definiendum id="1">size ( x )</definiendum>
				<definiens id="0">A ) size ( B ) where v ranges over all vectors for nouns 120 in group A , w ranges over the vectors for group B , and</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>The word segmentation task can be defined as finding the word segmentation 12d that maximize the joint probability of word sequence given character sequence P ( WIC ) .</sentence>
				<definiendum id="0">word segmentation task</definiendum>
			</definition>
			<definition id="1">
				<sentence>If wl is an unknown word whose part of speech is t , the word bigram probability P ( wi\ [ wl-a ) is approximated as the product of word bigram probability P ( &lt; U-t &gt; \ [ wi_l ) and the probability of wi given it is an unknown word whose part of speech is t , P ( wi\ [ &lt; U-t &gt; ) .</sentence>
				<definiendum id="0">P (</definiendum>
				<definiens id="0">an unknown word whose part of speech is t</definiens>
				<definiens id="1">the probability of wi given it is an unknown word whose part of speech is t</definiens>
			</definition>
			<definition id="2">
				<sentence>Without loss of generality , we decompose it into the product of word length probability and word spelling probability given its length , P ( wi\ [ &lt; UNK &gt; ) = P ( cx ... ck\ [ &lt; VNK &gt; ) = P ( kI &lt; UNK &gt; ) P ( cl ... cklk , &lt; UNK &gt; ) ( 4 ) where k is the length of the character sequence .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">word length probability and word spelling probability given its length , P ( wi\ [ &lt; UNK &gt; ) = P ( cx ... ck\ [ &lt; VNK &gt; ) = P ( kI &lt; UNK &gt; ) P ( cl ... cklk , &lt; UNK &gt; )</definiens>
			</definition>
			<definition id="3">
				<sentence>e-~p k ( 5 ) where p is the inverse of the number of characters in the character set .</sentence>
				<definiendum id="0">p</definiendum>
				<definiens id="0">the inverse of the number of characters in the character set</definiens>
			</definition>
			<definition id="4">
				<sentence>We then redistributed this evenly among all unobserved events a The second factor of Equation ( 13 ) is estimated from the Poisson distribution whose parameter '~ &lt; WT &gt; , &lt; U-t &gt; is the average length of words whose word type is &lt; WT &gt; and part of speech is &lt; U-t &gt; .</sentence>
				<definiendum id="0">U-t &gt;</definiendum>
				<definiens id="0">estimated from the Poisson distribution whose parameter '~ &lt; WT &gt;</definiens>
				<definiens id="1">the average length of words whose word type</definiens>
			</definition>
			<definition id="5">
				<sentence>P ( c~lci_l , &lt; WT &gt; , &lt; U-t &gt; ) = serving novel events to be r/ ( n+r ) , where n is the total number of events seen previously , and r is the number of symbols that are distinct .</sentence>
				<definiendum id="0">P</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">r</definiendum>
				<definiens id="0">the total number of events seen previously , and</definiens>
			</definition>
			<definition id="6">
				<sentence>V is the number of characters ( not tokens but types ) appeared in the corpus .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">the number of characters ( not tokens but types ) appeared in the corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>Let the number of words in the manually segmented corpus be Std , the number of words in the output of the word segmenter be Sys , and the number of matched words be M. Recall is defined as M/Std , and precision is defined as M/Sys .</sentence>
				<definiendum id="0">precision</definiendum>
				<definiens id="0">the number of words in the manually segmented corpus be Std , the number of words in the output of the word segmenter be Sys , and the number of matched words be M. Recall</definiens>
			</definition>
			<definition id="8">
				<sentence>It is calculated by F= ( f~2+l.0 ) xPxR f~2 x P + R ( 19 ) where P is precision , R is recall , and f~ is the relative importance given to recall over precision .</sentence>
				<definiendum id="0">R</definiendum>
				<definiendum id="1">f~</definiendum>
				<definiens id="0">the relative importance given to recall over precision</definiens>
			</definition>
			<definition id="9">
				<sentence>Precision is the percentage of correctly segmented unknown words in the system 's output to the all words that system identified as unknown words .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of correctly segmented unknown words in the system 's output to the all words that system identified as unknown words</definiens>
			</definition>
</paper>

		<paper id="1066">
			<definition id="0">
				<sentence>The introduction of statistical parsing brought with an obvious tactic for ranking the agenda : ( Bobrow , 1990 ) and ( Chitrao and Grishman , 1990 ) first used probabilistic context free grammars ( PCFGs ) to generate probabilities for use in a figure of merit ( FOM ) .</sentence>
				<definiendum id="0">PCFGs</definiendum>
				<definiendum id="1">FOM</definiendum>
				<definiens id="0">used probabilistic context free grammars</definiens>
			</definition>
			<definition id="1">
				<sentence>The philosophical backing for this figure is that we would like to rank an edge based on the value P ( N~ , kIto , n ) , ( 1 ) where N~ , k represents an edge of type i ( NP , S , etc. ) , which encompasses words j through k1 of the sentence , and t0 , ~ represents all n part-of-speech tags , from 0 to n 1 .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">encompasses words j through k1 of the sentence</definiens>
				<definiens id="1">all n part-of-speech tags , from 0 to n 1</definiens>
			</definition>
			<definition id="2">
				<sentence>Thus , the IM is an approximation ; if we can not calculate the full outside probability ( the probability of this constituent occurring with all the other tags in the sentence ) , we can at least calculate the probability of this constituent occurring with the previous and subsequent tag .</sentence>
				<definiendum id="0">IM</definiendum>
				<definiens id="0">an approximation</definiens>
				<definiens id="1">the probability of this constituent occurring with all the other tags in the sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>We hoped , however , that we might be able to find a way to simplify the algorithm such that it would be easier to implement and/or 516 Table 1 : Performance of various statistical schemata Trial description \ [ Gold98\ ] standard Correct , Chart competitors Correct , higher-merit competitors Correct , Chart or higher-merit MAP , higher-merit competitors Labelled Labelled Change in Edges Percent Precision Recall LP/LR avg .</sentence>
				<definiendum id="0">Chart</definiendum>
				<definiens id="0">competitors Correct , higher-merit competitors Correct , Chart or higher-merit MAP , higher-merit competitors Labelled Labelled Change in Edges Percent Precision Recall LP/LR avg</definiens>
			</definition>
</paper>

	</volume>
