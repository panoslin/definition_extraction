<?xml version="1.0" encoding="UTF-8"?>
	<volume id="W94">

		<paper id="0318">
			<definition id="0">
				<sentence>Electronic publishing is an application domain in which the acquisition , representation and presentation of knowledge are major tasks which are to be fulfilled by an editor .</sentence>
				<definiendum id="0">Electronic publishing</definiendum>
				<definiens id="0">an application domain in which the acquisition , representation and presentation of knowledge are major tasks which are to be fulfilled by an editor</definiens>
			</definition>
			<definition id="1">
				<sentence>textual resource According to systemic-functional theory ( SFL ) , the ultimate constraints on all linguistic expression are the extra-linguistic contexts of culture and situation .</sentence>
				<definiendum id="0">SFL</definiendum>
				<definiens id="0">the extra-linguistic contexts of culture and situation</definiens>
			</definition>
			<definition id="2">
				<sentence>Genre is a culture-specific category textually encoding a situation as it can typically occur in a culture or linguistic community .</sentence>
				<definiendum id="0">Genre</definiendum>
				<definiens id="0">a culture-specific category textually encoding a situation as it can typically occur in a culture or linguistic community</definiens>
			</definition>
			<definition id="3">
				<sentence>A typical GSP for a biography text then has the following stages , of which not all are obligatory : GSP stages Names , birth and death Education , development of career Major activities , major works Influences , analogies to other artists Impact Uncovering such generic structures is a large empirical task that needs to be addressed in text generation : a system such as KPML can then be seen as one candidate way of noting the results of such studies in an immediately usable fashion .</sentence>
				<definiendum id="0">typical GSP</definiendum>
				<definiens id="0">of which not all are obligatory : GSP stages Names , birth and death Education , development of career Major activities , major works Influences , analogies to other artists Impact Uncovering such generic structures</definiens>
			</definition>
			<definition id="4">
				<sentence>155 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 biography ( instant tat e Particular ( v / Mea-Person ) ) ( stage Name ) ( inst ant tat e Name ( v / Mea-Named actee Particular ) ) ( stage Birth ) ( instantiat e Birth ( v / Mea-Birth-Event actor Particular ) ) ( int er-pr e s elect -feature Birth reconstruction ) ( int er-Pres elect-f eature Birth activity ) ( stage Education ) Figure 2 : Realization statements for the genre \ [ biography\ ] way that grammatical structures are constructed in our grammars -- i.e. , by means of the selection of grammatical features in system networks .</sentence>
				<definiendum id="0">Realization</definiendum>
				<definiens id="0">statements for the genre \ [ biography\ ] way that grammatical structures are constructed in our grammars -- i.e. , by means of the selection of grammatical features in system networks</definiens>
			</definition>
			<definition id="5">
				<sentence>Field describes the states and events and participants occurring in a particular situation .</sentence>
				<definiendum id="0">Field</definiendum>
				<definiens id="0">describes the states and events and participants occurring in a particular situation</definiens>
			</definition>
			<definition id="6">
				<sentence>INST-484 ) ( LEXICOGPAMMAR : EVENT-q OBJECT ) ) Semantic image created for this stage includes : concept : WORKER is a subconcept of concept : PERSON concept : HOUSINGAKEA is a subconcept of concept : DECOMPOSABLE-OBJECT concept : AEG is a subconcept of concept : NAMED-OBJECT concept : HENNINGSDORF is a subconcept of concept : THREE-D-LOCATION concept : BEHRENS is a subconcept of concept : PERSON concept : BUILD is a subconcept of concept : CREATIVE-MATERIAL-ACTION Following registerial instances selected : Unit with : syntagmatic structure : ( ACTOR TARGET FIELD-ACTIVITY ) with register features selected : ( SPATIALLY-LOCATED TEMPORALLY-UNLOCATED HUMANITIES EXPLORATION WRITTEN-TRANSMISSION CENTRAL-PARTICIPANT MEACREATION-REALiZATION DOA-ACTIVITIES ACTIVITY-EXPECTANCY TIMES-PAST ACTIViTY-SEQUENCE FIELD UNMARKED-AFFECT ONE-OFF UNINVOLVED-CONTACT UNEQUAL-STATUS TENOR NONPSEUDO-UNPROJECTED UNPROJECTEDGENRE-STRUCTURED SOLIDIFIED VISUALLY-0BJECTIFIED INFORMING DOCUMENTATION PUBLIC-GENERAL PUBLIC AURAL-NONE VISUAL-NONE MODE REGISTER MEACP4~ATION START ) With substructure : Figure 5 : Extract from register profile 159 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 oo , Setting local context according to constraints : ( ( CONSTRAINTS ( ( : EVENT-q OBJECT ) ) ) ( MACR0-THEMES ( BEHKENS ) ) ( DISC-INDIVIDUALS ( HENNINGSDORF AEG DARMSTADT BEHRENS ) ) ) Semantic input to lexicogrammar : ( ( V-626 / ( BUILD PROCESS ) : SPATIAL-LOCATING ( V-620 / ( HENNINGSDORF THREE-D-LOCATION NAMED-OBJECT OBJECT ) : NAME HENNINGSDORF ) : BENEFICIARY ( V-623 / ( WORKER OBJECT ) : PART-OF ( V-622 / ( AEG GROUP NAMED-OBJECT OBJECT ) : NAME AEG ) : SINGULARITY-Q NONSINGULAR : MULTIPLICITY-QMULTIPLE ) : ACTEE ( V-624 : ACTOR ( V-625 / ( HOUSINGARFEA OBJECT ) : SINGULARITY-Q SINGULAR : MULTIPLICITY-Q UNITARY ) / ( BEHRENS NAMED-OBJECT MALE OBJECT ) : NAME BEHRENS ) : TENSE PAST ) ) Discourse Semantics : an individual is being referred to : ( BEHRENS NAMED-OBJECTMALE OBJECT ) Already mentioned locally ; dynamically changing reference strategy .</sentence>
				<definiendum id="0">INST-484 ) ( LEXICOGPAMMAR</definiendum>
				<definiendum id="1">WORKER</definiendum>
				<definiendum id="2">HOUSINGAKEA</definiendum>
				<definiendum id="3">AEG</definiendum>
				<definiendum id="4">HENNINGSDORF</definiendum>
				<definiendum id="5">BEHRENS</definiendum>
				<definiendum id="6">HENNINGSDORF THREE-D-LOCATION NAMED-OBJECT OBJECT</definiendum>
				<definiendum id="7">SINGULARITY-Q NONSINGULAR</definiendum>
				<definiendum id="8">MULTIPLICITY-QMULTIPLE )</definiendum>
				<definiendum id="9">SINGULARITY-Q SINGULAR</definiendum>
				<definiens id="0">a subconcept of concept : DECOMPOSABLE-OBJECT concept</definiens>
				<definiens id="1">a subconcept of concept : NAMED-OBJECT concept</definiens>
				<definiens id="2">a subconcept of concept</definiens>
				<definiens id="3">a subconcept of concept : PERSON concept : BUILD is a subconcept of concept : CREATIVE-MATERIAL-ACTION Following registerial instances selected : Unit with : syntagmatic structure : ( ACTOR TARGET FIELD-ACTIVITY ) with register features selected : ( SPATIALLY-LOCATED TEMPORALLY-UNLOCATED HUMANITIES EXPLORATION WRITTEN-TRANSMISSION CENTRAL-PARTICIPANT MEACREATION-REALiZATION DOA-ACTIVITIES ACTIVITY-EXPECTANCY TIMES-PAST ACTIViTY-SEQUENCE FIELD UNMARKED-AFFECT ONE-OFF UNINVOLVED-CONTACT UNEQUAL-STATUS TENOR NONPSEUDO-UNPROJECTED UNPROJECTEDGENRE-STRUCTURED SOLIDIFIED VISUALLY-0BJECTIFIED INFORMING DOCUMENTATION PUBLIC-GENERAL PUBLIC AURAL-NONE VISUAL-NONE MODE REGISTER MEACP4~ATION START</definiens>
				<definiens id="4">EVENT-q OBJECT ) ) ) ( MACR0-THEMES ( BEHKENS ) ) ( DISC-INDIVIDUALS ( HENNINGSDORF AEG DARMSTADT BEHRENS ) ) ) Semantic input to</definiens>
				<definiens id="5">MULTIPLICITY-Q UNITARY ) / ( BEHRENS NAMED-OBJECT MALE OBJECT ) : NAME BEHRENS ) : TENSE PAST ) ) Discourse Semantics : an individual is being referred to : ( BEHRENS NAMED-OBJECTMALE OBJECT ) Already mentioned locally</definiens>
			</definition>
</paper>

		<paper id="0204">
			<definition id="0">
				<sentence>E £ and ( 2 ) I I E £ or i I = 0 ; Replacements£ is the set of replacements over £ .</sentence>
				<definiendum id="0">Replacements£</definiendum>
				<definiens id="0">the set of replacements over £</definiens>
			</definition>
			<definition id="1">
				<sentence>US-strings£ is the set of strings over the set £2 U \ [ £ x { 0 } \ ] of replacements .</sentence>
				<definiendum id="0">US-strings£</definiendum>
				<definiens id="0">the set of strings over the set £2 U \ [ £ x { 0 } \ ] of replacements</definiens>
			</definition>
			<definition id="2">
				<sentence>33 We use roman letters to denote themselves : for instance , T denotes the letter I. Boldface letters denote constant replacements : for instance , I is the pair ( l , l ) .</sentence>
				<definiendum id="0">T</definiendum>
				<definiens id="0">for instance</definiens>
			</definition>
			<definition id="3">
				<sentence>RULE NOTATION AND EXAMPLES The rules with which we are concerned are like the rewrite rules of generative phonology ; they are general , context-conditioned replacements .</sentence>
				<definiendum id="0">RULE NOTATION</definiendum>
				<definiens id="0">generative phonology ; they are general , context-conditioned replacements</definiens>
			</definition>
			<definition id="4">
				<sentence>Language : Let £ = { a , b , ... , z , + , # , ' , i } Declare the following subsets of £ : C = { b , c , d , f , g , h , j , k , l , m , n , p , q , r , s , t , v , w , x , y , z } Csib = { s , x , z } Example rules : Example 1 Rule encoding : { d ) , { ( +,0 ) } ) Rule notation : + -- ~ 0 \ [ Rule description : Delete + .</sentence>
				<definiendum id="0">Language</definiendum>
				<definiens id="0">C = { b , c , d , f , g , h , j , k , l , m , n , p , q , r</definiens>
			</definition>
			<definition id="5">
				<sentence>A context type is a pair C = ( X , Y ) , where X and Y are sets of US-Strings .</sentence>
				<definiendum id="0">context type</definiendum>
				<definiens id="0">sets of US-Strings</definiens>
			</definition>
			<definition id="6">
				<sentence>An indexed US-String over £ is a triple ( as , l , y ) , where a , y E US .</sentence>
				<definiendum id="0">US-String over £</definiendum>
			</definition>
			<definition id="7">
				<sentence>An indexed US-string is a presentation of a nonempty US-string that divides the string into three components : ( 1 ) a replacement occurring in the string , ( 2 ) the material to the left of that replacement , and ( 3 ) the material to the right of it .</sentence>
				<definiendum id="0">indexed US-string</definiendum>
				<definiens id="0">a presentation of a nonempty US-string that divides the string into three components : ( 1 ) a replacement occurring in the string , ( 2 ) the material to the left of that replacement</definiens>
			</definition>
			<definition id="8">
				<sentence>Finite encodability A subset X of US-strings j : is left-encoded by a set U in case X = LeftExp ( U ) , and is rightencoded by 17 in case X = RightExp ( V ) .</sentence>
				<definiendum id="0">Finite encodability A subset X</definiendum>
			</definition>
			<definition id="9">
				<sentence>X21 ) , n = max ( I Yll , I Y zl ) , and k is the length of the longest string in Y1 U Y2 .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">the length of the longest string in Y1 U Y2</definiens>
			</definition>
			<definition id="10">
				<sentence>A } , where S is a finite set of states , i E S is the initial state , T C S is the set of terminal states , and .</sentence>
				<definiendum id="0">S</definiendum>
				<definiendum id="1">T C S</definiendum>
				<definiens id="0">the initial state</definiens>
				<definiens id="1">the set of terminal states</definiens>
			</definition>
			<definition id="11">
				<sentence>Ad is a triple ( % 1 , 7r ' ) where 7r , 7c ' are paths , and l , n E US-strings£ .</sentence>
				<definiendum id="0">Ad</definiendum>
				<definiens id="0">a triple</definiens>
			</definition>
			<definition id="12">
				<sentence>EXAMPLE : SPELLING RULES FOR ENGLISH STEM+SUFFIX COMBINATIONS The following is an adaptation of the treatment in Antworth ( 1990 ) of English spelling rules , which 4This use of competition builds some directional bias into the definition of DFSM 's , i.e. , some preference for their use in generation .</sentence>
				<definiendum id="0">EXAMPLE</definiendum>
				<definiens id="0">SPELLING RULES FOR ENGLISH STEM+SUFFIX COMBINATIONS The following is an adaptation of the treatment in Antworth ( 1990 ) of English spelling rules , which 4This use of competition builds some directional bias into the definition of DFSM 's , i.e. , some preference for their use in generation</definiens>
			</definition>
			<definition id="13">
				<sentence>The proof of NP-hardness is a reduction of 3-SAT .</sentence>
				<definiendum id="0">proof of NP-hardness</definiendum>
				<definiens id="0">a reduction of 3-SAT</definiens>
			</definition>
			<definition id="14">
				<sentence>, where ~ is the the set of non-terminals plus the set of terminals .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">the the set of non-terminals plus the set of terminals</definiens>
			</definition>
</paper>

		<paper id="0102">
			<definition id="0">
				<sentence>k of English \ [ 54\ ] ; BNC \ [ 441 , \ [ 22\ ] ; Brown \ [ 24\ ] ; ICE \ [ 12\ ] , \ [ 28\ ] , \ [ 64\ ] ; Lancaster-IBM \ [ 26\ ] , \ [ 22\ ] ; LOB \ [ 1\ ] , \ [ 3\ ] , \ [ 38\ ] , \ [ .12\ ] ; London-Lund \ [ 62\ ] ; Nijmegen \ [ 12\ ] ; PoW \ [ 23\ ] , \ [ 55\ ] , \ [ 57\ ] ; SEC \ [ 631 ; TOSCA \ [ 46\ ] , \ [ 311 , \ [ 12\ ] ; UPenn \ [ 53\ ] , \ [ 45\ ] ; etc ) are used , among other things , as atithoritative exal n pies by researchers in English Language Teaching and Lexicography ( e.g. \ [ 44\ ] ) , and as training data for statistical syntactic constraint models to improve recognition accuracy in speech and handwriting recognisers ( e.g. \ [ 37\ ] , \ [ I01 ) .</sentence>
				<definiendum id="0">London-Lund</definiendum>
				<definiens id="0">\ [ 62\ ] ; Nijmegen \ [ 12\ ] ; PoW \ [ 23\ ] , \ [ 55\ ] , \ [ 57\ ] ; SEC \ [ 631 ; TOSCA \ [ 46\ ] , \ [ 311 , \ [ 12\ ] ; UPenn \ [ 53\ ] , \ [ 45\ ] ; etc ) are used</definiens>
			</definition>
			<definition id="1">
				<sentence>The SEC is a collection of recordings of radio broadcasts with accompanying annotated transcriptie , s , collected by Lancaster University and IBM UK as a general research resource .</sentence>
				<definiendum id="0">SEC</definiendum>
				<definiens id="0">a collection of recordings of radio broadcasts with accompanying annotated transcriptie , s , collected by Lancaster University and IBM UK as a general research resource</definiens>
			</definition>
			<definition id="2">
				<sentence>( ; crry Knowles ( Lancaster ) and Peter Roach ( Leeds ) are collaborating in an ESRC-funded project to set up a time-aligned database of recorded speech , accompanied by phonetic and graphemic transcriptions .</sentence>
				<definiendum id="0">crry Knowles</definiendum>
				<definiens id="0">collaborating in an ESRC-funded project to set up a time-aligned database of recorded speech , accompanied by phonetic and graphemic transcriptions</definiens>
			</definition>
			<definition id="3">
				<sentence>The non-corpus-based Generalised Phrase Structure Grammar ( GPSG ) ( as used in the Alvey Natural Language Toolkit ANLT ) should also be included .</sentence>
				<definiendum id="0">non-corpus-based Generalised Phrase Structure Grammar ( GPSG )</definiendum>
				<definiens id="0">used in the Alvey Natural Language Toolkit ANLT ) should also be included</definiens>
			</definition>
			<definition id="4">
				<sentence>Although the Text Encoding Initiaitive ( TEI ) \ [ 61\ ] , \ [ 18\ ] offers general guidelines for text formatting standards , and some corpora ( including BNC , ICE ) aim to be `` rEI-conformant '' , in practice it seems almost as hard for Corpus linguists to agree to accept a single annotation format as it is to agree on a single annotation scheme .</sentence>
				<definiendum id="0">Text Encoding Initiaitive ( TEI</definiendum>
				<definiens id="0">hard for Corpus linguists to agree to accept a single annotation format as it is to agree on a single annotation scheme</definiens>
			</definition>
			<definition id="5">
				<sentence>The default mapping is to HP ( pronominal head of the nominal group ) , but if the CELEX lexicon contains subcategorisation information in the form of a Y in column 8 , then we can assign the label for wh-pronou , head ( HWH ) .</sentence>
				<definiendum id="0">default mapping</definiendum>
				<definiens id="0">pronominal head of the nominal group</definiens>
			</definition>
			<definition id="6">
				<sentence>The SEC consists of transcripts of scripted ( and probably rehearsed ) radio broadcasts .</sentence>
				<definiendum id="0">SEC</definiendum>
				<definiens id="0">consists of transcripts of scripted ( and probably rehearsed ) radio broadcasts</definiens>
			</definition>
			<definition id="7">
				<sentence>'Looking Up : An Account of the COBUILD Project in Lexical Computing .</sentence>
				<definiendum id="0">'Looking Up</definiendum>
			</definition>
</paper>

		<paper id="0201">
			<definition id="0">
				<sentence>Now we deline ti = h iftl -- I\ [ , , IH and ii = l ifti =L , , I.L. Generalising our equation once more , we have the following , where R is a factor called the transition ratio .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a factor called the transition ratio</definiens>
			</definition>
</paper>

		<paper id="0329">
</paper>

		<paper id="0301">
			<definition id="0">
				<sentence>&lt; CLAUSE &gt; BECAUSE &lt; CLAUSE &gt; &lt; CLAUSE &gt; BECAUSE OF &lt; NP &gt; &lt; CLAUSE &gt; DUE TO &lt; NP &gt; &lt; CLAUSE &gt; SO &lt; CLAUSE &gt; &lt; CLAUSE &gt; CONSEQUENTLY &lt; CLAUSE &gt; &lt; NP &gt; CAUSE &lt; NP &gt; &lt; NP &gt; BE TIIE RESULT OF &lt; NP &gt; Assign a name to the relation assumed to underlie each such group -in the present case , CAUSATION is a good mnemonic candidate , though RELATION.1 would also do -and the preliminary semantic analysis of discourse relations is complete .</sentence>
				<definiendum id="0">CAUSATION</definiendum>
				<definiens id="0">a good mnemonic candidate</definiens>
			</definition>
			<definition id="1">
				<sentence>I do n't believe that endorsing the Nuclear Freeze Initiative is the right step for California CC .</sentence>
				<definiendum id="0">Nuclear Freeze Initiative</definiendum>
				<definiens id="0">the right step for California CC</definiens>
			</definition>
			<definition id="2">
				<sentence>Next to each discourse relation ( underlined once ) or rhetorical type ( underlined twice ) is a parenthesized indication of the pattern used in our sample variant .</sentence>
				<definiendum id="0">discourse relation</definiendum>
				<definiens id="0">a parenthesized indication of the pattern used in our sample variant</definiens>
			</definition>
</paper>

		<paper id="0203">
			<definition id="0">
				<sentence>If L is a language and W is a set of .</sentence>
				<definiendum id="0">W</definiendum>
				<definiens id="0">a language</definiens>
			</definition>
			<definition id="1">
				<sentence>t ) jects , and P ( W ) is the set of all snl ) sets of W , then the interpretation function I ma .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the set of all snl ) sets of W , then the interpretation function I ma</definiens>
			</definition>
			<definition id="2">
				<sentence>( 2 ) Term Interpretation l • io Io ( l ) X A Y I ( X ) N I ( Y ) X VY I ( X ) UI ( Y ) -~x w \ i ( x ) With this interpretation function , we can determine that big A animate A slow is a CONTRADICTION having a null interpretation in W , while big V slow is a TAUTOLOGY as I ( big V slow ) is the same as I ( big ) U I ( slow ) which equals W. The term PREDICATE will be used to describe a statement in a language which has a well-defined interpretation .</sentence>
				<definiendum id="0">Y I</definiendum>
				<definiendum id="1">slow</definiendum>
				<definiendum id="2">I</definiendum>
				<definiens id="0">the same as I ( big ) U I ( slow ) which equals W. The term PREDICATE will be used to describe a statement in a language which has a well-defined interpretation</definiens>
			</definition>
			<definition id="3">
				<sentence>A modal theory consists of a universe U is a set of worlds Wj , jew , called TYPES , together with a set of relations Rk , kET¢ : Wdom ( j ) ~ Wcod ( k ) from one world to another .</sentence>
				<definiendum id="0">modal theory</definiendum>
				<definiens id="0">consists of a universe U is a set of worlds Wj , jew , called TYPES , together with a set of relations Rk , kET¢ : Wdom ( j ) ~ Wcod ( k ) from one world to another</definiens>
			</definition>
			<definition id="4">
				<sentence>A MODAL OPERATOR rk is a special symI ) ol in tile language which is interpreted as the relation Rk .</sentence>
				<definiendum id="0">MODAL OPERATOR rk</definiendum>
				<definiens id="0">a special symI ) ol in tile language which is interpreted as the relation Rk</definiens>
			</definition>
			<definition id="5">
				<sentence>26 I Constraints in a modal theory In model-theoretic terms , a constraint is any wellformed expression in the language to which an interpretation is attached .</sentence>
				<definiendum id="0">constraint</definiendum>
				<definiens id="0">any wellformed expression in the language to which an interpretation is attached</definiens>
			</definition>
			<definition id="6">
				<sentence>Prince &amp; Smolensky ( 1993:3 ) state that constraints are essentially universal and of very general formulation ... interlinguistic differences arise from the permutation of constraint-Tunking .</sentence>
				<definiendum id="0">Prince</definiendum>
				<definiens id="0">essentially universal and of very general formulation ... interlinguistic differences arise from the permutation of constraint-Tunking</definiens>
			</definition>
</paper>

		<paper id="0315">
			<definition id="0">
				<sentence>In a context of discourse , the message planning component is a text planner and the message is a text plan *This work was realized at the lstituto per la Ricerca Scientifica e Tecnolo # ica ( IRST ) in Trento ( Italy ) .</sentence>
				<definiendum id="0">message planning component</definiendum>
				<definiendum id="1">message</definiendum>
				<definiendum id="2">IRST</definiendum>
				<definiens id="0">a text planner and the</definiens>
			</definition>
			<definition id="1">
				<sentence>In a context of dialogue , the message planner is a 'fictitious ' component consisting of a dialogue act planner , which precedes the generation system , and a planner for surface linguistic acts , which is a component of the generation system .</sentence>
				<definiendum id="0">message planner</definiendum>
				<definiens id="0">a 'fictitious ' component consisting of a dialogue act planner , which precedes the generation system</definiens>
				<definiens id="1">a component of the generation system</definiens>
			</definition>
			<definition id="2">
				<sentence>Meteer distinguished the COMPOSITE-MATRIXADJUNCT structure , which corresponds to the hypotactic relation ( i.e. , the relation between dependent elements and their dominating node ) , and the COMPOSITECOORDINATE structure , which represents the paratactic relation ( i.e. , the relation between elements of equal status ) .</sentence>
				<definiendum id="0">hypotactic relation</definiendum>
				<definiens id="0">the relation between dependent elements and their dominating node</definiens>
				<definiens id="1">represents the paratactic relation ( i.e. , the relation between elements of equal status )</definiens>
			</definition>
			<definition id="3">
				<sentence>For example , the lexical resource which corresponds to the direct transitive verb `` to write '' is defined as follows : \ [ `` to-write '' \ ] ( direct-transltive-nucleus ) ( write-action ) A lexical resource can be linked to more than one textual category ; for example , the following resources `` article '' \ ] ( slngular-neuter-nucleus plural-neuter-nucleus ) ( a icle ) `` quick '' \ ] ( object-modifier clause-modifier ) ( quick ) correspond to the nouns `` article '' and `` articles '' and to the adjective `` quick '' and the adverb `` quickly '' , respectively .</sentence>
				<definiendum id="0">direct-transltive-nucleus ) ( write-action</definiendum>
				<definiens id="0">slngular-neuter-nucleus plural-neuter-nucleus ) ( a icle ) `` quick '' \ ] ( object-modifier clause-modifier ) ( quick ) correspond to the nouns `` article '' and `` articles '' and to the adjective `` quick '' and the adverb `` quickly '' , respectively</definiens>
			</definition>
			<definition id="4">
				<sentence>The component consists of an integrated system for editing , modifying and maintaining unification-based grammars .</sentence>
				<definiendum id="0">component</definiendum>
			</definition>
</paper>

		<paper id="0306">
			<definition id="0">
				<sentence>Instructional tex-ts have been the object of many studies recently , motivated by the increased need to produce manuals ( especially multilingual manuals ) coupled with the cost of translators and technical writers .</sentence>
				<definiendum id="0">Instructional tex-ts</definiendum>
				<definiens id="0">especially multilingual manuals ) coupled with the cost of translators and technical writers</definiens>
			</definition>
</paper>

		<paper id="0321">
			<definition id="0">
				<sentence>The Interactive Discourse Planner ( IDP ) is designed to plan text to describe and/or justify a domain plan interactively .</sentence>
				<definiendum id="0">Interactive Discourse Planner ( IDP</definiendum>
				<definiens id="0">designed to plan text to describe and/or justify a domain plan interactively</definiens>
			</definition>
			<definition id="1">
				<sentence>IDP uses questions from the user to recognize how to extend its own text plan in a way that both satisfies its listener and achieves the system 's discourse goal .</sentence>
				<definiendum id="0">IDP</definiendum>
				<definiens id="0">uses questions from the user to recognize how to extend its own text plan in a way that both satisfies its listener and achieves the system 's discourse goal</definiens>
			</definition>
			<definition id="2">
				<sentence>IDP detects a digression when the user asks a question about a discourse entity or a proposition that is part of the system 's text plan , and the answer to the question can not be ir/corporated into the text plan .</sentence>
				<definiendum id="0">IDP</definiendum>
				<definiens id="0">detects a digression when the user asks a question about a discourse entity or a proposition that is part of the system 's text plan</definiens>
			</definition>
			<definition id="3">
				<sentence>In this example , IDP fails to make any coherent connection between the question and the text plan that they system has used so far .</sentence>
				<definiendum id="0">IDP</definiendum>
				<definiens id="0">fails to make any coherent connection between the question and the text plan that they system has used so far</definiens>
			</definition>
			<definition id="4">
				<sentence>The EES Text Planner records its discourse goals and the plans that it formulates to achieve them in a dialogue history .</sentence>
				<definiendum id="0">EES Text Planner</definiendum>
				<definiens id="0">records its discourse goals and the plans that it formulates to achieve them in a dialogue history</definiens>
			</definition>
			<definition id="5">
				<sentence>The dialogue history is a stack of text plans from previous exchanges .</sentence>
				<definiendum id="0">dialogue history</definiendum>
			</definition>
			<definition id="6">
				<sentence>IDP deduces propositions that state how acts decompose into plans ( ACT-PLAN case frame ) by satisfying a rule 's antecedents .</sentence>
				<definiendum id="0">IDP</definiendum>
			</definition>
			<definition id="7">
				<sentence>IDP uses two types of text plans ( TPs ) separating those plans that address the system 's discourse goat ( DG ) directly , from those plans that provide additional information that augments the system 's essential text message .</sentence>
				<definiendum id="0">IDP</definiendum>
				<definiens id="0">uses two types of text plans ( TPs ) separating those plans that address the system 's discourse goat ( DG ) directly , from those plans that provide additional information that augments the system 's essential text message</definiens>
			</definition>
			<definition id="8">
				<sentence>Therefore , IDP considers the growth points for the DTPs on the active path in the following order : IDP prefers DTPs that replan a DTP over CTPs that expand a DTP .</sentence>
				<definiendum id="0">IDP</definiendum>
				<definiens id="0">considers the growth points for the DTPs on the active path in the following order : IDP prefers DTPs that replan a DTP over CTPs that expand a DTP</definiens>
			</definition>
			<definition id="9">
				<sentence>The topic is the proposition that the listener questions in her digressive question .</sentence>
				<definiendum id="0">topic</definiendum>
				<definiens id="0">the proposition that the listener questions in her digressive question</definiens>
			</definition>
			<definition id="10">
				<sentence>In the second stage , if there is a topic , the Analyzer collects CTPs ( ctps ) that use the topic as a nuclear proposiFigure 4 : Detecting Digressive Questions from the Listener 's Feedback IS-DIGRESSION-TYPE-1 ( fb ) If fb = `` Why '' { `` not '' } proposition and DONE ( system , say ( proposition ) ) Then return ( proposition ) Else return ( 'nil ) Figure 5 : Test 1 : For Detecting Digressive Questions Based on What the System Said Previously tion .</sentence>
				<definiendum id="0">CTPs</definiendum>
				<definiens id="0">Test 1 : For Detecting Digressive Questions Based on What the System Said Previously tion</definiens>
			</definition>
			<definition id="11">
				<sentence>The Analyzer uses the first digression test , IS-DIGRESSION-TYPE-I , that is described in Figure 5 .</sentence>
				<definiendum id="0">Analyzer</definiendum>
				<definiens id="0">uses the first digression test</definiens>
			</definition>
			<definition id="12">
				<sentence>, the Analyzer tries to find a way to expand the DTP-portion of its 185 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 TP on the active path ( Section 5.3 ) .</sentence>
				<definiendum id="0">Analyzer</definiendum>
			</definition>
			<definition id="13">
				<sentence>First , the Analyzer gathers all the propositions that IDP has said as the content of a say act ( expressed-props ) .</sentence>
				<definiendum id="0">Analyzer</definiendum>
				<definiens id="0">gathers all the propositions that IDP has said as the content of a say act ( expressed-props )</definiens>
			</definition>
			<definition id="14">
				<sentence>Next , the Analyzer tries to find a CTP that uses the proposition as a nucleus ( Figure 4 ) .</sentence>
				<definiendum id="0">Analyzer</definiendum>
			</definition>
			<definition id="15">
				<sentence>Next , the Analyzer attempts to process the question as a digression ( Section 6 ) .</sentence>
				<definiendum id="0">Analyzer</definiendum>
			</definition>
			<definition id="16">
				<sentence>The SNePS Actor models a cognitive agent operating in a single-agent world .</sentence>
				<definiendum id="0">SNePS Actor</definiendum>
				<definiens id="0">models a cognitive agent operating in a single-agent world</definiens>
			</definition>
</paper>

		<paper id="0308">
			<definition id="0">
				<sentence>The English Results The English corpus is made up of 451 clauses ( approximately 3500 words ) taken from 9 instruction manuals intended for non-expert readers .</sentence>
				<definiendum id="0">English corpus</definiendum>
				<definiens id="0">made up of 451 clauses ( approximately 3500 words ) taken from 9 instruction manuals intended for non-expert readers</definiens>
			</definition>
</paper>

		<paper id="0105">
			<definition id="0">
				<sentence>The probabilities were calculated using the following equation , where X~ : -- ~ Y~ # # Z~s~ is a specific schema , X is the set of X-bar schemata and A and B and C are variables over category , SPEC and COMP feature bundles : c , z &amp; ) z P ( X~ • zS'Ixcb , ) = C ( A ~ B C ) ( I ) This is different to manner in which probabilities are collected for stochastic context-free grammars , where the identity of the mother node is taken into account , as in the equation below : c ( x : r s ' c , Z &amp; ) P ( xcS : ~ YcI ' ZcS : Ix ) = C ( X~ : -- ~ B ( 2 ) C ) This would result in misleading probabilities for the Xbar schemata since the use of schemata ( 3 ) , ( 4 ) , and ( 5 ) would immediately bring down the probability of a parse compared to a parse of the same string which happened to use only ( 1 ) and ( 2 ) .</sentence>
				<definiendum id="0">X~</definiendum>
				<definiendum id="1">X</definiendum>
				<definiens id="0">the set of X-bar schemata and A and B and C are variables over category , SPEC and COMP feature bundles : c , z &amp; ) z P ( X~ • zS'Ixcb , ) = C ( A ~ B C ) ( I ) This is different to manner in which probabilities are collected for stochastic context-free grammars , where the identity of the mother node is taken into account , as in the equation below : c ( x : r s ' c , Z &amp; ) P ( xcS : ~ YcI ' ZcS : Ix ) = C</definiens>
			</definition>
			<definition id="1">
				<sentence>The probal ) ilities for em'h of tim verbs ' thcta t ; l'hls were calculated using the equati~ m bch Jw , w her ( , I ' ( s , It , ) is the probability of the theta grid st occurring with thc verb v , ( v , si ) is an occurrence of the items in si being licensed by v , and S ranges over all theta gr !</sentence>
				<definiendum id="0">si )</definiendum>
				<definiens id="0">the probability of the theta grid st occurring with thc verb v</definiens>
				<definiens id="1">an occurrence of the items in si being licensed by v , and S ranges over all theta gr</definiens>
			</definition>
			<definition id="2">
				<sentence>Discussion Limitations of the Grammar The grammar employed is a partial characterisation of Chomsky 's Government-Binding theory \ [ Chomsky1981 , Chomsky1986\ ] and only takes account of very local constralnts ( i.e. X-bar , Theta and Case ) ; a way of encoding all constraints in the proper branch formalism ( e.g. \ [ Crocker1992\ ] ) will be needed before a grammar of sufficient coverage to be useful in corpora analysis can be formulated .</sentence>
				<definiendum id="0">Discussion Limitations</definiendum>
				<definiendum id="1">grammar employed</definiendum>
				<definiens id="0">a way of encoding all constraints in the proper branch formalism</definiens>
			</definition>
</paper>

		<paper id="0316">
			<definition id="0">
				<sentence>In text generation , content selection and discourse organization ( i.e. the text planning tasks ) have often been dealt with independently from the linguistic realization of the tezt planthe information selected and structured by the text planning process ( eft , e.g. , \ [ McKeown and Swartout , 1987\ ] ) .</sentence>
				<definiendum id="0">discourse organization</definiendum>
				<definiens id="0">information selected and structured by the text planning process</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , Operz is defined as taking the ACTOR as grammatical'subject and the keyword of the situation as direct object .</sentence>
				<definiendum id="0">Operz</definiendum>
				<definiens id="0">taking the ACTOR as grammatical'subject and the keyword of the situation as direct object</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , SHOOTING is a situation with the predicate named shooting , shot , etc. ; with the participants ACTOR ( named gunman , marksman , etc. ) and ACTEE ( named target , victim , ... ) ; with the obligatory circumstantials INSTRUMENT ( r/fie , gun , etc. ) and MEANS ( bullet ) , and optional circumstantials LOCATION , TIME , etc. , which do not have a situation-specific lexicalization .</sentence>
				<definiendum id="0">SHOOTING</definiendum>
				<definiens id="0">a situation with the predicate named shooting , shot , etc. ; with the participants ACTOR ( named gunman , marksman , etc. ) and ACTEE ( named target , victim , ...</definiens>
			</definition>
			<definition id="3">
				<sentence>The constituent layer contains feature specifications for the realization of predicate constituents and their attributes , s For example , to name the ACTOR in SHOOTING , his/her proper name ; a situation non-specific realization , e.g. , his/her function in relation to other entities in the knowledge base ( e.g. , grandfather ) ; the situation-specific and contextually neutral lexeme marksman ; or a negatively loaded situation-related expression such as murderer , killer , etc. can be chosen .</sentence>
				<definiendum id="0">killer</definiendum>
				<definiens id="0">the situation-specific and contextually neutral lexeme marksman ; or a negatively loaded situation-related expression such as murderer ,</definiens>
			</definition>
			<definition id="4">
				<sentence>As stated above , LF sequences ( and LFs in general ) are organized in terms of their semantic , functional , and syntactic features .</sentence>
				<definiendum id="0">LF sequences</definiendum>
				<definiendum id="1">LFs</definiendum>
				<definiens id="0">in general ) are organized in terms of their semantic , functional , and syntactic features</definiens>
			</definition>
			<definition id="5">
				<sentence>Dob~ and Novak \ [ Dob~ and Novak , 1992\ ] use RST structure relations and the Ted Structure representation proposed by Meteer in parallel : via RST relations , the content selection and discourse organization is done ; the representation of the arguments of the RST relations chosen in terms of Tezt Structures ensures the linguistic realization and provides constraints for the guidance of the process of content selection and discourse organization .</sentence>
				<definiendum id="0">Dob~</definiendum>
			</definition>
</paper>

		<paper id="0107">
			<definition id="0">
				<sentence>For example , the fourth entry in the Table 1 reads that the tree a2 , anchored by a verb ( V ) , has a left , and a right dependent ( - , + ) and the first word to the left ( -1 ) with the tree as serves as a dependent of the current word .</sentence>
				<definiendum id="0">V</definiendum>
				<definiens id="0">a dependent of the current word</definiens>
			</definition>
</paper>

		<paper id="0310">
			<definition id="0">
				<sentence>Cohesion , to Halliday and Hasan , is the defining property of a text qua text -- what it is that makes a text a semantic unit rather than a jumble of unconnected phrases .</sentence>
				<definiendum id="0">Cohesion</definiendum>
				<definiens id="0">the defining property of a text qua text -- what it is that makes a text a semantic unit rather than a jumble of unconnected phrases</definiens>
			</definition>
			<definition id="1">
				<sentence>Text Structure is the level where the selected content is first rendered into an abstract linguistic form that dictates its approximate sequential order and hierarchical structure .</sentence>
				<definiendum id="0">Text Structure</definiendum>
				<definiens id="0">the level where the selected content is first rendered into an abstract linguistic form that dictates its approximate sequential order and hierarchical structure</definiens>
			</definition>
			<definition id="2">
				<sentence>instead the same sentence where Bob is a writer of detective novels , where the interpretation `` writing '' because of the sortal specification of the subject in determining the type of the object .</sentence>
				<definiendum id="0">Bob</definiendum>
			</definition>
			<definition id="3">
				<sentence>The Text Structure is imposing the contraint that the unit convey information of type duration ; this parameter provides it .</sentence>
				<definiendum id="0">Text Structure</definiendum>
				<definiens id="0">imposing the contraint that the unit convey information of type duration</definiens>
			</definition>
</paper>

		<paper id="0311">
			<definition id="0">
				<sentence>However , like some leading researchers in generation , we argue that it is of paramount importance to first know the kind of information that should be coded in the lexicon , which means to pay more attention to `` the nature of words '' ( McDonald , 1988 ) and to have a `` real knowledge of \ [ the\ ] lexical semantics '' , as was pointed out by Marcus ( 1987 ) : `` In some important sense , \ [ the\ ] systems have no real knowledge of lexical semantics ... . They use fragments of linguistic structure which eventually have words as their frontiers , but have little or no explicit knowledge of what these words mean . ''</sentence>
				<definiendum id="0">] systems</definiendum>
			</definition>
			<definition id="1">
				<sentence>Sinclair ( 1991 ) states that `` a word which occurs in close proximity to a word under investigation is called a collocate of it ... Collocation is the occurrence of two or more words within a short space of each other in a text '' .</sentence>
				<definiendum id="0">Collocation</definiendum>
				<definiens id="0">a word which occurs in close proximity to a word under investigation is called a collocate of it ...</definiens>
			</definition>
			<definition id="2">
				<sentence>In this theory , lexicM knowledge is encoded in an entry of the Explanatory Combinatorial Dictlonary , each entry being divided into three zones : the semantic zone ( a semantic network representing the meaning of the entry in terms of more primitive words ) , the syntactic zone ( the grammatical properties of the entry ) and the lexical combinatorics zone ( containing the values of the Lexical Functions ( LFs ) ) ~ .</sentence>
				<definiendum id="0">syntactic zone</definiendum>
				<definiens id="0">encoded in an entry of the Explanatory Combinatorial Dictlonary , each entry being divided into three zones : the semantic zone ( a semantic network representing the meaning of the entry in terms of more primitive words ) , the</definiens>
			</definition>
			<definition id="3">
				<sentence>LFs are central to the study of collocations and can be defined as the following : a lexicalfunction F is a correspondence which associates a lexical item L , called the key word of F , with a set of lexical items F ( L ) the value of F ( Mel'~uk , 1988 ) .</sentence>
				<definiendum id="0">LFs</definiendum>
				<definiendum id="1">lexicalfunction F</definiendum>
			</definition>
			<definition id="4">
				<sentence>GLT can be briefly characterized as a system which involves four 93 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 levels of representation which are connected by a set of generative devices accounting for a compositional interpretation of words in context , namely : the argument structure which specifies the predicate argument structure for a word and the conditions under which the variables map to syntactic expressions ; the event structure giving the particular event types such as S ( state ) , P ( process ) or T ( transition ) ; the qualia structure distributed among four roles FORM ( formal ) , CONST ( constitutive ) , TELIC and AGENT ( Agentive ) ; and the inheritance structure which involves two different kinds of mechanisms : • the fixed inheritance mechanism , which is basically a fixed network of the traditional isa relationship found in AI , enriched with the different roles of the qualia structure ; • the projective inheritance mechanism , which can be intuitively characterized as a way of triggering semantically related concepts which define for each role the projective conclusion space ( PCS ) .</sentence>
				<definiendum id="0">P ( process</definiendum>
				<definiendum id="1">FORM</definiendum>
				<definiens id="0">a system which involves four 93 7th International Generation Workshop • Kennebunkport</definiens>
				<definiens id="1">the argument structure which specifies the predicate argument structure for a word and the conditions under which the variables map to syntactic expressions</definiens>
				<definiens id="2">basically a fixed network of the traditional isa relationship found in AI , enriched with the different roles of the qualia structure ; • the projective inheritance mechanism , which can be intuitively characterized as a way of triggering semantically related concepts which define for each role the projective conclusion space ( PCS )</definiens>
			</definition>
			<definition id="5">
				<sentence>TELIC = drink ( P , v : individual , x ) | \ [ AGENT= produce ( T , w : brewer , x ) J Ms. Rifkind is a writer and editor living in New York .</sentence>
				<definiendum id="0">Ms. Rifkind</definiendum>
				<definiens id="0">a writer</definiens>
			</definition>
			<definition id="6">
				<sentence>Mr. Ferguson is an editorial writer for Scripps Howard News Service in Washington , D.C. writer ARGSTR = \ [ ARGI = x : author'\ [ | human-LCP \ ] J = |FORM = htman ( x ) L QUALIA L TELIC = write ( T , x , v : text ) 5For a broader account of the semantic interpretation of nominals , including nominalizations , see Pustejovsky and Anick ( 1988 ) .</sentence>
				<definiendum id="0">Mr. Ferguson</definiendum>
			</definition>
			<definition id="7">
				<sentence>Ancient is a relative adjective that submodifies the agentive role of the modified noun : ancient oRM= , , ... ...</sentence>
				<definiendum id="0">Ancient</definiendum>
				<definiens id="0">a relative adjective that submodifies the agentive role of the modified noun</definiens>
			</definition>
			<definition id="8">
				<sentence>In this view , ancient stories ( in example ( 3 ) ) are stories which were narrated in the past , so : distant_past ( e r ) A narrate ( e T , z , stories ) By contrast , the English adjective former is a property modifier and can only modify the telic role of the noun : former QUALIA \ [ change .</sentence>
				<definiendum id="0">ancient stories</definiendum>
				<definiendum id="1">English adjective former</definiendum>
				<definiens id="0">distant_past ( e r ) A narrate ( e T , z , stories ) By contrast , the</definiens>
			</definition>
			<definition id="9">
				<sentence>past ( e P ) A perform_the_job_of_architect ( e P , z ) In French , two adjectives with the same meaning past can modify these two roles : ancien and vieuz , which will receive the following feature structure ( which does not deal with the absolute sense ) : vleux QUALIA \ [ change_st ate-LCP \ ] LFORM = p- .</sentence>
				<definiendum id="0">vieuz</definiendum>
			</definition>
			<definition id="10">
				<sentence>Take the examples of Adj-Noun collocations involving grand and gros with nouns denoting activities : ( 4 ) a. un grand/gros mangeur ( a big eater ) b. un grand/gros fraudeur ( a big smuggler ) c. un *grand/gros client ( a big client ) d. un grand/*gros fumeur ( a heavy smoker ) e. un grand/*gros professeur ( a great professor ) Here , grand and gros are intensifiers of the predicate in the telic .</sentence>
				<definiendum id="0">Take the examples of Adj-Noun collocations</definiendum>
				<definiens id="0">a big smuggler ) c. un *grand/gros client ( a big client ) d. un grand/*gros fumeur ( a heavy smoker ) e. un grand/*gros professeur ( a great professor ) Here , grand and gros are intensifiers of the predicate in the telic</definiens>
			</definition>
</paper>

		<paper id="0108">
			<definition id="0">
				<sentence>guages vary in the way they encode whquestions .</sentence>
				<definiendum id="0">guages</definiendum>
				<definiens id="0">vary in the way they encode whquestions</definiens>
			</definition>
			<definition id="1">
				<sentence>Parameter Setting Proposal Let us suppose that there are n binary parameters each of which can take one of two values ( '+ ' or '- ' ) in a particular natural language .</sentence>
				<definiendum id="0">binary</definiendum>
				<definiens id="0">parameters each of which can take one of two values ( '+ ' or '- ' ) in a particular natural language</definiens>
			</definition>
			<definition id="2">
				<sentence>Consider text as generating tuples of the form ( v , d , w ) , where v is one of the top twenty words ( most of which are verbs ) , d is either the position to the left of the verb or to the right , and w is the word at that position .</sentence>
				<definiendum id="0">v</definiendum>
				<definiendum id="1">w</definiendum>
				<definiens id="0">one of the top twenty words ( most of which are verbs</definiens>
			</definition>
			<definition id="3">
				<sentence>Thus , nominative clitics ( eg. , `` je '' , `` tu '' , `` il '' , etc. ) occur first , followed by the negative element `` ne '' , fi ) llowed by accusative clitics ( eg. , `` la '' , `` me '' , `` re '' ) and dative clitics ( `` lui '' ) , followed , at last , I ) y the first element of the verbal sequence ( an auxiliary or the main verb ) .</sentence>
				<definiendum id="0">nominative clitics</definiendum>
				<definiendum id="1">accusative clitics</definiendum>
				<definiens id="0">an auxiliary or the main verb )</definiens>
			</definition>
</paper>

		<paper id="0206">
			<definition id="0">
				<sentence>6O A segment-specification consists of a character representation of some segment ( one or more characters , e.g. `` k '' or `` oh '' ) , plus a set of features , not all of which need be instantiated .</sentence>
				<definiendum id="0">A segment-specification</definiendum>
				<definiens id="0">consists of a character representation of some segment ( one or more characters , e.g. `` k '' or `` oh '' ) , plus a set of features , not all of which need be instantiated</definiens>
			</definition>
			<definition id="1">
				<sentence>An alphabet consists of a set of segmentspecifications .</sentence>
				<definiendum id="0">alphabet</definiendum>
			</definition>
			<definition id="2">
				<sentence>A ( phonetic ) word consists of a list of one or more segments , where each segment consists of a set Of features .</sentence>
				<definiendum id="0">phonetic ) word</definiendum>
				<definiens id="0">consists of a list of one or more segments , where each segment consists of a set Of features</definiens>
			</definition>
			<definition id="3">
				<sentence>A phonological rule consists of an input ( left-hand ) side , an output ( right-hand ) side , a left ; environment , and a right environment .</sentence>
				<definiendum id="0">phonological rule</definiendum>
				<definiens id="0">consists of an input ( left-hand ) side , an output ( right-hand ) side , a left ; environment , and a right environment</definiens>
			</definition>
			<definition id="4">
				<sentence>( Optional segments arise in analysis during the unapplication of deletion rules , as discussed later . )</sentence>
				<definiendum id="0">Optional segments</definiendum>
				<definiens id="0">arise in analysis during the unapplication of deletion rules</definiens>
			</definition>
			<definition id="5">
				<sentence>The resulting ( potential ) overgeneration is the reason for the test phase of the generate-and-test algorithm .</sentence>
				<definiendum id="0">overgeneration</definiendum>
				<definiens id="0">the reason for the test phase of the generate-and-test algorithm</definiens>
			</definition>
</paper>

		<paper id="0309">
			<definition id="0">
				<sentence>Abstract : This paper discusses the principles that should govern the construction of two components of a system for natural language generation ( NLG ) : ( 1 ) the ontology or , rather , as the paper argues , the 'ontological ' aspects of a belief system -and ( 2 ) the semantic representation of noun senses .</sentence>
				<definiendum id="0">Abstract</definiendum>
				<definiens id="0">This paper discusses the principles that should govern the construction of two components of a system for natural language generation ( NLG ) : ( 1 ) the ontology or</definiens>
			</definition>
			<definition id="1">
				<sentence>Keywords : ontology , system network , belief system , knowledge base , semantics , noun senses , natural language generation 'Or '' I One of the =lvens of Computational Linguistics ( CL ) which is taken here in a sense that includes Machine Translation ( MT ) is that any such system needs an ontology , l But what , precisely , is an ontology ?</sentence>
				<definiendum id="0">l</definiendum>
				<definiens id="0">ontology , system network , belief system , knowledge base</definiens>
				<definiens id="1">includes Machine Translation ( MT ) is that any such system needs an ontology ,</definiens>
			</definition>
			<definition id="2">
				<sentence>In fact , as Hovy and KnigSht ( 19~ ) state , the 'ontology base ' in Pangloss is in part based on Penman ( being a merger of this , the semantic categories from the Longman Dictionary of Contemporary English ( LDOCE ) , which is intended as a taxonomy for the nouns of English ) , and ULTRA ( Nirenburg and Defrise 1992 ) which itself draws on the LDOCE categories ) .</sentence>
				<definiendum id="0">ULTRA</definiendum>
			</definition>
			<definition id="3">
				<sentence>Linguists working in the Chomskyan tradition had recently introduced the concept of selectional restrictions which effectively presuppose semantic features .</sentence>
				<definiendum id="0">Linguists</definiendum>
				<definiens id="0">working in the Chomskyan tradition had recently introduced the concept of selectional restrictions which effectively presuppose semantic features</definiens>
			</definition>
			<definition id="4">
				<sentence>Here we shall consider just the two major SFG natural language generators : Penman , which is to be used as the generator for the current Pangloss project , and GENESYS , which is the generator in the COMMUNAL Project .</sentence>
				<definiendum id="0">GENESYS</definiendum>
				<definiens id="0">the generator in the COMMUNAL Project</definiens>
			</definition>
			<definition id="5">
				<sentence>The Penman Project was the first lar , ~e SFG-based generator , and the mare work that establishes the structure and nature of what Halliday has called the lexieogrammar was done in the very early eighties ( based fairly closely on Halliday 's work of the seventms ) .</sentence>
				<definiendum id="0">Penman Project</definiendum>
				<definiens id="0">establishes the structure and nature of what Halliday has called the lexieogrammar was done in the very early eighties ( based fairly closely on Halliday 's work of the seventms )</definiens>
			</definition>
			<definition id="6">
				<sentence>Thus Penman fails to utilize the many advantages of a fully integrated semantic network that provides for direct andintegrated realizations in grammar and lexis ( and indeed also in intonation or punctuataon ) .</sentence>
				<definiendum id="0">Penman</definiendum>
				<definiens id="0">fails to utilize the many advantages of a fully integrated semantic network that provides for direct andintegrated realizations in grammar and lexis</definiens>
			</definition>
			<definition id="7">
				<sentence>Once one commits oneself to having a separate component to handle reasoning ( includingproperty inheritance , etc ) that is OUTSIDE LANGUAGE ( even though , as I have always insisted , its internal structures are strongly INFLUENCED by language ) , then it becomes immediately clear that the system networks inside the language system are NOT in fact well-suited for use in reasoning .</sentence>
				<definiendum id="0">OUTSIDE LANGUAGE</definiendum>
				<definiens id="0">strongly INFLUENCED by language ) , then it becomes immediately clear that the system networks inside the language system are NOT in fact well-suited for use in reasoning</definiens>
			</definition>
</paper>

		<paper id="0112">
</paper>

		<paper id="0106">
			<definition id="0">
				<sentence>fives ) satisfies the opposite constraint containing adjectives of relatively low frequency ( between 50 and 250 ) .</sentence>
				<definiendum id="0">fives )</definiendum>
			</definition>
</paper>

		<paper id="0304">
			<definition id="0">
				<sentence>Propositions are denoted by ( P rl r2 * ) where P is a predicate , the ri are role fillers , and * denotes 0 or more additional role fillers .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">a predicate , the ri are role fillers , and * denotes 0 or more additional role fillers</definiens>
			</definition>
			<definition id="1">
				<sentence>Dora=in Knowledge Constrains Natural Ordering Differentia Context If ( N z tt ) where N is a Natural-Ordering and z is in the predecessor role of N , If ( Subsumption c s ) and s is differentiated within c by ( P s * ) If C is context in which S holds then ( N z v , =~ b ' ) then ( Subsumption c s ) , ue~ ( p s * ) .</sentence>
				<definiendum id="0">Dora=in Knowledge Constrains Natural Ordering Differentia Context If</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">a Natural-Ordering and z is in the predecessor role of N , If ( Subsumption c s ) and s is differentiated within c by ( P s * ) If C is context in which S holds then ( N z v , =~ b ' ) then ( Subsumption c s ) , ue~ ( p s * )</definiens>
			</definition>
			<definition id="2">
				<sentence>Lavid &amp; Hovy ( unpublished working paper ) define `` theme '' as `` that element that informs the listener as discourse unfolds how to relate the incoming information to what is already known . ''</sentence>
				<definiendum id="0">theme</definiendum>
				<definiens id="0">informs the listener as discourse unfolds how to relate the incoming information to what is already known</definiens>
			</definition>
			<definition id="3">
				<sentence>This rule suggests that the contrast class c be introduced first , for example : `` An electric field is a kind of force field that applies a force to a charged object . ''</sentence>
				<definiendum id="0">electric field</definiendum>
			</definition>
			<definition id="4">
				<sentence>is preferred over UAn electric field applies a force to a charged object and is a kind of force field . ''</sentence>
				<definiendum id="0">UAn electric field</definiendum>
				<definiens id="0">applies a force to a charged object</definiens>
			</definition>
</paper>

		<paper id="0324">
			<definition id="0">
				<sentence>Due to its strong focus on the pragmatics of a dialogue , leaving semantics aside , COlt is a domain independent conversation model .</sentence>
				<definiendum id="0">COlt</definiendum>
				<definiens id="0">a domain independent conversation model</definiens>
			</definition>
			<definition id="1">
				<sentence>In the field of information retrieval ( IR ) the interactive and communicative aspects of IR have only recently been emphasized ( cf. for example , Belkin and Vickery , 1985 ; Belkin et al. , 1993 ) .</sentence>
				<definiendum id="0">IR</definiendum>
			</definition>
			<definition id="2">
				<sentence>By adopting basic concepts of speech act theory and existing discourse models , and extending the CfA model for the situation of information-seeking human-computer interactions , the COR model shows the following features : • it depicts the interaction as a cooperative two-party `` negotiation '' where commitments ( to supply information or meta-information ) can be made , retracted or rejected ; • it permits mixed-initiative dialogues and is flexible enough to describe all possible even extremely complex interaction patterns ( this includes the temporary role changes of information seeker/information provider , which frequently occur in highly vague task settings such as information-seeking ) ; • it provides the means for an explicit representation of the dialogue history in an abstract form , i.e. , disregarding the interaction mode ( graphical , linguistic , mixed ) .</sentence>
				<definiendum id="0">commitments</definiendum>
				<definiendum id="1">meta-information</definiendum>
				<definiens id="0">an explicit representation of the dialogue history in an abstract form</definiens>
			</definition>
			<definition id="3">
				<sentence>In the figures 2 and 3 `` A : request '' and `` A : inform '' denote atomic acts , whereas a suffix notation indicates structured contributions , for example : ASSERT ( A , B ) or DIALOGUE ( B , A , solicit context information ) .</sentence>
				<definiendum id="0">B</definiendum>
				<definiendum id="1">DIALOGUE</definiendum>
				<definiens id="0">request '' and `` A : inform '' denote atomic acts , whereas a suffix notation indicates structured contributions , for example : ASSERT ( A ,</definiens>
			</definition>
			<definition id="4">
				<sentence>• A may start with an ASSERT ( entering the subnet type displayed in figure 2 ) to give context information concerning her request ; she may add the explicit REQUEST immediately afterwards , or may skip the explicit utterance ( jump ) , in case she believes that B is able to infer the intended request from the context .</sentence>
				<definiendum id="0">ASSERT</definiendum>
				<definiens id="0">information concerning her request ; she may add the explicit REQUEST immediately afterwards , or may skip the explicit utterance ( jump ) , in case she believes that B is able to infer the intended request from the context</definiens>
			</definition>
			<definition id="5">
				<sentence>RST is a theory which describes the structure of written monologues .</sentence>
				<definiendum id="0">RST</definiendum>
				<definiens id="0">a theory which describes the structure of written monologues</definiens>
			</definition>
			<definition id="6">
				<sentence>focuses on illocutions , dialogue acts and their sequential order , PaST provides means to describe the semantics of the links between the single acts ; ( 3 ) The description of dialogues in terms of COR-RST gives rise to a systematic approach to dynamically generate dialogue contributions for either of the participants .</sentence>
				<definiendum id="0">PaST</definiendum>
				<definiens id="0">provides means to describe the semantics of the links between the single acts</definiens>
			</definition>
			<definition id="7">
				<sentence>Corinna , as a prototypical implementation of the combined theories , uses PENMAN to generate natural language utterances .</sentence>
				<definiendum id="0">Corinna</definiendum>
				<definiens id="0">a prototypical implementation of the combined theories , uses PENMAN to generate natural language utterances</definiens>
			</definition>
			<definition id="8">
				<sentence>SPL : A sentence plan language for text generation .</sentence>
				<definiendum id="0">SPL</definiendum>
				<definiens id="0">A sentence plan language for text generation</definiens>
			</definition>
</paper>

		<paper id="0205">
			<definition id="0">
				<sentence>A computational system , LexPhon , has been devised to demonstrate the principles of LP .</sentence>
				<definiendum id="0">computational system</definiendum>
				<definiens id="0">devised to demonstrate the principles of LP</definiens>
			</definition>
			<definition id="1">
				<sentence>Lexical phonology ( Kiparsky , 1982 ; 1982b ; Halle and Mohanan , 1985 , Booij and Rubach , 1987 ; and many others ) is a recent approach to generative phonology ( Chomsky and Halle , 1968 ) in which the application of the phonological rules , which map from the abstract , underlying form of a word to its surface , phonetic realisation , is tied to the morphological word formation ~ processes taking place .</sentence>
				<definiendum id="0">Lexical phonology</definiendum>
				<definiens id="0">a recent approach to generative phonology ( Chomsky and Halle , 1968 ) in which the application of the phonological rules</definiens>
			</definition>
			<definition id="2">
				<sentence>LexPhon is a collection of interactive linguistic tools comprising a phonological processor , or interpreter , sets of processes for creating and updating ph onem e inventories , phonological rule-sets and LP control structures and investigative tools for exploring relationships within the phoneme inventory .</sentence>
				<definiendum id="0">LexPhon</definiendum>
				<definiens id="0">a collection of interactive linguistic tools comprising a phonological processor , or interpreter , sets of processes for creating</definiens>
			</definition>
			<definition id="3">
				<sentence>e.g. Unresolved atoms in t h e final working string \ [ wordboundary , compbounda ry , s , p , o , o , n , s , compboundary , f , u , l , compboundary , wordbou ndary\ ] The phonological rules are applied , in numerical order , in blocks defined by the stratum definition predicates .</sentence>
				<definiendum id="0">e.g. Unresolved</definiendum>
				<definiens id="0">atoms in t h e final working string \ [ wordboundary , compbounda ry , s , p , o , o , n , s , compboundary , f , u , l</definiens>
			</definition>
			<definition id="4">
				<sentence>The loopback mechanism returns the current working string unchanged if no affix is found but invokes the phonological rule predicate followed by the normal 'cyclerule ' predicate if affixation has been successful .</sentence>
				<definiendum id="0">loopback mechanism</definiendum>
				<definiens id="0">returns the current working string unchanged if no affix</definiens>
			</definition>
			<definition id="5">
				<sentence>Harris provides a series of possible definitions for the nasal assimilation rule .</sentence>
				<definiendum id="0">Harris</definiendum>
				<definiens id="0">provides a series of possible definitions for the nasal assimilation rule</definiens>
			</definition>
			<definition id="6">
				<sentence>o , a , n , g , a , wb ganga ajenjo nieto WbpilpN , t , fl , N~O , Wb wb , n , ldC , t , o , wb un hielo wb , un3 , x , c , i , o , wb nuevo wb , n , u , c , v , o , wb un huevo wb , un3 , u , c.v.o , wb UI'I OSO wb , un3 , o , s , o , wb wb , kon l , p , a , d , r , c , wb mr , e con padre wb , kon3 , p , a , d , r , c , wb until to discolour the lakes of the lakes at ranch huge haunch bargain absinth grandson a frost new an e~8 abear aquaintanee with father Optional boundaries are dealt with within the theory of LP by assuming that no boundary is specified in the rule and the rule is either attached to several strata ( Halle and Mohanan , 1985 ) or to the first stratum following the resolution of all boundaries across which it may apply ( Booij and Rubach , 1987 ) .</sentence>
				<definiendum id="0">un3</definiendum>
				<definiens id="0">n , u , c , v , o , wb un huevo wb ,</definiens>
			</definition>
			<definition id="7">
				<sentence>The Phonological Rules and Phoneme Inventory used here are the default databases provided for LexPhon , mainly derived from Halle and Mohanan 's ( 1985 ) description of LP .</sentence>
				<definiendum id="0">Phonological Rules</definiendum>
				<definiendum id="1">Phoneme Inventory</definiendum>
				<definiens id="0">the default databases provided for LexPhon , mainly derived from Halle and Mohanan 's ( 1985 ) description of LP</definiens>
			</definition>
</paper>

		<paper id="0328">
			<definition id="0">
				<sentence>The tenor of discourse involves the selection of a number of options in the subsystems that configure the participants ' speech roles .</sentence>
				<definiendum id="0">tenor of discourse</definiendum>
				<definiens id="0">involves the selection of a number of options in the subsystems that configure the participants ' speech roles</definiens>
			</definition>
			<definition id="1">
				<sentence>The LANGUAGE ROLE is a continuum with the two ends of the scale being whether the language is constitutive or ancillary ( the language in a face-to-face service encounter being ancillary since it accompanies an activity and is not the sole meaningful activity , and the language of a physics research paper being constitutive since the text creates the entire exchange ) .</sentence>
				<definiendum id="0">LANGUAGE ROLE</definiendum>
				<definiens id="0">a continuum with the two ends of the scale being whether the language is constitutive or ancillary ( the language in a face-to-face</definiens>
			</definition>
			<definition id="2">
				<sentence>The CHANNEL OF DISCOURSE is the modality through which the language is received , including typically the options GRAPHIC and PHONIC .</sentence>
				<definiendum id="0">CHANNEL OF DISCOURSE</definiendum>
				<definiens id="0">the modality through which the language is received , including typically the options GRAPHIC and PHONIC</definiens>
			</definition>
</paper>

		<paper id="0207">
			<definition id="0">
				<sentence>A phase-space reconstruction of its behavior consists of the set of points ( x , y ) in the Plane , where the x-coordinate represents the displacement -- -positive or negative -- -of the pendulum at any given point in time , and the y-coordinate represents its velocity .</sentence>
				<definiendum id="0">phase-space reconstruction of its behavior</definiendum>
			</definition>
</paper>

		<paper id="0104">
			<definition id="0">
				<sentence>As statistical methods ask for a good rcl ) rcsentatiou in number of the samples , we decided to extract in a first round only terms of length 2 that we will call base-term which matched a list of previously determined patterns : N ADJ station terrienne ( Earth station ) NI de ( D~T ) N2 zone de eouverture ( coverage zone ) Nt h ( DET ) N2 rdflecteur d grille ( grid reflector ) Nt PREP N~ liaison par satellite ( satellite link ) Ni N2 diode tunnel ( tunnel diode ) Of course , terms exist whose length is greater than 2 .</sentence>
				<definiendum id="0">ADJ station terrienne</definiendum>
				<definiens id="0">N2 zone de eouverture ( coverage zone ) Nt h ( DET ) N2 rdflecteur d grille ( grid reflector ) Nt PREP N~ liaison par satellite ( satellite link ) Ni N2 diode tunnel ( tunnel diode ) Of course , terms exist whose length is greater than 2</definiens>
			</definition>
			<definition id="1">
				<sentence>Insertion of moditicrs inside a base-term structure does not raise problem , expect when this modifier is an adjective inserted inside a N1 PREP N2 structure .</sentence>
				<definiendum id="0">Insertion of moditicrs</definiendum>
				<definiens id="0">an adjective inserted inside a N1 PREP N2 structure</definiens>
			</definition>
			<definition id="2">
				<sentence>A contingency table is defined for each pair ( Li , Lj ) : I II Lj I Lj , with j ' # j Li , with i ' ¢ i c d where : a stands for the frequency of pairs involving both Li and Lj , b stands for the frequency of pairs involving Li and Lj , , c stands for tile frequency of pairs involving Li , and Lj , d stands for the frequency of pairs involving Li , and The statistical literature proposes many scores which can be used to test the strength of the bond between the two variables of a contingency table .</sentence>
				<definiendum id="0">contingency table</definiendum>
				<definiens id="0">the frequency of pairs involving both Li and Lj , b stands for the frequency of pairs involving Li and Lj , , c stands for tile frequency of pairs involving Li</definiens>
				<definiens id="1">scores which can be used to test the strength of the bond between the two variables of a contingency table</definiens>
			</definition>
			<definition id="3">
				<sentence>Some are well-known such as the association ratio , close to the concept of mutual information , introduced by \ [ Church and Hanks , 1990\ ] : a IM = log~ ( a+b ) Ca+c ) ( 1 ) the O 2 coefficient introduced by \ [ Gale and Church , 1991\ ] : 02 = ( adbe ) 2 ( a + b ) ( a + c ) ( b + c ) ( b + d ) ( 2 ) or the Loglike coefficient introduced by \ [ Dunning , 1993\ ] : Loglike = a loga + blogb + clogc + dlogd - ( a + b ) log ( a + b ) ( a + c ) logCa -Ic ) - ( b + d ) logCb + d ) ( c + d ) logCc + d ) + ( a+b+c+d ) logCa+b+c+d ) ( 3 ) A property of these scores is that their values increase with the strength of the bond of the lemmas .</sentence>
				<definiendum id="0">+ c ) logCa -Ic</definiendum>
				<definiendum id="1">a+b+c+d ) logCa+b+c+d</definiendum>
			</definition>
			<definition id="4">
				<sentence>We give in figure 2 the topmost 11 french pairs sorted by the Loglike coefficient ( Logl ) ( Nbc is the number of the pair occurrences and IM the value of 33 association ratio ) .</sentence>
				<definiendum id="0">Nbc</definiendum>
				<definiens id="0">the number of the pair occurrences</definiens>
			</definition>
			<definition id="5">
				<sentence>( a ) N1 N2 : Dist = 2 MDist = 2 • liaison sdmaphore , liaisons sdmaphores ( common signalling link ( s ) ) • canal support , canaux support , canaux supports ( bearer channe 0 34 ( b ) N\ ] pReP N2 : Dist = 3 MDist = 2 * accusg ( s ) de rgception ( acknowledgement of receipt ) • refroidissement d air , re/roidissement par air ( cooling by air ) ( c ) N1 PREP DF .</sentence>
				<definiendum id="0">liaisons sdmaphores</definiendum>
			</definition>
</paper>

		<paper id="0208">
			<definition id="0">
				<sentence>'l'll~ ( ~ouipnteF siinulations reported here arc it Ih'st attcnilit to iiica. , lllre I.he IlSefllhless of Iistriliutiollil , I am l iihouol , acl , ic ilirornlation in seglil~qil , ilig liholiCllll , si , qllCli ( ' ( , s. The algorithlllS hy-in~l , licsiz~ ' ~lill'~'r ( 'nl .</sentence>
				<definiendum id="0">'l'll~</definiendum>
				<definiens id="0">lllre I.he IlSefllhless of Iistriliutiollil , I am l iihouol , acl , ic ilirornlation in seglil~qil , ilig liholiCllll , si</definiens>
			</definition>
			<definition id="1">
				<sentence>Word stress in English fairly accurately predicts the location of word beginnings ( Cutler &amp; Norris , 1988 ; Cutler &amp; Butterfield , 1992 ) ; Jusczyk , Cutler and II , edanz ( 1993 ) demonstrated that 9-monthohls ( but not 6-month-olds ) are sensitive to the common strong/weak word stress pattern in English .</sentence>
				<definiendum id="0">Word stress</definiendum>
				<definiens id="0">sensitive to the common strong/weak word stress pattern in English</definiens>
			</definition>
			<definition id="2">
				<sentence>The total length of all the words in the lexicon is tile sum of this formula over all lexical items : t/ , T1 lien ( p ) , len ( wi ) ) = len ( p ) Z lc , , ( w , ) ( 2 ) i=1 i=1 As stated al ) ovc , the length liehls used to divide the phoneme string are lixe ( Mcugth , lu e ; u'h field is an integer I ) etween one an ( I the munl ) er of phonemes in the longest word .</sentence>
				<definiendum id="0">total length of all the words</definiendum>
				<definiendum id="1">u'h field</definiendum>
				<definiens id="0">tile sum of this formula over all lexical items : t/ , T1 lien ( p ) , len ( wi ) ) = len ( p ) Z lc</definiens>
			</definition>
			<definition id="3">
				<sentence>Given a lexicon , tim saml ) le can I ) e encoded by ret ) lacing words with their respective indices into the lexicon : Encoded Sample l : 1 , 6 , 5 , 2 , 3 ; 5 , 2 , 3 ; l , 6 , 4 , 2 , 3 ; Encoded Saml ) le 2 : 2 , 11_2 , 6 , 4 , 5 ; 11 , 3 , 8 ; 1 , 9 , 10 , 7 , 8 ; Our simulation attempts to find the hypothesis that minimizes the combined sizes of the lexicon and encoded sample .</sentence>
				<definiendum id="0">Our simulation</definiendum>
				<definiens id="0">attempts to find the hypothesis that minimizes the combined sizes of the lexicon and encoded sample</definiens>
			</definition>
			<definition id="4">
				<sentence>The total length of the representation of the entire hyl ) othesis is the sam of the rel ) resentation lengt , hs of the word inw , ntory ( 'f ) llnlnl , I , he code word illV ( qltory ~'ohllllll ; Hid I , ho na .</sentence>
				<definiendum id="0">othesis</definiendum>
				<definiens id="0">total length of the representation of the entire hyl )</definiens>
			</definition>
			<definition id="5">
				<sentence>For example ( correSl ) On ( ling to the utterance `` green and '' ) : 4hi I , lu , n , h , ~ical terms , the syllal ) h. ( , ns ( .</sentence>
				<definiendum id="0">correSl</definiendum>
				<definiens id="0">n , h , ~ical terms</definiens>
			</definition>
</paper>

		<paper id="0333">
			<definition id="0">
				<sentence>This paper describes the architecture and methodology of the ZARDOZ multilingual sign translation system , which is designed to translate spoken language ( specifically English text ) into a number of different sign-languages , in particular ISL ( Irish ) , ASL ( American ) and JSL ( Japanese ) .</sentence>
				<definiendum id="0">ASL</definiendum>
				<definiens id="0">the architecture and methodology of the ZARDOZ multilingual sign translation system , which is designed to translate spoken language ( specifically English text</definiens>
			</definition>
			<definition id="1">
				<sentence>The refined interlingua provides grist for the ( vi ) discourse tracking agency , which does anaphoric resolution , before being passed to the generation panels of the system : ( vii ) the sign syntax agency , which employs a robust scheme of spatial dependency graphs ( see Section 5 ) , and ( viii ) the sign 249 7th International Generation Workshop * Kennebunkport , Maine • June 21-24 , 1994 Schematizafion Discourse Tr .</sentence>
				<definiendum id="0">discourse tracking agency</definiendum>
				<definiens id="0">does anaphoric resolution , before being passed to the generation panels of the system : ( vii ) the sign syntax agency</definiens>
			</definition>
			<definition id="2">
				<sentence>To decouple the input and output languages , ZARDOZ adopts an lnterlingua approach ( e.g. Mitamura et al 1991 ) , which places a languageindependent interface between source and target .</sentence>
				<definiendum id="0">ZARDOZ</definiendum>
				<definiens id="0">places a languageindependent interface between source and target</definiens>
			</definition>
			<definition id="3">
				<sentence>An SDgraph is a partial ordering of case types from the syntactic/semantic case ontology , which indicates which elements are to be selected from an interlingua structure , and their relative order in the output stream .</sentence>
				<definiendum id="0">SDgraph</definiendum>
				<definiens id="0">a partial ordering of case types from the syntactic/semantic case ontology , which indicates which elements are to be selected from an interlingua structure , and their relative order in the output stream</definiens>
			</definition>
			<definition id="4">
				<sentence>An SD-graph represents a syntactic context , or general state of affairs , rather than a rule of grammar ; in effect , an SD-graph is a collection of weak rules ( or preferences ) folded together .</sentence>
				<definiendum id="0">SD-graph</definiendum>
				<definiendum id="1">SD-graph</definiendum>
				<definiens id="0">a syntactic context , or general state of affairs</definiens>
			</definition>
			<definition id="5">
				<sentence>Key : left to right arrows irmlicate Before ; right to left arrows indicate After ; vertical arrows indicate Same Position As ; Grey arrows indicate Closer Proximity ; Grey nodes indicate Sign Literals as opposed to constituent types , while black nodes represent the J'~ed points of the graph ) An SD-graph is a collection of constraints for ordering the elements of an interlingua frame structure .</sentence>
				<definiendum id="0">Key</definiendum>
				<definiens id="0">left to right arrows irmlicate Before ; right to left arrows indicate After ; vertical arrows indicate Same Position As ; Grey arrows indicate Closer Proximity ; Grey nodes indicate Sign Literals as opposed to constituent types , while black nodes represent the J'~ed points of the graph ) An SD-graph is a collection of constraints for ordering the elements of an interlingua frame structure</definiens>
			</definition>
			<definition id="6">
				<sentence>ZARDOZ employs the basic Hobbs algorithm for this task ( see Hobbs 1978 ) , augmented with discourse registers which track the movement of referents between peripheral and central focus .</sentence>
				<definiendum id="0">ZARDOZ</definiendum>
				<definiens id="0">employs the basic Hobbs algorithm for this task ( see Hobbs 1978 ) , augmented with discourse registers which track the movement of referents between peripheral and central focus</definiens>
			</definition>
			<definition id="7">
				<sentence>For example , if B1LL is signed on the left and MARY on the fight , then a left to right motion while signing the verb ASL-CHASE , indicates BILL is the pursuer and MARY the pursued .</sentence>
				<definiendum id="0">BILL</definiendum>
				<definiens id="0">the pursuer</definiens>
			</definition>
</paper>

		<paper id="0327">
			<definition id="0">
				<sentence>The flat QLF notation means that the planner need not 'know ' about the syntactic form of feature structures as defined by a particular grammar , but simply decide which grammatical constraints hold of each logical element 's realisation .</sentence>
				<definiendum id="0">flat QLF notation</definiendum>
			</definition>
			<definition id="1">
				<sentence>generate ( Syntax : -semantics , Text ) : ed~e ( S~ntax : Text : \ [ \ ] : _ : \ [ \ ] ) .</sentence>
				<definiendum id="0">generate ( Syntax</definiendum>
				<definiendum id="1">Text )</definiendum>
				<definiens id="0">ed~e ( S~ntax : Text : \ [ \ ] : _ : \ [ \ ] )</definiens>
			</definition>
			<definition id="2">
				<sentence>The lookup procedure retrieves a word whose semantics is a subset of that in Semantics , returning the wordand ts syntactic and semantic description , acceptable/3 ensures that the semantics of the word is a subset of the target semantics , and also returns the `` unused '' part of the semantics in Compl .</sentence>
				<definiendum id="0">lookup procedure</definiendum>
				<definiens id="0">retrieves a word whose semantics is a subset of that in Semantics , returning the wordand ts syntactic and semantic description , acceptable/3 ensures that the semantics of the word is a subset of the target semantics , and also returns the `` unused '' part of the semantics in Compl</definiens>
			</definition>
			<definition id="3">
				<sentence>Add edge is a recursiveprocedure that does the main work .</sentence>
				<definiendum id="0">Add edge</definiendum>
				<definiens id="0">a recursiveprocedure that does the main work</definiens>
			</definition>
			<definition id="4">
				<sentence>Here we argue : 1 ) QLF is suitable for specifying the content of the target to be generated incrementally , 2 ) a chart-based generation algorithm is suitable for incremental generation , and 3 ) CG rules used can determine the level of 'talkativeness ' of an incremental generation system .</sentence>
				<definiendum id="0">QLF</definiendum>
				<definiens id="0">suitable for specifying the content of the target to be generated incrementally</definiens>
			</definition>
			<definition id="5">
				<sentence>Now , while a CG with only forward and backward application ( FA and BA ) , implies a standard notion of constituency , rules like type raising ( TR ) and functional composition ( FC ) give rise to a more generous notion of constituency ( this is what makes 'non227 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 constituent co-ordination ' possible ) .</sentence>
				<definiendum id="0">functional composition ( FC</definiendum>
				<definiens id="0">implies a standard notion of constituency , rules like type raising</definiens>
			</definition>
</paper>

		<paper id="0114">
			<definition id="0">
				<sentence>The effort for lexiconbuilding , although it is not yet complete , was relatively modest ( 0.25 of a manyear ) thanks to Wordnet , which suggests good portability .</sentence>
				<definiendum id="0">Wordnet</definiendum>
				<definiens id="0">suggests good portability</definiens>
			</definition>
			<definition id="1">
				<sentence>To simplify matters , we restricted the grammar to binary parse rules ( context-free rules with one or two symbols for the replacement ) .</sentence>
				<definiendum id="0">rules</definiendum>
				<definiens id="0">context-free rules with one or two symbols for the replacement )</definiens>
			</definition>
			<definition id="2">
				<sentence>The standard deviation when n is the size of the subpopulation , N is the size of the population , and A the count for the population , is qA ( N-A ) ( N-n ) /nN2 ( N-1 ) ( Cochran , 1977 ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the size of the subpopulation</definiens>
				<definiens id="1">the size of the population</definiens>
			</definition>
			<definition id="3">
				<sentence>Third , even if multiple interpretations can not be ruled out for a word , an information retrieval system can just try each , and take the union of the results ( i.e. as a logical disjunction ) ; generally only one interpretation will every match a query .</sentence>
				<definiendum id="0">information retrieval</definiendum>
				<definiens id="0">a logical disjunction</definiens>
			</definition>
</paper>

		<paper id="0312">
			<definition id="0">
				<sentence>SAGE is composed of a knowledge representation language and simulator , which form the underlying model and constitute the `` speaker '' ; a graphics component , which displays the actions of the simulator and provides an anchor for locative and deictic relations ; and the generator SPOKESMAN , which produces a textual narration of events .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiendum id="1">simulator</definiendum>
				<definiendum id="2">graphics component</definiendum>
				<definiendum id="3">generator SPOKESMAN</definiendum>
				<definiens id="0">displays the actions of the simulator and provides an anchor for locative and deictic relations</definiens>
				<definiens id="1">produces a textual narration of events</definiens>
			</definition>
			<definition id="1">
				<sentence>SPOKESMAN is data directed in that it links to the other components both through mappings from concepts in the knowledge representation and through instances of objects and events created by the simulator .</sentence>
				<definiendum id="0">SPOKESMAN</definiendum>
				<definiens id="0">data directed in that it links to the other components both through mappings from concepts in the knowledge representation and through instances of objects and events created by the simulator</definiens>
			</definition>
			<definition id="2">
				<sentence>SAGE allows us to approach these questions experimentally , using SAGE to provide a context in which to make the decision about where the information is best represented and the decisions best made .</sentence>
				<definiendum id="0">SAGE</definiendum>
			</definition>
			<definition id="3">
				<sentence>The underlying program of SAGE , that is , the part in which objects and events are modelled , is a knowledge based simulation system with two parts : the knowledge representation language and the simulator .</sentence>
				<definiendum id="0">SAGE</definiendum>
				<definiens id="0">a knowledge based simulation system with two parts : the knowledge representation language and the simulator</definiens>
			</definition>
			<definition id="4">
				<sentence>VSFL is a reimplementation of SFL , which is a descendent of KL-One .</sentence>
				<definiendum id="0">VSFL</definiendum>
				<definiens id="0">a reimplementation of SFL , which is a descendent of KL-One</definiens>
			</definition>
			<definition id="5">
				<sentence>SCORE is a reimplementation of the SPROKET simulator .</sentence>
				<definiendum id="0">SCORE</definiendum>
			</definition>
			<definition id="6">
				<sentence>SCORE provides a language for declaratively representing the plans of agents , where a plan is a partial ordering of procedures and subgoals for accomplishing goals and handling contingencies .</sentence>
				<definiendum id="0">SCORE</definiendum>
				<definiens id="0">provides a language for declaratively representing the plans of agents</definiens>
			</definition>
			<definition id="7">
				<sentence>Application Program Objects ~ Composin~ the utterance Text Structure TEXT PLANNER ~ Mapping to linguistic resources Linguistic Sp~cation Choosing phrases and attaching them MUMBLE-86 Surface Structure Mo~hology Word Stream Figure 4 : SPOKESMAN Each representational level is a complete description of the utterance and provides constraints and context for the further decisions that go into its construction .</sentence>
				<definiendum id="0">Application Program Objects</definiendum>
				<definiendum id="1">representational level</definiendum>
				<definiens id="0">the utterance Text Structure TEXT PLANNER ~ Mapping to linguistic resources Linguistic Sp~cation Choosing phrases and attaching them MUMBLE-86 Surface Structure Mo~hology Word Stream</definiens>
			</definition>
			<definition id="8">
				<sentence>The Text Structure , which is the central representation level of the text planner , provides a vocabulary for mapping from the terms of the model level to the linguistic terms of the generator .</sentence>
				<definiendum id="0">Text Structure</definiendum>
				<definiens id="0">the central representation level of the text planner , provides a vocabulary for mapping from the terms of the model level to the linguistic terms of the generator</definiens>
			</definition>
			<definition id="9">
				<sentence>Another role of the Text Structure is to keep track of discourse level information , such as focus and what entities have been referenced and in what context .</sentence>
				<definiendum id="0">Text Structure</definiendum>
				<definiens id="0">to keep track of discourse level information</definiens>
			</definition>
			<definition id="10">
				<sentence>( mapping-tables ( spr : : concept 'spr : : catch ) class-to-text-structure ( : realization-class transition-event-class : arguments ( : agent ( spr : : who ( core-event-object self ) ) : event ( core-event-object self ) : theme ( spr : : what ( core-event-object self ) : time ( determine-tense self ) ) ) object-to-tree-family ( : argument-structure-class transitive-event : arguments ( `` ( mumble : : verb `` catch '' ) ) ) ) ( mapping-tables ( spr : : concept 'spr : : look-for ) class-to-text-structure ( : realization-class process-event-class : arguments ( : agent ( spr : :who ( core-event-object self ) ) : event ( core-event-object self ) : theme ( spr : :what ( core-event-object self ) ) : time ( determine-tense self ) ) ) obj ect-to-treefamily ( : argument-structure-class transitive-prepcomp : arguments ( `` ( mumble : : verb `` look '' ) • ( mumble : : prep `` for '' ) ) ) ) Figure 11 : Mapping tables CATCH The choices described above result in the Text Structure representation , as shown in Figure 12 : We have seen that what are generally treated as a single phenomenon stretch across multiple levels in SAGE : • Event time and speech time are facts of the underlying program , whereas reference time is part of the discourse model in the generator .</sentence>
				<definiendum id="0">realization-class transition-event-class</definiendum>
				<definiens id="0">:who ( core-event-object self ) ) : event ( core-event-object self ) : theme ( spr : :what ( core-event-object self ) ) : time ( determine-tense self</definiens>
				<definiens id="1">a single phenomenon stretch across multiple levels in SAGE : • Event time and speech time are facts of the underlying program , whereas reference time is part of the discourse model in the generator</definiens>
			</definition>
</paper>

		<paper id="0325">
			<definition id="0">
				<sentence>The language generator plays a key role in a two-medium conversational system for a naturalistic foreign language learning environment .</sentence>
				<definiendum id="0">language generator</definiendum>
				<definiens id="0">plays a key role in a two-medium conversational system for a naturalistic foreign language learning environment</definiens>
			</definition>
			<definition id="1">
				<sentence>A view is an abstraction of what to say and how to say it , expressed as a structure .</sentence>
				<definiendum id="0">view</definiendum>
				<definiens id="0">an abstraction of what to say and how to say it , expressed as a structure</definiens>
			</definition>
			<definition id="2">
				<sentence>The instantiatedi view is a language-independent intermediate representation which ultimately yields an output sentence .</sentence>
				<definiendum id="0">instantiatedi view</definiendum>
				<definiens id="0">a language-independent intermediate representation which ultimately yields an output sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>Tourguide is an interaction type with three moves : an action by the tutor , a description of that action , and acknowledgement by the student .</sentence>
				<definiendum id="0">Tourguide</definiendum>
				<definiens id="0">an interaction type with three moves : an action by the tutor , a description of that action , and acknowledgement by the student</definiens>
			</definition>
			<definition id="4">
				<sentence>What-Action takes four possible values : Student-Did or Tutor-Did for the most recent action executed by the student or tutor ; and Tutor-Did or 219 7th International Generation Workshop * Kennebunkport , Maine • June 21-24 , 1994 Tutor-Thought for an action constructed by the tutor as the basis of something already said or about to be said .</sentence>
				<definiendum id="0">What-Action</definiendum>
				<definiens id="0">takes four possible values : Student-Did or Tutor-Did for the most recent action executed by the student or tutor</definiens>
			</definition>
			<definition id="5">
				<sentence>Each IV slot can be filled by ( i ) an IV-O , ( ii ) a microworld object , ( iii ) a class , which is a language-independent meaning corresponding to a common noun , ( iv ) a list of items of the three foregoing kinds , or ( v ) another IV .</sentence>
				<definiendum id="0">IV slot</definiendum>
				<definiens id="0">a language-independent meaning corresponding to a common noun</definiens>
			</definition>
</paper>

		<paper id="0305">
			<definition id="0">
				<sentence>Schema-based Natural Language Generation ( NLG ) systems , e.g. , \ [ Weiner , 1980 ; McKeown , 1985 ; Paris , 1988\ ] , determine the information to be presented based on common patterns of discourse .</sentence>
				<definiendum id="0">Schema-based Natural Language Generation</definiendum>
				<definiens id="0">determine the information to be presented based on common patterns of discourse</definiens>
			</definition>
			<definition id="1">
				<sentence>The discourse planning mechanism , which is the focus of this paper , generates a set of Rhetorical Devices ( RDs ) , where each RD is a rhetorical action , such as Assert , Negate or Instantiate , applied to a proposition .</sentence>
				<definiendum id="0">discourse planning mechanism</definiendum>
				<definiendum id="1">RD</definiendum>
				<definiens id="0">a rhetorical action</definiens>
			</definition>
			<definition id="2">
				<sentence>The process of generating candidate sets of RDs considers the following factors : ( 1 ) the effect of inferences from an RD on a user 's beliefs ; ( 2 ) the amount of prerequisite information required by the user to understand the concepts in an RD ; and ( 3 ) the amount of information to be included in referring expressions which identify the concepts in an RD. anism extracts discourse rdations and constraints from the set of RDs generated in Step 2 .</sentence>
				<definiendum id="0">RDs</definiendum>
				<definiens id="0">the effect of inferences from an RD on a user 's beliefs ; ( 2 ) the amount of prerequisite information required by the user to understand the concepts in an RD</definiens>
			</definition>
			<definition id="3">
				<sentence>Algorithm Expand-sets-of-RDs ( n ) sition in node n. ( Not all the RDs generated in this step axe capable of conveying an intended proposition by themselves , but they may be able to do so in combination with other RDs . )</sentence>
				<definiendum id="0">Algorithm Expand-sets-of-RDs</definiendum>
				<definiens id="0">Not all the RDs generated in this step axe capable of conveying an intended proposition by themselves , but they may be able to do so in combination with other RDs</definiens>
			</definition>
			<definition id="4">
				<sentence>An RD-Graph is a directed graph that contains the following components : ( 1 ) the set of propositions to be conveyed ( pl , .</sentence>
				<definiendum id="0">RD-Graph</definiendum>
				<definiens id="0">a directed graph that contains the following components : ( 1 ) the set of propositions to be conveyed ( pl ,</definiens>
			</definition>
			<definition id="5">
				<sentence>Sample RDs ( labelled ) Proposition RDs and labels Pl : \ [ Wallaby hop\ ] A ( pI ) Jail\ ] I ( pl ) \ [ none\ ] A ( p2 ) \ [ -\ ] P2 : \ [ Kangaroo hop\ ] A ( p2 ) Jail\ ] A ( pl ) \ [ I ( -p3 ) \ ] x ( p~ ) \ [ -\ ] `` P3 : \ [ Wombat -~hop\ ] i \ ] ~\ ] ' ( P3 ) \ [ A ( pl ) +I ( p1 ) , A ( p2 ) \ ] I I ( '~p3 ) \ [ I ( pl ) , A ( pI ) \ ] ( c ) Attach to each combination of RDs the list of negative RDs it can overcome .</sentence>
				<definiendum id="0">Sample RDs</definiendum>
				<definiens id="0">( c ) Attach to each combination of RDs the list of negative RDs it can overcome</definiens>
			</definition>
			<definition id="6">
				<sentence>After the relevant aspects and required level of expertise of each concept have been determined , WISHFUL-II applies the content selection step described in Section 2 to determine the prerequisite propositions of each concept .</sentence>
				<definiendum id="0">WISHFUL-II</definiendum>
				<definiens id="0">applies the content selection step described in Section 2 to determine the prerequisite propositions of each concept</definiens>
			</definition>
			<definition id="7">
				<sentence>The total weight of a node is the sum of the weights of the nodes in the path from the root of the search tree to this node .</sentence>
				<definiendum id="0">total weight of a node</definiendum>
				<definiens id="0">the sum of the weights of the nodes in the path from the root of the search tree to this node</definiens>
			</definition>
</paper>

		<paper id="0302">
			<definition id="0">
				<sentence>We show how this algorithm , called DPOCL ( Decompositional POCL ) , provides a formal and explicit model of intentional and informational structure in its plans .</sentence>
				<definiendum id="0">DPOCL ( Decompositional POCL</definiendum>
			</definition>
			<definition id="1">
				<sentence>A causal link ( shown using a solid arc and labeled with the effect that it contributes ) connects Cause-to-Believe ( falrest ( L , B ) ) to the End-Subplan step .</sentence>
				<definiendum id="0">causal link</definiendum>
				<definiens id="0">shown using a solid arc and labeled with the effect that it contributes ) connects Cause-to-Believe ( falrest ( L , B ) ) to the End-Subplan step</definiens>
			</definition>
			<definition id="2">
				<sentence>This phenomenon is represented by the CombineBelief ( z'~ action , where ~ is a vector of relevant beliefs .</sentence>
				<definiendum id="0">~</definiendum>
				<definiens id="0">a vector of relevant beliefs</definiens>
			</definition>
			<definition id="3">
				<sentence>An Explicit Representation of Informational Structure Decomposition operators in DPOCL enable us to represent the knowledge speakers have about how to use domain information to achieve communicative intentions .</sentence>
				<definiendum id="0">Explicit Representation of Informational Structure Decomposition</definiendum>
				<definiens id="0">operators in DPOCL enable us to represent the knowledge speakers have about how to use domain information to achieve communicative intentions</definiens>
			</definition>
			<definition id="4">
				<sentence>With respect to the class of plans that DPOCL can generate , DPOCL is primitive complete .</sentence>
				<definiendum id="0">DPOCL</definiendum>
				<definiens id="0">primitive complete</definiens>
			</definition>
</paper>

		<paper id="0326">
			<definition id="0">
				<sentence>The categorization into what constitutes an entity and what constitutes a predicate is problematical , since there are several sources of inaccuracies in this approach : for instance , complications due to vague expressions and due to qlmntification relations , and conceptualizations made by the addressee , which may differ significantly from those embodied in the system 's conception consisting of entities and relations .</sentence>
				<definiendum id="0">addressee</definiendum>
			</definition>
</paper>

		<paper id="0109">
			<definition id="0">
				<sentence>In the middle are `` structural '' approaches , where patterns are defined as relations among a set of primitives which may or may not be associated with probabilities .</sentence>
				<definiendum id="0">patterns</definiendum>
				<definiens id="0">relations among a set of primitives which may or may not be associated with probabilities</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , a private plane identifier consists of the name of a plane type , some digits , and one or two letter words ( e.g. `` Sessna six one two one kilo '' ) .</sentence>
				<definiendum id="0">private plane identifier</definiendum>
				<definiens id="0">consists of the name of a plane type , some digits , and one or two letter words ( e.g. `` Sessna six one two one kilo '' )</definiens>
			</definition>
			<definition id="2">
				<sentence>Sparser is a bottom-up chart parser which uses a semantic phrase structure grammar ( i.e. the nonterminals are semantic categories , such as HEADING or FLIGHT-ID , rather than traditional syntactic categories , such as CLAUSE or NOUN-PHRASE ) .</sentence>
				<definiendum id="0">Sparser</definiendum>
				<definiens id="0">a bottom-up chart parser which uses a semantic phrase structure grammar ( i.e. the nonterminals are semantic categories , such as HEADING or FLIGHT-ID , rather than traditional syntactic categories , such as CLAUSE or NOUN-PHRASE )</definiens>
			</definition>
</paper>

		<paper id="0314">
			<definition id="0">
				<sentence>However , in ( 4 ) b , Ahmet is the topic or a link to the previous context whereas the subject , Fatma , is the focus , and thus the OSV word order is used .</sentence>
				<definiendum id="0">Ahmet</definiendum>
				<definiens id="0">the topic or a link to the previous context whereas the subject</definiens>
			</definition>
			<definition id="1">
				<sentence>However , by using the CCG type-raising and composition rules , CCG formalisms can produce nontraditional syntactic constituents which may match the intonational phrasing .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiens id="0">by using the CCG type-raising and composition rules</definiens>
			</definition>
			<definition id="2">
				<sentence>The notation ... is a variable which can unify with one or more elements of a set .</sentence>
				<definiendum id="0">notation ...</definiendum>
				<definiens id="0">a variable which can unify with one or more elements of a set</definiens>
			</definition>
</paper>

		<paper id="0110">
			<definition id="0">
				<sentence>Spoken language is a social mechanism evolved for communication among entities whose biological properties constrain the possibilities .</sentence>
				<definiendum id="0">Spoken language</definiendum>
			</definition>
			<definition id="1">
				<sentence>The Markov model ( minus the hidden part ) estimates the likelihoods of words given the previous word ( or N words ) , based on a training corpus of sentence transcriptions .</sentence>
				<definiendum id="0">Markov model</definiendum>
				<definiens id="0">the hidden part ) estimates the likelihoods of words given the previous word ( or N words ) , based on a training corpus of sentence transcriptions</definiens>
			</definition>
			<definition id="2">
				<sentence>When speech is the input , however , speaker disfluencies , novel syntactic constructions and recognition errors pose serious difficulties for traditional parsers .</sentence>
				<definiendum id="0">speech</definiendum>
				<definiens id="0">speaker disfluencies , novel syntactic constructions and recognition errors pose serious difficulties for traditional parsers</definiens>
			</definition>
			<definition id="3">
				<sentence>Interpretation is the stage at which a representation of meaning is constructed .</sentence>
				<definiendum id="0">Interpretation</definiendum>
				<definiens id="0">the stage at which a representation of meaning is constructed</definiens>
			</definition>
</paper>

		<paper id="0101">
			<definition id="0">
				<sentence>A speech translation system , which by necessity combines speech and language technology , is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation .</sentence>
				<definiendum id="0">speech translation system</definiendum>
				<definiens id="0">a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation</definiens>
			</definition>
			<definition id="1">
				<sentence>relate 'category ' predicates co , ct , c2 holding of a string and two spanning substrings ( we limit the rules here to two daughters for simplicity ) : c0 ( s0 ) A daughters ( so , sl , s2 ) el ( st ) A cz ( s2 ) A ( so = concat ( st , s2 ) ) ( Here , and subsequently , variables like so and st are implicitly universally quantified . )</sentence>
				<definiendum id="0">daughters</definiendum>
				<definiens id="0">ct , c2 holding of a string and two spanning substrings</definiens>
				<definiens id="1">variables like so and st are implicitly universally quantified</definiens>
			</definition>
			<definition id="2">
				<sentence>Interpretation In the logic-based model , interpretation is the process of identifying from the possible interpretations ~ of s for 3 which form ( s , qt ) hold , ones that are consistent with the , ' , ,m~ , xt of interpretation .</sentence>
				<definiendum id="0">interpretation</definiendum>
				<definiens id="0">the process of identifying from the possible interpretations ~ of s for 3 which form ( s , qt ) hold , ones that are consistent with the , ' , ,m~ , xt of interpretation</definiens>
			</definition>
			<definition id="3">
				<sentence>A relation graph is a directed labeled graph consisting of a set of relation edges .</sentence>
				<definiendum id="0">relation graph</definiendum>
				<definiens id="0">a directed labeled graph consisting of a set of relation edges</definiens>
			</definition>
			<definition id="4">
				<sentence>Each edge has the form of an atomic proposition ~ ( wi , w~ ) where r is a relation symbol , wi is the lexical head of a phrase and wj is the lexical head of another phrase ( typically a subphrase of the phrase headed by w~ ) .</sentence>
				<definiendum id="0">r</definiendum>
				<definiendum id="1">wi</definiendum>
				<definiendum id="2">wj</definiendum>
				<definiens id="0">a relation symbol</definiens>
				<definiens id="1">the lexical head of another phrase ( typically a subphrase of the phrase headed by w~ )</definiens>
			</definition>
			<definition id="5">
				<sentence>The content model , P ( C ) , and generation model , P ( WIC ) , are components of the overall statistical model for spoken language translation given earlier .</sentence>
				<definiendum id="0">P ( C</definiendum>
				<definiendum id="1">P ( WIC</definiendum>
				<definiens id="0">the overall statistical model for spoken language translation given earlier</definiens>
			</definition>
			<definition id="6">
				<sentence>An expansion step takes a node and chooses a possibly empty set of edges ( relation labels and ending nodes ) starting from that node .</sentence>
				<definiendum id="0">expansion step</definiendum>
				<definiens id="0">takes a node and chooses a possibly empty set of edges ( relation labels and ending nodes ) starting from that node</definiens>
			</definition>
			<definition id="7">
				<sentence>To determine the probability of deriving a relation graph C for a phrase headed by h0 we make use of parameters ( 'dependency parameters ' ) P ( r ( h , w ) lh , r ) for the probability , given a node h and a relation r , that w is an r-dependent of h. Under the assumption that the dependents of a head are chosen independently from each other , the probability of deriving C is : P ( C ) = P ( Top ( ho ) ) I~Ir ( h.~ ) ¢c P ( r ( h , w ) lh , r ) where P ( Top ( ho ) ) is the probability of choosing h0 to start the derivation .</sentence>
				<definiendum id="0">P ( Top ( ho ) )</definiendum>
				<definiens id="0">the probability , given a node h and a relation r</definiens>
				<definiens id="1">the probability of choosing h0 to start the derivation</definiens>
			</definition>
			<definition id="8">
				<sentence>The probability of an expansion of h giving rise to local edges E ( h ) is now : P ( E ( h ) lh ) = Fir P ( N ( r , nr ) lh ) k ( nr ) I\ ] l &lt; i &lt; r~ P ( r ( h , w\ [ ) lh , r ) . where r ranges over the set of relation labels and h has nr r-dependents w~ ... w nP . k ( nr ) is a combinatorie constant for taking account of the fact that we are not distinguishing permutations of the dependents ( e.g. there are n , . ! permutations of the r-dependents of h if these dependents are all distinct ) . So if h0 is the root of a tree C , we have P ( C ) = P ( Top ( ho ) ) rIheh~aa , ( c ) P ( Ec ( h ) lh ) where heads ( C ) is thc set of nodes in C and Ec ( h ) is the set of edges headed by h in C. The above formulation is only an approximation for relation graphs that are not trees because the independence assumptions which allow the dependency parameters to be simply multiplied together no longer hold for the general case. Dependency graphs with cycles do arise as the most natural analyses of certain linguistic constructions , but calculating their probabilities on a node by node basis as above may still provide probability estimates that are accurate enough for practical purposes. Generation Model We now return to the generation model P ( WIC ) . As mentioned earlier , since C includes the words in W and a set of relations between them , the generation model is concerned only with surface order. One possibility is to use 'bi-relation ' parameters for the probability that an ri-dependent immediately follows an u-dependent. This approach is problematic for oui : overall statistical model because such parameters are not independent from the 'detail ' parameters specifying the number of r-dependents of a head. We therefore adopt the use of 'sequencing ' parameters , these being probabilities of particular orderings of dependents given that the multiset of dependency relations is known. We let the identity relation e stand for the head itself. Specifically , we have parameters P ( slM ( s ) ) where s is a sequence of relation labels including an occurrence of e and M ( s ) is the multiset for this sequence. For a head h in a relation graph C , let swch be the sequence of dependent relations induced by a particular word string W generated from C. We now have s &gt; ( WlC ) = I-Ih~w ( Il .</sentence>
				<definiendum id="0">P ( r</definiendum>
				<definiendum id="1">k ( nr )</definiendum>
				<definiendum id="2">s</definiendum>
				<definiens id="0">the root of a tree C</definiens>
				<definiens id="1">thc set of nodes in C and Ec ( h ) is the set of edges headed by h in C. The above formulation is only an approximation for relation graphs that are not trees because the independence assumptions which allow the dependency parameters to be simply multiplied together no longer hold for the general case. Dependency graphs with cycles do arise as the most natural analyses of certain linguistic constructions , but calculating their probabilities on a node by node basis as above may still provide probability estimates that are accurate enough for practical purposes. Generation Model We now return to the generation model P ( WIC ) . As mentioned earlier , since C includes the words in W and a set of relations between them</definiens>
			</definition>
			<definition id="9">
				<sentence>ly , and Et is the set of edges for the target graph .</sentence>
				<definiendum id="0">Et</definiendum>
				<definiens id="0">the set of edges for the target graph</definiens>
			</definition>
			<definition id="10">
				<sentence>These sets are assumed to be disjoint for different source graph nodes , so we can replace the factors in the above product with parameters : P ( MIw ) where w is a source language word and M is a multiset of target language words .</sentence>
				<definiendum id="0">w</definiendum>
				<definiendum id="1">M</definiendum>
				<definiens id="0">a source language word</definiens>
				<definiens id="1">a multiset of target language words</definiens>
			</definition>
			<definition id="11">
				<sentence>To apply a derivation step we need a notion of graph matching that respects edge labels : g is an isomorphism ( modulo node labels ) from a graph G to a graph H if g is a one-one and onto function from the nodes of G to the nodes of H such that r ( a , b ) e V iff r ( g ( a ) , g ( b ) ) • H. The derivation step with parameter P ( T\ [ IS~ , f~ ) is applicable to the source edges St , under the alignment f , giving rise to the target edges Ti if ( i ) there is an isomorphism hi from S~ to Si ( ii ) there is an isomorphism gi from ~ to T~ ' ( iii ) for any node v of Ti it must be the case that hi ( fi ( gi ( v ) ) ) -f ( v ) .</sentence>
				<definiendum id="0">g</definiendum>
				<definiens id="0">a one-one and onto function from the nodes of G to the nodes of H such that r ( a , b ) e V iff r</definiens>
			</definition>
</paper>

		<paper id="0330">
</paper>

		<paper id="0313">
			<definition id="0">
				<sentence>In contrast , causation designates a relation that is interpreted by the speaker as being a relation between a cause and an effect , cf. definition below .</sentence>
				<definiendum id="0">causation</definiendum>
				<definiens id="0">designates a relation that is interpreted by the speaker as being a relation between a cause and an effect</definiens>
			</definition>
			<definition id="1">
				<sentence>Text generation is a process in which meaning represented as non-linguistic knowledge at a higher level of abstraction than wordings is organised and re-expressed over a number of steps so that it can be presented as worded units .</sentence>
				<definiendum id="0">Text generation</definiendum>
				<definiens id="0">a process in which meaning represented as non-linguistic knowledge at a higher level of abstraction than wordings is organised and re-expressed over a number of steps so that it can be presented as worded units</definiens>
			</definition>
			<definition id="2">
				<sentence>HaUiday sets the following correspondences between context and language : field tends to determine ideational meanings , tenor interpersonal ones , and mode textual ones .</sentence>
				<definiendum id="0">HaUiday</definiendum>
				<definiens id="0">sets the following correspondences between context and language : field tends to determine ideational meanings , tenor interpersonal ones , and mode textual ones</definiens>
			</definition>
			<definition id="3">
				<sentence>The Causer is the participant viewed as bringing about the entire event ; the Causee is the participant carrying out the activity designated by the effected predicate ; and the Affectee , when expressed , is the participant `` that is the endpoint of the energy ( literal or metaphorical ) expended in the entire causative event '' ( Kemmer and Verhagen , forthcoming ) .</sentence>
				<definiendum id="0">Causer</definiendum>
				<definiendum id="1">Causee</definiendum>
				<definiens id="0">the participant carrying out the activity designated by the effected predicate</definiens>
				<definiens id="1">the endpoint of the energy ( literal or metaphorical ) expended in the entire causative event ''</definiens>
			</definition>
</paper>

		<paper id="0320">
			<definition id="0">
				<sentence>Text planning is the task for a speaker ( S ) of deciding what information to : communicate to a hearer ( H ) and how and when to communicate it .</sentence>
				<definiendum id="0">Text planning</definiendum>
			</definition>
			<definition id="1">
				<sentence>Examples of presentational relations include the MOTIVATION relation , which increases H 's desire to perform an action , hopefully persuading H to form an intention to do the action .</sentence>
				<definiendum id="0">MOTIVATION relation</definiendum>
				<definiens id="0">increases H 's desire to perform an action</definiens>
			</definition>
			<definition id="2">
				<sentence>Both subject-matter and presentational relations relate two clauses : ( 1 ) the NUCLEUS which realizes the main point ; and ( 2 ) the SATELLITE which is auxiliary optional information .</sentence>
				<definiendum id="0">SATELLITE</definiendum>
				<definiens id="0">auxiliary optional information</definiens>
			</definition>
			<definition id="3">
				<sentence>For example in the MOTIVATION relation shown in figure 1 , the SATELLITE is the belief which provides motivation to do the action realized by the proposal or suggestion in the NI ; CLEUS .</sentence>
				<definiendum id="0">SATELLITE</definiendum>
			</definition>
			<definition id="4">
				<sentence>AWM consists of a three dimensional space in which propositions acquired from perceiving the world are stored in chronological sequence according to the location of a moving memory pointer .</sentence>
				<definiendum id="0">AWM</definiendum>
				<definiens id="0">consists of a three dimensional space in which propositions acquired from perceiving the world are stored in chronological sequence according to the location of a moving memory pointer</definiens>
			</definition>
			<definition id="5">
				<sentence>The DESIGN-IIOUSE plan requires the agents to agree on how to DESIGN-R .</sentence>
				<definiendum id="0">DESIGN-IIOUSE plan</definiendum>
				<definiens id="0">requires the agents to agree on how to DESIGN-R</definiens>
			</definition>
			<definition id="6">
				<sentence>The All-Implicit strategy is an expansion of a discourse plan to make a PROPOSAL , in which a PROPOSAL decomposes trivially to the communicative act of Pn .</sentence>
				<definiendum id="0">All-Implicit strategy</definiendum>
			</definition>
</paper>

		<paper id="0103">
			<definition id="0">
				<sentence>The statistical methods are based on distributional analysis ( we defined a measure called mutual conditioned plausibility , a derivation of the well known mutual information ) , and cluster analysis ( a COBWEB-like algorithm for word classification is presented in \ [ Basili et al , 1993 , a\ ] ) .</sentence>
				<definiendum id="0">cluster analysis</definiendum>
				<definiens id="0">a derivation of the well known mutual information ) , and</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , the following collocations including the word measurement in the RSD : V prep_N ( deriveJrom , measurement ) , V prep_N ( determineJrom , measurement ) and V_prep_N ( infer , from , measurement ) let the ARIOSTO_LEX system learn the following selectional restriction : \ [ COGNrNONI &lt; - ( /igurative_source ) &lt; -\ [ measurement\ ] , where COGNITION is a Wordnet category for the verbs determine , infer and derive , and figurative_source is one of the conceptual relations used .</sentence>
				<definiendum id="0">COGNITION</definiendum>
				<definiendum id="1">figurative_source</definiendum>
				<definiens id="0">V prep_N ( deriveJrom , measurement ) , V prep_N ( determineJrom , measurement</definiens>
				<definiens id="1">a Wordnet category for the verbs determine , infer and derive , and</definiens>
				<definiens id="2">one of the conceptual relations used</definiens>
			</definition>
			<definition id="2">
				<sentence>In the RSD ( Remote Sensing ) we found the pattem : produce - &gt; ( agent ) - &gt; \ [ ORGANIZATION , PERSON\ ] - &gt; ( source ) - &gt; \ [ INSTRUMENTALITY\ ] or : by abbreviations , e.g. CGN = 'cognition , knowledge ' .</sentence>
				<definiendum id="0">RSD</definiendum>
				<definiens id="0">INSTRUMENTALITY\ ] or : by abbreviations , e.g. CGN = 'cognition , knowledge '</definiens>
			</definition>
</paper>

		<paper id="0323">
			<definition id="0">
				<sentence>NL-Soar is a computer system that performs language comprehension and generation within the framework of the Soar architecture \ [ New90\ ] .</sentence>
				<definiendum id="0">NL-Soar</definiendum>
			</definition>
			<definition id="1">
				<sentence>NL-Soar provides language capabilities for systems working in a real-time environment .</sentence>
				<definiendum id="0">NL-Soar</definiendum>
				<definiens id="0">provides language capabilities for systems working in a real-time environment</definiens>
			</definition>
			<definition id="2">
				<sentence>NL-Soar is a language comprehension and generation facility designed to provide integrated real-time 1 language capabilities for other systems built within the Soar architecture \ [ New90\ ] .</sentence>
				<definiendum id="0">NL-Soar</definiendum>
			</definition>
			<definition id="3">
				<sentence>This allows NL-Soar to operate without limiting the task 's ability to respond to things that happen during generation , and vice versa .</sentence>
				<definiendum id="0">NL-Soar</definiendum>
				<definiens id="0">to operate without limiting the task 's ability to respond to things that happen during generation , and vice versa</definiens>
			</definition>
			<definition id="4">
				<sentence>Soar is an architecture for building cognitive models and AI systems that has been applied to a wide range of problems \ [ RLN93\ ] .</sentence>
				<definiendum id="0">Soar</definiendum>
			</definition>
			<definition id="5">
				<sentence>~J ~\ [ Pr°~cee~ Decision Cycl ( l : ) ~ Figure 1 : The Soar Architecture Soar has two different memories : a short-term or working memory ( 2 ) and a long-term production or recognition memory ( 1 ) .</sentence>
				<definiendum id="0">~J ~\ [ Pr°~cee~ Decision Cycl</definiendum>
			</definition>
			<definition id="6">
				<sentence>When quiescence is reached at the end of a decision cycle , Soar attempts to decide what to do next ( 6 ) .</sentence>
				<definiendum id="0">Soar</definiendum>
			</definition>
			<definition id="7">
				<sentence>Soar builds new productions ( called `` chunks '' ) whenever processing in a subgoal produces a result in the state of a higher goal ( i.e. adds or removes an attribute-value pair ) .</sentence>
				<definiendum id="0">Soar</definiendum>
				<definiens id="0">builds new productions ( called `` chunks '' ) whenever processing in a subgoal produces a result in the state of a higher goal ( i.e. adds or removes an attribute-value pair )</definiens>
			</definition>
			<definition id="8">
				<sentence>Situation Model ( or s-model ) This models the objects , properties , and relations discussed in the discourse , i.e. a representation of the semantics of what is being said .</sentence>
				<definiendum id="0">Situation Model</definiendum>
				<definiens id="0">properties , and relations discussed in the discourse</definiens>
			</definition>
			<definition id="9">
				<sentence>NTD-Soar is a system designed to simulate the activities of the NASA Test Director , the person responsible for co-ordinating the activities involved in the launch of a space shuttle \ [ NLJ94\ ] .</sentence>
				<definiendum id="0">NTD-Soar</definiendum>
				<definiendum id="1">Test Director</definiendum>
				<definiens id="0">a system designed to simulate the activities of the NASA</definiens>
			</definition>
</paper>

		<paper id="0332">
			<definition id="0">
				<sentence>A transition is a concept trans with three relations , pre , means , and post .</sentence>
				<definiendum id="0">transition</definiendum>
				<definiens id="0">a concept trans with three relations , pre , means , and post</definiens>
			</definition>
			<definition id="1">
				<sentence>The association for trans is defined by the rule : Trans=\ [ trans\ ] ( means ) -Ev=\ [ event\ ] Pre-R= ( pre ) ~ Pre-C=concept Post-R= ( post ) -- ~ Post-C=concept Res when Trans ~ Resl=cat\ [ Event=cat Pre-R ~ Res2= cat IResl \ [ Pre= cat Pre-C ~ Pre Post-R ~ Res\ [ Res21Post=ca~ Post-C ~ Post , Ev ==-z Event 247 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 The transition is matched to bind variables in the head .</sentence>
				<definiendum id="0">association for trans</definiendum>
				<definiendum id="1">Pre-C=concept Post-R=</definiendum>
				<definiens id="0">The transition is matched to bind variables in the head</definiens>
			</definition>
</paper>

		<paper id="0322">
			<definition id="0">
				<sentence>A full answer consists of a direct answer ( which we call the nucleus ) and , possibly , extra relevant information ( satellites ) .</sentence>
				<definiendum id="0">full answer</definiendum>
				<definiens id="0">consists of a direct answer ( which we call the nucleus ) and , possibly , extra relevant information ( satellites )</definiens>
			</definition>
			<definition id="1">
				<sentence>191 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 Operator : Use-cause Use-condition Use-contrast Use-elaboration Use-obstacle Use-otherwise Use-possible-cause Use-possible-obstacle Use-result Use-usually Stimulus conditions : Explanation-indicated Clarify-conditionindicated Appeasement-indicated Answer-ref-indicated Clarify-extentindicated Substitute-indicated Answer-ref-indicated Clarify-conceptindicated Explanation-indicated Excuse-indicated Explanation-indicated Excuse-indicated Explanation-indicated ExPlanation-indicated Excuse-indicated Explanation-indicated Explanation-indicated Table 2 : Stimulus conditions of discourse plan operators ators for providing causal explanations .</sentence>
				<definiendum id="0">Use-cause Use-condition Use-contrast Use-elaboration Use-obstacle Use-otherwise Use-possible-cause Use-possible-obstacle Use-result</definiendum>
				<definiens id="0">Explanation-indicated Clarify-conditionindicated Appeasement-indicated Answer-ref-indicated Clarify-extentindicated Substitute-indicated Answer-ref-indicated Clarify-conceptindicated Explanation-indicated Excuse-indicated Explanation-indicated Excuse-indicated Explanation-indicated ExPlanation-indicated Excuse-indicated Explanation-indicated Explanation-indicated Table 2</definiens>
			</definition>
			<definition id="2">
				<sentence>For example in ( 2 ) , 6 which indirectly conveys a No , R gives an explanation of why R wo n't get a car .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">gives an explanation of why R wo n't get a car</definiens>
			</definition>
			<definition id="3">
				<sentence>In the first phase , content planning , the generator creates a discourse plan for a full answer , i.e. , a direct answer and extra appropriate information , e.g. ( lc ) given explicitly , followed by ( ld ) : ( le ) .</sentence>
				<definiendum id="0">content planning</definiendum>
				<definiendum id="1">generator</definiendum>
				<definiens id="0">creates a discourse plan for a full answer , i.e. , a direct answer and extra appropriate information</definiens>
			</definition>
			<definition id="4">
				<sentence>Note that if this plan were output 195 7th International Generation Workshop • Kennebunkport , Maine • June 21-24 , 1994 ( Answer-no s h ( occur ( go-party s ) past ) ) : Nucleus : b. ( inform s h ( not ( occur ( go-party s ) past ) ) ) Satellites : ( Use-obstacle s h ( not ( occur ( go-party s ) past ) ) ) Nucleus : ( inform s h ( not ( in-state ( can-sit sitter ) past ) ) ) Satellites : ( Use-obstacle s h ( not ( in-state ( can-sit sitter ) past ) ) ) Nucleus : ( inform s h ( in-state ( sick sitter ) past ) ) C. d. Figure 4 : Discourse plan for ( 11 ) without undergoing plan-pruning , it would be realized as a direct answer ( llb ) , and extra information ( llc ) - ( lld ) .</sentence>
				<definiendum id="0">inform s</definiendum>
				<definiens id="0">go-party s ) past ) ) ) Nucleus : ( inform s h ( not ( in-state ( can-sit sitter ) past )</definiens>
				<definiens id="1">a direct answer ( llb ) , and extra information</definiens>
			</definition>
			<definition id="5">
				<sentence>In answer to ( 16a ) , a negative-polarity , negative-bias question , 19 R provides an indirect answer , giving the extent to which the proposition that R has been up18 ( 15 ) and ( 16 ) are Stenstrbm 's ( 105 ) and ( 111 ) , respectively .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">provides an indirect answer , giving the extent to which the proposition</definiens>
			</definition>
			<definition id="6">
				<sentence>19Although it is expressed as a declarative sentence , Stenstr6m classifies ( 16a ) as a request for confirmation , which we treat as a type of Yes-No question .</sentence>
				<definiendum id="0">confirmation</definiendum>
				<definiens id="0">a declarative sentence</definiens>
			</definition>
			<definition id="7">
				<sentence>Ours is the first system to generate a wide range of types of indirect answers ( as well as full answers ) .</sentence>
				<definiendum id="0">Ours</definiendum>
			</definition>
</paper>

		<paper id="0111">
			<definition id="0">
				<sentence>Decision trees are an established learning technique that is also based on surveying a wide space of possible factors and repeatedly selecting a most significant factor or combination of factors .</sentence>
				<definiendum id="0">Decision trees</definiendum>
				<definiens id="0">an established learning technique that is also based on surveying a wide space of possible factors and repeatedly selecting a most significant factor or combination of factors</definiens>
			</definition>
			<definition id="1">
				<sentence>Brill suggests the simple ranking function of choosing ( one of ) the rule ( s ) that makes the largest net improvement in the current training set tag assignments .</sentence>
				<definiendum id="0">Brill</definiendum>
				<definiens id="0">suggests the simple ranking function of choosing ( one of ) the rule ( s ) that makes the largest net improvement in the current training set tag assignments</definiens>
			</definition>
			<definition id="2">
				<sentence>Experiments suggest that part of the difference is due to knowledge embodied in the templates .</sentence>
				<definiendum id="0">Experiments</definiendum>
				<definiens id="0">suggest that part of the difference is due to knowledge embodied in the templates</definiens>
			</definition>
</paper>

		<paper id="0307">
			<definition id="0">
				<sentence>Because set ( speed_SP ) is a basic-level operation , SPIN selects it as a sequential operation .</sentence>
				<definiendum id="0">speed_SP )</definiendum>
				<definiens id="0">a basic-level operation , SPIN selects it as a sequential operation</definiens>
			</definition>
			<definition id="1">
				<sentence>Because the operation specifies a particular instrument to be used , SPIN chooses an enablement to communicate this guidance ( sentence 3 ) .</sentence>
				<definiendum id="0">SPIN</definiendum>
			</definition>
			<definition id="2">
				<sentence>SPIN puts an emphasis on the planning stage , thus many aspects of the linguistic realization are left unconsidered .</sentence>
				<definiendum id="0">SPIN</definiendum>
				<definiens id="0">puts an emphasis on the planning stage</definiens>
			</definition>
</paper>

		<paper id="0202">
			<definition id="0">
				<sentence>Note that a simple-phon has the features ROOTS , VOWELS , and SYLLS , which must be of types root + , vowel + , and syllable + , respectively .</sentence>
				<definiendum id="0">SYLLS</definiendum>
				<definiens id="0">must be of types root + , vowel + , and syllable +</definiens>
			</definition>
			<definition id="1">
				<sentence>IT\ ] root • \ [ ~\ ] appendix A VOWELS : \ [ \ ] VOWel + A SYLLS : ( EJsyllable .</sentence>
				<definiendum id="0">SYLLS</definiendum>
				<definiens id="0">root • \ [ ~\ ] appendix A VOWELS : \ [ \ ] VOWel + A</definiens>
			</definition>
			<definition id="2">
				<sentence>Metrical Phonology Malak-Malak Ih'call that the type vowel is defined as having a feature STm , ; SS .</sentence>
				<definiendum id="0">Metrical Phonology Malak-Malak Ih'call</definiendum>
				<definiens id="0">having a feature STm , ; SS</definiens>
			</definition>
			<definition id="3">
				<sentence>, I. Our uniform coustraint-based architecture allows us to : complex-word2 ( VOWELS : \ [ Trowel + * ~vower A ROOTS : \ [ ~Jro0t + • \ [ 4Z\ ] rool+ A HARMONY : harm A WORD : qA SYNSEM : ( SEM : ( OPERATOR : \ [ ~\ ] operator A OPERAND : ~sem ) A SYNCA'I ' : \ [ ~syncat A SUBCAT : \ [ 8_-\ ] subcat-list ) A STEM : ( word A VOWELS : E\ ] A ROOTS : \ [ ~\ ] A SVNSEM : E\ ] ( SEM : ~ ^ SUBCAT : \ [ ~\ ] ) /N HARMONY : L_Tj ) A MOD : ( SUJfiZ A VOWELS : E\ ] A ROOTS : E\ ] A SYNSEM : ( SEM : ( OPERATOR : \ [ ~\ ] A OPERAND : E\ ] ) ) A SYNCAT : \ [ \ ] A SUBCAT : ( HD : E\ ] A TL : \ [ @ ) ^ HARMONY : \ [ ~harm ) A corn l'atih le ( \ [ ~\ ] , lED .</sentence>
				<definiendum id="0">SVNSEM</definiendum>
				<definiens id="0">E\ ] ( SEM : ~ ^ SUBCAT : \ [ ~\ ] ) /N HARMONY : L_Tj ) A MOD : ( SUJfiZ A VOWELS : E\ ] A ROOTS : E\ ] A SYNSEM : ( SEM : ( OPERATOR : \ [ ~\ ] A OPERAND : E\ ] ) ) A SYNCAT : \ [ \ ] A SUBCAT : ( HD : E\ ] A TL : \ [ @</definiens>
			</definition>
</paper>

		<paper id="0303">
			<definition id="0">
				<sentence>Functional hierarchy ( FH ) is a new paradigm for organizing expert system knowledge bases , based on the procedural abstraction principles of Liskov and Guttag \ [ Liskov &amp; Guttag 1986\ ] .</sentence>
				<definiendum id="0">Functional hierarchy</definiendum>
				<definiendum id="1">FH )</definiendum>
			</definition>
			<definition id="1">
				<sentence>There is also RETURN for when a student returns to a branch he skipped from , which is the equivalent of GOOD , and SKIPPING-RETURN , which is a return to a skipped branch at the price of leaving the current branch unfinished , equivalent to SKIPPING .</sentence>
				<definiendum id="0">SKIPPING-RETURN</definiendum>
				<definiens id="0">the equivalent of GOOD</definiens>
			</definition>
			<definition id="2">
				<sentence>MACH-III includes navigational milestones in its output .</sentence>
				<definiendum id="0">MACH-III</definiendum>
				<definiens id="0">includes navigational milestones in its output</definiens>
			</definition>
			<definition id="3">
				<sentence>The intended audience for the MACH-III critiques consists of students who have already had several weeks of training in troubleshooting HAWK radar systems .</sentence>
				<definiendum id="0">intended audience for the MACH-III critiques</definiendum>
				<definiens id="0">consists of students who have already had several weeks of training in troubleshooting HAWK radar systems</definiens>
			</definition>
			<definition id="4">
				<sentence>Either the generation system builds the structure to produce a coherent text ( e.g. \ [ Hovy 1988\ ] ) , or plausible structures ( such as McKeown 's schemas \ [ McKeown 1982\ ] ) are built ahead of time , and the system selects one that defines the desired structure .</sentence>
				<definiendum id="0">generation system</definiendum>
				<definiens id="0">builds the structure to produce a coherent text ( e.g. \ [ Hovy 1988\ ] ) , or plausible structures ( such as McKeown 's schemas \ [</definiens>
			</definition>
			<definition id="5">
				<sentence>Functional hierarchy is a paradigm that explicitly organizes the knowledge to reflect the structure of the knowledge used by actual experts .</sentence>
				<definiendum id="0">Functional hierarchy</definiendum>
				<definiens id="0">a paradigm that explicitly organizes the knowledge to reflect the structure of the knowledge used by actual experts</definiens>
			</definition>
			<definition id="6">
				<sentence>Generated RST Structure ( Part 1 ) The actual MACH-III output is ( START # &lt; NODE FEEDTHROUGH NOT HULLED &gt; ) ( START # &lt; NODE RF INPUTS &gt; ) ( SKIPPING ( TEST-THING ( THE W4 ) : CONTINUITY ) ( # &lt; NODE NOISE INTRODUCED &gt; ) ) ( RETURNING # &lt; NODE NOISE INTRODUCED &gt; ) ( SKIPPING-RETURN ( REPLACETHING ( THE SCAN-DRIVERASSEMBLY ) ) ( # &lt; NODE RF INPUTS &gt; # &lt; NODE FEEDTHROUGH NOT HULLED &gt; ) ) ( GOOD ( PUSH BITE-TEST-AFTERREPLACE # &lt; DEVICE SCANDRIVER -ASS EMBLY &gt; ) ) ( FINISHED # &lt; NODE NOISE INTRODUCED &gt; ) Recall that MACH-III critiques are basically concerned with student actions .</sentence>
				<definiendum id="0">Generated RST Structure</definiendum>
				<definiens id="0">PUSH BITE-TEST-AFTERREPLACE # &lt; DEVICE SCANDRIVER -ASS EMBLY &gt; ) ) ( FINISHED # &lt; NODE NOISE INTRODUCED &gt; ) Recall that MACH-III critiques are basically concerned with student actions</definiens>
			</definition>
</paper>

		<paper id="0331">
			<definition id="0">
				<sentence>The LOLITA generator has been developed as a part of a complete NL system .</sentence>
				<definiendum id="0">LOLITA generator</definiendum>
			</definition>
			<definition id="1">
				<sentence>Realisation is the final step in the generation hierarchy and involves the traversal of the semantic network to produce sentences .</sentence>
				<definiendum id="0">Realisation</definiendum>
				<definiens id="0">the final step in the generation hierarchy and involves the traversal of the semantic network to produce sentences</definiens>
			</definition>
</paper>

		<paper id="0113">
			<definition id="0">
				<sentence>( ( SPEECH-ACT ( *MULTIPLE* *STATE-CONSTRAINT *REJECT ) ) ( SENTENCE-TYPE *STATE ) ( FRAME *BUSY ) ( WHO ( ( FRAME *I ) ) ) ( WItEN ( ( FRAME *SPECIAL-TIME ) ( NAME WEEK ) ( SPECIFIER ( *MULTIPLE* ALL-RANGE NEXT ) ) ) ) ) Figure 1 : Sample interlingua representation returned by the parser for 'Tm busy all next The interlingua specification determines the set of possible interlingua structures .</sentence>
				<definiendum id="0">SPEECH-ACT ( *MULTIPLE* *STATE-CONSTRAINT *REJECT ) ) ( SENTENCE-TYPE *STATE )</definiendum>
				<definiendum id="1">FRAME *BUSY ) ( WHO</definiendum>
				<definiens id="0">Sample interlingua representation returned by the parser for 'Tm busy all next The interlingua specification determines the set of possible interlingua structures</definiens>
			</definition>
			<definition id="1">
				<sentence>It is defined by the following formula : log\ [ P ( ck Ivm ) /P ( cD\ ] where ck is the kth element of the input vector and vrn is the mth element of the output vector .</sentence>
				<definiendum id="0">ck</definiendum>
				<definiendum id="1">vrn</definiendum>
				<definiens id="0">the following formula : log\ [ P ( ck Ivm ) /P ( cD\ ] where</definiens>
				<definiens id="1">the kth element of the input vector</definiens>
				<definiens id="2">the mth element of the output vector</definiens>
			</definition>
			<definition id="2">
				<sentence>( A chunk is the Repair Module 's internal representation of a skipped portion of the input utterance . )</sentence>
				<definiendum id="0">chunk</definiendum>
				<definiens id="0">the Repair Module 's internal representation of a skipped portion of the input utterance</definiens>
			</definition>
			<definition id="3">
				<sentence>In the case that this is a new symbol which the net has no information about yet , it will return a ranked list of types based on how frequently those types are the correct output .</sentence>
				<definiendum id="0">types</definiendum>
				<definiens id="0">the net has no information about yet , it will return a ranked list of types based on how frequently those</definiens>
			</definition>
			<definition id="4">
				<sentence>System Architecture The heart of the Repair Module , see Figure 5 , is the Hypothesis Generation Module whose purpose it is to generate repair hypotheses which axe instructions for reconstructing the speaker 's meaning by performing operations on the Chunk Structure of the parse .</sentence>
				<definiendum id="0">Repair Module</definiendum>
			</definition>
			<definition id="5">
				<sentence>The Chunk Structure represents the relationships between tile partial analysis and the analysis for each skipped segm~nt of the utterance .</sentence>
				<definiendum id="0">Chunk Structure</definiendum>
				<definiens id="0">represents the relationships between tile partial analysis</definiens>
			</definition>
			<definition id="6">
				<sentence>After the user responds , the status of the hy07 m Repair Module E i I , 'igurc 5 : Repair Module System Arehitechture pothesis is noted in the Dynamic Repair Memory and if the response was positive , the Interlingua Update Module makes the specified repair and updates the Dynamic Repair Memory structure .</sentence>
				<definiendum id="0">Interlingua Update Module</definiendum>
				<definiens id="0">makes the specified repair and updates the Dynamic Repair Memory structure</definiens>
			</definition>
			<definition id="7">
				<sentence>It is the Interlingua Update Module which uses these hypotheses to actually make the repairs in order to derive the complete meaning representation for the utterance from the partial analysis and the analysis for the skipped portions .</sentence>
				<definiendum id="0">Interlingua Update Module</definiendum>
				<definiens id="0">uses these hypotheses to actually make the repairs in order to derive the complete meaning representation for the utterance from the partial analysis and the analysis for the skipped portions</definiens>
			</definition>
			<definition id="8">
				<sentence>Hypothesis : ( top-level-frame ( ( frame-name *free ) ) ) Question : Is your sentence mainly about someone being free ?</sentence>
				<definiendum id="0">Hypothesis</definiendum>
			</definition>
			<definition id="9">
				<sentence>Current Slot : who Hypothesis : ( frame-slot ( ( frame-name *free ) ( who ( ( frame *i ) ) ) ) ) Question : Is it `` I '' who is being free in your sentence ?</sentence>
				<definiendum id="0">Current Slot</definiendum>
			</definition>
</paper>

		<paper id="0317">
			<definition id="0">
				<sentence>Depending on the discourse history , the following are two of the possible verbalizations : ment of U , and u is an element of U , u* 1v = u. '' element , u * 1v= u. '' Note that , an explicit reference to a premise or an inference method is not restricted to a nominal phrase , as opposed to the traditional subsequent references .</sentence>
				<definiendum id="0">u</definiendum>
			</definition>
			<definition id="1">
				<sentence>PROVERB is a text planner that verbalizes natural deduction ( ND ) style proofs \ [ Gen35\ ] .</sentence>
				<definiendum id="0">PROVERB</definiendum>
			</definition>
			<definition id="2">
				<sentence>Therefore , our discourse model consists basically of the part of the input proof tree which has already been conveyed .</sentence>
				<definiendum id="0">discourse model</definiendum>
				<definiens id="0">consists basically of the part of the input proof tree which has already been conveyed</definiens>
			</definition>
			<definition id="3">
				<sentence>For instance , the PCA ( Begin-Cases Goal : Formula Assumptions : ( A B ) ) creates two attentional units with A and B as the assumptions , and Formula as the goal by producing the verbalization : `` To prove Formula , let us consider the two cases by assuming A and B. '' Top-down presentation operators express communicative norms concerning how a proof to be presented can be split into subproofs , as well as how the hierarchically structured subproofs can be mapped onto some linear order for presentation .</sentence>
				<definiendum id="0">PCA</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiendum id="2">Formula</definiendum>
				<definiens id="0">communicative norms concerning how a proof to be presented can be split into subproofs , as well as how the hierarchically structured subproofs can be mapped onto some linear order for presentation</definiens>
			</definition>
			<definition id="4">
				<sentence>In this formalism , every proof is a sequence of proof lines , each of them is of the form : Label ~ ~Conclusion ( Justification reason-pointers ) where Justification is either an ND inference rule , a definition or theorem , which justifies the derivation of the Conclusion using formulas in lines pointed to by reasonpointers as the premises .</sentence>
				<definiendum id="0">Justification</definiendum>
			</definition>
</paper>

		<paper id="0115">
			<definition id="0">
				<sentence>Brute-force methods ( ie those that exploit the massive raw computing power currently available cheaply ) may well produce some useful results ( eg Brown et al 1993 ) .</sentence>
				<definiendum id="0">Brute-force methods</definiendum>
				<definiens id="0">ie those that exploit the massive raw computing power currently available cheaply ) may well produce some useful results</definiens>
			</definition>
</paper>

		<paper id="0319">
			<definition id="0">
				<sentence>SPOKESMAN uses the realization specification language of MUMBLE \ [ McDonald , 1983\ ] as its deep syntactic representation ; I have found it difficult to compare this language to the others , but McDonald ( personal communication ) agrees that it conveys essentially the same information as SPL .</sentence>
				<definiendum id="0">McDonald</definiendum>
				<definiens id="0">personal communication ) agrees that it conveys essentially the same information as SPL</definiens>
			</definition>
</paper>

	</volume>
