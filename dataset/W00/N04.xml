<?xml version="1.0" encoding="UTF-8"?>
	<volume id="N04">

		<paper id="1010">
			<definition id="0">
				<sentence>Our acquisition algorithm consists of four steps , as explained in this section .</sentence>
				<definiendum id="0">acquisition algorithm</definiendum>
				<definiens id="0">consists of four steps , as explained in this section</definiens>
			</definition>
			<definition id="1">
				<sentence>There could be many non-hypernyms that are 3In our experiments , N is a set consisting of 37,639 words , each of which appeared more than 500 times in 33 years of Japanese newspaper articles ( Yomiuri newspaper 1987-2001 , Mainichi newspaper 1991-1999 and Nikkei newspaper 19831990 ; 3.01 GB in total ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiens id="0">a set consisting of 37,639 words</definiens>
			</definition>
			<definition id="2">
				<sentence>Recall that a common suffix of an HCS is a good candidate to be a hypernym .</sentence>
				<definiendum id="0">HCS</definiendum>
				<definiens id="0">a good candidate to be a hypernym</definiens>
			</definition>
			<definition id="3">
				<sentence>O ( group of galaxies ) , ÔøΩ RÔøΩÔøΩÔøΩÔøΩ ( member ) , ÔøΩO ‚ÄìÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩO ( Andromeda Galaxy ) * , ÔøΩO % ( The Galaxy ) * , ( galaxy ) 10 ‚Äì + ‚Äì ‚Äì ÔøΩ ÔøΩÔøΩO ( local group of galaxies ) ÔøΩÔøΩÔøΩÔøΩ ( Brazil ) , ÔøΩÔøΩÔøΩÔøΩÔøΩ ( Philippine ) ,  ( Korea ) , ‚ÄìÔøΩÔøΩÔøΩ ( India ) , ÔøΩÔøΩÔøΩÔøΩ ( U.S.A. ) , ÔøΩÔøΩ ( Thailand ) , ÔøΩÔøΩ80 + ‚Äì + ‚Äì ÔøΩ ( China ) , ÔøΩÔøΩÔøΩ ( Peru ) , ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ( Australia ) , ( Japan ) ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ( Argentina ) , ÔøΩÔøΩÔøΩÔøΩ ( Spain ) ‚Äò*‚Äô indicates a hyponym candidate that is a true hyponym of the provided hypernym candidate .</sentence>
				<definiendum id="0">O ( group of galaxies</definiendum>
				<definiendum id="1">ÔøΩO ‚ÄìÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩO</definiendum>
				<definiendum id="2">Galaxy ) *</definiendum>
				<definiendum id="3">ÔøΩÔøΩÔøΩÔøΩ ( Spain ) ‚Äò*‚Äô</definiendum>
				<definiens id="0">indicates a hyponym candidate that is a true hyponym of the provided hypernym candidate</definiens>
			</definition>
</paper>

		<paper id="1036">
			<definition id="0">
				<sentence>HQWU\ 1 ( WUDQVGXFHU ZKLFK LV FRQVWUXFWHG E\ DOLJQLQJ WDJJHG 1 ( V IURP WKH VDPH SDUDOOHO FRUSXV DFFRUGLQJ WR PXOWLSOH 1 ( DOLJQPHQW FRVWV +XDQJ HW DO $ PRQJ WDJJHG 3 ( 5621 DQG /2 &amp; $ 7,21 1 ( V 1 ( V DUH QRW FRYHUHG E\ WKH /‚Äô &amp; WUDQVODWLRQ OH [ LFRQ $ IWHU PDQXDOO\ UHPRYLQJ LQFRUUHFWO\ WDJJHG 1 ( V WUXH 1 ( V FRUUHVSRQGLQJ WR XQLTXH 1 ( V DUH WUDQVODWHG ZLWK WKH WUDQVOLWHUDWLRQ PRGHO SOXV WKH VHPDQWLF FRQWH [ W YHFWRU PRGHO DQG WKH WUDQVODWLRQ K\ SRWKHVHV DUH FRPSDUHG ZLWK WKH UHIHUHQFH WUDQVODWLRQV IRU HYDOXDWLRQ 7DEOH VKRZV WKH W\SH DQG WRNHQ 1 ( WUDQVODWLRQ SUHFLVLRQ XVLQJ GLIIHUHQW VLPLODULW\ PRGHOV ZKHUH ‚Ä°7UDQVOLW¬∑ PHDQV XVLQJ WKH WUDQVOLWHUDWLRQ PRGHO RQO\ DQG ‚Ä° 6 &amp; 9¬∑ PHDQV DGGLWLRQDOO\ FRPELQLQJ WKH FRQWH [ W YHFWRU VHPDQWLF PRGHO , W DOVR VKRZV WKH SHUIRUPDQFH RI WKH EDVHOLQH V\VWHP ZKHUH WKH WUDQVODWLRQV EDVLFDOO\ FRPH IURP VHYHUDO SKUDVH DQG 1 ( WUDQVGXFHUV WUDLQHG IURP WKH 0 ZRUGV ELOLQJXDO FRUSXV 7KH OLPLWHG SDUDO OHO FRUSXV FRYHUDJH H [ SODLQV WKH UHODWLYHO\ ORZHU SHU IRUPDQFH RI WKH % DVHOLQH V\VWHP DV WKH VRXUFH 1 ( V FDQQRW EH IRXQG LQ WKH SDUDOOHO FRUSXV : KHQ ILQGLQJ 1 ( WUDQVODWLRQV IURP WKH UHWULHYHG PRQROLQJXDO WH [ W WKH VXUIDFH VWULQJ WUDQVOLWHUDWLRQ PRGHO DORQH LQFUHDVHV WKH WUDQVODWLRQ SUHFLVLRQ E\ DERXW DQG WKH FRQWH [ W YHF WRU VHPDQWLF PRGHO DGGLWLRQDOO\ LPSURYHV WKH WUDQVODWLRQ DFFXUDF\ E\ DERXW ) XUWKHU HUURU DQDO\VLV LQGLFDWHV WKDW RI HUURUV DUH GXH WR WKH OLPLWHG FRYHUDJH RI UHWULHYHG GRFXPHQWV L H FRUUHFW 1 ( WUDQVODWLRQV DUH HLWKHU QRW LQFOXGHG LQ RU QRW UHWULHYHG IURP WKH LQGH [ HG ( QJOLVK FRUSXV 7RNHQ 3UHFLVLRQ 7\SH 3UHFLVLRQ % DVHOLQH 7UDQVOLW 6 &amp; 9 7DEOH 1 ( 7UDQVODWLRQ 3UHFLVLRQ : H LQWHJUDWH ERWK VHWV RI 1 ( WUDQVODWLRQ K\SRWKHVHV LQWR WKH EDVHOLQH V\VWHP ‚Ä° 7UDQVOLW¬∑ DQG ‚Ä° 6 &amp; 9¬∑ DQG WHVW WKHP LQ GLIIHUHQW WUDQVODWLRQ WDVNV WKH VPDOO GDWD WUDFN DQG WKH ODUJH GDWD WUDFN GLIIHULQJ LQ WKH DPRXQW RI ELOLQJXDO UHVRXUFHV DOORZHG IRU XVH 7R DFFXUDWHO\ PHDV XUH WKH FRQWULEXWLRQ RI WKH SURSRVHG 1 ( WUDQVODWLRQ PHWKRG ZH ILUVW H [ WUDFW VHQWHQFHV FRQWDLQLQJ WKHVH UDUHO\ RFFXUULQJ 1 ( V IURP WKH ZKROH WHVW VHW &amp; KL QHVH VHQWHQFHV WUDQVODWH DQG HYDOXDWH RQ WKLV VXEVHW WKHQ ZH HYDOXDWH WKH 1 ( WUDQVODWLRQV RQ WKH ZKROH WHVW GDWD 7KH WUDQVODWLRQ TXDOLW\ LV PHDVXUHG E\ WKH DXWR PDWLF 07 HYDOXDWLRQ PHWULFV VXFK DV 1,67 DQG % OHX VFRUHV 7DEOH VKRZV WKH WUDQVODWLRQ VFRUHV RI GLIIHUHQW V\V WHP FRQILJXUDWLRQV RQ WKH 1 ( VHQWHQFHV VXEVHW DQG WD EOH VKRZV WKH WUDQVODWLRQ VFRUHV RQ WKH ZKROH WHVW GDWD % HFDXVH WKH VHOHFWHG VHQWHQFHV DUH KDUG WR WUDQVODWH GXH WR WKHVH UDUHO\ RFFXUULQJ 1 ( V WKHLU WUDQVODWLRQV KDYH ORZHU 1,67 DQG % OHX VFRUHV WKDQ WKH ZKROH WHVW VHW GLIIHUHQFH LQ 1,67 DQG GLIIHUHQFH LQ % OHX IRU WKH % DVHOLQH : KHQ DGGLQJ WUDQVOLWHUDWHG 1 ( WUDQVODWLRQV DQ REYLRXV LPSURYHPHQW FDQ EH REVHUYHG LQ DOO WKH FDVHV $ GGLWLRQDOO\ DGGLQJ WKH FRQWH [ W YHFWRU PRGHO DOVR OHDGV WR D VPDOO EXW FRQVLVWHQW LPSURYHPHQW 6PDOO WUDFN /DUJH WUDFN 1,67 % OHX 1,67 % OHX % DVHOLQH 7UDQVOLW 6 &amp; 9 7DEOH &amp; ( 07 ( YDOXDWLRQ RQ 1 ( 6HQWHQFHV 6XEVHW 6PDOO WUDFN /DUJH WUDFN 1,67 % OHX 1,67 % OHX % DVHOLQH 7UDQVOLW 6 &amp; 9 7DEOH &amp; ( 07 ( YDOXDWLRQ RQ : KROH 7HVW 6HW &amp; RQFOXVLRQ : H SURSRVH DQ DSSURDFK WR WUDQVODWH UDUHO\ RFFXUULQJ 1 ( V E\ FRPELQLQJ WKHLU SKRQHWLF DQG VHPDQWLF VLPLODUL WLHV *LYHQ D VRXUFH 1 ( DQG LWV FRQWH [ W WKLV DSSURDFK JHQHUDWHV TXHULHV LQ WKH WDUJHW ODQJXDJH DFFRUGLQJ WR WKH FRQWH [ W WUDQVODWLRQ K\SRWKHVHV WKHQ VHDUFKHV IRU UHOH YDQW GRFXPHQWV IURP D WDUJHW FRUSXV 7DUJHW 1 ( V LQ UHWULHYHG GRFXPHQWV DUH FRPSDUHG ZLWK WKH VRXUFH 1 ( EDVHG RQ WKHLU SKRQHWLF DQG FRQWH [ WXDO VHPDQWLF VLPL ODULWLHV DQG WKH EHVW PDWFKHG RQH LV VHOHFWHG DV WKH FRU UHFW WUDQVODWLRQ ( [ SHULPHQWV VKRZ WKDW WKLV DSSURDFK DFKLHYHV RQ WUDQVODWLRQ DFFXUDF\ DQG FRQVLVWHQWO\ LPSURYHV WKH WUDQVODWLRQ TXDOLW\ RQ GLIIHUHQW WDVNV 5HIHUHQFHV &lt; $ O 2QDL ] DQ DQG .</sentence>
				<definiendum id="0">V IURP WKH VDPH SDUDOOHO FRUSXV DFFRUGLQJ WR PXOWLSOH 1</definiendum>
				<definiens id="0">H SURSRVH DQ DSSURDFK WR WUDQVODWH UDUHO\ RFFXUULQJ 1 ( V E\ FRPELQLQJ WKHLU SKRQHWLF DQG VHPDQWLF VLPLODUL WLHV *LYHQ D VRXUFH 1 ( DQG LWV FRQWH [ W WKLV DSSURDFK JHQHUDWHV TXHULHV LQ WKH WDUJHW ODQJXDJH DFFRUGLQJ WR WKH FRQWH [ W WUDQVODWLRQ K\SRWKHVHV WKHQ VHDUFKHV IRU UHOH YDQW GRFXPHQWV IURP D WDUJHW FRUSXV 7DUJHW 1 ( V LQ UHWULHYHG GRFXPHQWV DUH FRPSDUHG ZLWK WKH VRXUFH 1 ( EDVHG RQ WKHLU SKRQHWLF DQG FRQWH [ WXDO VHPDQWLF VLPL ODULWLHV DQG WKH EHVW PDWFKHG RQH LV VHOHFWHG DV WKH FRU UHFW WUDQVODWLRQ ( [ SHULPHQWV VKRZ WKDW WKLV DSSURDFK DFKLHYHV RQ WUDQVODWLRQ DFFXUDF\ DQG FRQVLVWHQWO\ LPSURYHV WKH WUDQVODWLRQ TXDOLW\ RQ</definiens>
			</definition>
</paper>

		<paper id="2010">
			<definition id="0">
				<sentence>However , introducing zeros in the inverse covariance can be seen as deleting arcs in an Undirected Graphical Model ( UGM ) where each node represents each dimension of a single Gaussian ( Bilmes , 2000 ) .</sentence>
				<definiendum id="0">node</definiendum>
				<definiens id="0">deleting arcs in an Undirected Graphical Model ( UGM ) where each</definiens>
			</definition>
			<definition id="1">
				<sentence>Suppose that we have a mixture of M Gaussians : p ( x ) = MX m p ( z = m ) N ( x ; m ; m ) ( 1 ) It is known from linear algebra that any square matrix A can be decomposed as A = LDU , where L is a lower triangular matrix , D is a diagonal matrix and U is an upper triangular matrix .</sentence>
				<definiendum id="0">L</definiendum>
				<definiendum id="1">D</definiendum>
				<definiendum id="2">U</definiendum>
				<definiens id="0">a diagonal matrix and</definiens>
			</definition>
			<definition id="2">
				<sentence>In the special case where A is also symmetric and positive definite the decomposition becomes A = UTDU where U is an upper triangular matrix with ones in the main diagonal .</sentence>
				<definiendum id="0">U</definiendum>
				<definiens id="0">an upper triangular matrix with ones in the main diagonal</definiens>
			</definition>
			<definition id="3">
				<sentence>MI attempts to avoid the overfitting problem by zeroing regression coefficients between least marginally dependent feature elements .</sentence>
				<definiendum id="0">MI</definiendum>
				<definiens id="0">attempts to avoid the overfitting problem by zeroing regression coefficients between least marginally dependent feature elements</definiens>
			</definition>
			<definition id="4">
				<sentence>DMIimp attempts to discriminate against impostors , MIimp attempts to build a speaker-independent structure which will be more robustly estimated since there are more data to estimate the mutual informations and DMIconf attempts to discriminate against the most confusable target speaker .</sentence>
				<definiendum id="0">DMIimp</definiendum>
				<definiendum id="1">MIimp</definiendum>
				<definiens id="0">attempts to discriminate against impostors</definiens>
				<definiens id="1">estimate the mutual informations and DMIconf attempts to discriminate against the most confusable target speaker</definiens>
			</definition>
			<definition id="5">
				<sentence>Structural EM is an algorithm that generalizes on the EM algorithm by searching in the combined space of structure and parameters .</sentence>
				<definiendum id="0">Structural EM</definiendum>
				<definiens id="0">an algorithm that generalizes on the EM algorithm by searching in the combined space of structure and parameters</definiens>
			</definition>
			<definition id="6">
				<sentence>EER is defined as the point where the probability of false alarms is equal to the probability of missed detections .</sentence>
				<definiendum id="0">EER</definiendum>
				<definiens id="0">the point where the probability of false alarms is equal to the probability of missed detections</definiens>
			</definition>
</paper>

		<paper id="3009">
			<definition id="0">
				<sentence>Gemini : A natural language system for spoken lan guage understanding .</sentence>
				<definiendum id="0">Gemini</definiendum>
				<definiens id="0">A natural language system for spoken lan guage understanding</definiens>
			</definition>
</paper>

		<paper id="4011">
			<definition id="0">
				<sentence>In each entry form ‚Äúa ( b ) , c ( d ) ‚Äù , ‚Äúa‚Äù indicates the number of inputs in which the referring expressions were correctly recognized by the speech recognizer ; ‚Äúb‚Äù indicates the number of inputs in which the referring expressions were correctly recognized and were correctly resolved ; ‚Äúc‚Äù indicates the number of inputs in which the referring expressions were not correctly recognized ; ‚Äúd‚Äù indicates the number of inputs in which the referring expressions were not correctly recognized , but were correctly resolved .</sentence>
				<definiendum id="0">‚Äúb‚Äù</definiendum>
			</definition>
</paper>

		<paper id="2003">
			<definition id="0">
				<sentence>NER identifies different kinds of names such as ‚Äùperson‚Äù , ‚Äùlocation‚Äù or ‚Äùdate‚Äù , while WSD distinguishes the senses of ambiguous words .</sentence>
				<definiendum id="0">WSD</definiendum>
				<definiens id="0">distinguishes the senses of ambiguous words</definiens>
			</definition>
			<definition id="1">
				<sentence>The new constraint for the h-th feature function can be rewritten as : 1 N +ŒªM Nsummationdisplay i=1 summationdisplay l Pt+1 ( l|xi ) kh ( l , xi ) ( 3 ) + ŒªN +ŒªM N+Msummationdisplay i=N+1 summationdisplay l Pt+1 ( l|xi ) kh ( l , xi ) = 1N +ŒªM Nsummationdisplay i=1 kh ( li , xi ) + ŒªN +ŒªM N+Msummationdisplay i=N+1 summationdisplay l Pt ( l|xi ) kh ( l , xi ) where t is the index of the bootstrap iterations .</sentence>
				<definiendum id="0">t</definiendum>
			</definition>
			<definition id="2">
				<sentence>N is the total number of instances .</sentence>
				<definiendum id="0">N</definiendum>
			</definition>
			<definition id="3">
				<sentence>support ( x , l ) denotes how frequently indicator x and label l occur together .</sentence>
				<definiendum id="0">l )</definiendum>
				<definiens id="0">denotes how frequently indicator x and label l occur together</definiens>
			</definition>
			<definition id="4">
				<sentence>Non-statistical knowledge is the knowlege which is not obtainable from statistics in a sparse data set , but rather from other resources like a dictionary or WordNet .</sentence>
				<definiendum id="0">Non-statistical knowledge</definiendum>
				<definiens id="0">the knowlege which is not obtainable from statistics in a sparse data set , but rather from other resources like a dictionary or WordNet</definiens>
			</definition>
</paper>

		<paper id="4031">
			<definition id="0">
				<sentence>The diameter a9 of the Web ( that is , the average number of links from any given page to another ) has been found to be a constant ( approximately a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a33a32a35a34a23a26a29a28a31a30a19a36 , where a32 is the number of documents on the Web and a36 is the average document out-degree ( i.e. , the number of pages linked from the document ) .</sentence>
				<definiendum id="0">a32</definiendum>
			</definition>
			<definition id="1">
				<sentence>Menczer ( Menczer , 2001 ) introduces the link-content conjecture states that the semantic content of a web page can be inferred from the pages that point to it .</sentence>
				<definiendum id="0">Menczer</definiendum>
				<definiens id="0">introduces the link-content conjecture states that the semantic content of a web</definiens>
			</definition>
			<definition id="2">
				<sentence>Since a cluster of documents contains a subset of an entire language , a document model is a special case of a language model .</sentence>
				<definiendum id="0">document model</definiendum>
				<definiens id="0">a special case of a language model</definiens>
			</definition>
			<definition id="3">
				<sentence>We compute IDF using the formula a147a105a9a148a175a56a43a29a6a91a48a135a24a176a72a68a177a174a178a39a179 a61 a43a75a15a91a72a136a74a54a180a174a181a39a76a123a182a146a88a66a183a132a184a11a180a17a48 , where a9a105a175a56a43a78a6a91a48 is the document frequency ( fraction of all documents containing a6 ) and a144a185a9 is the number of documents in the collection .</sentence>
				<definiendum id="0">a9a105a175a56a43a78a6a91a48</definiendum>
				<definiendum id="1">a144a185a9</definiendum>
				<definiens id="0">the document frequency ( fraction of all documents containing a6 ) and</definiens>
				<definiens id="1">the number of documents in the collection</definiens>
			</definition>
			<definition id="4">
				<sentence>The X axis represents the prior probability a58 while the Y axis corresponds to the posterior probability a58a62a196 .</sentence>
				<definiendum id="0">X axis</definiendum>
				<definiens id="0">represents the prior probability a58 while the Y axis corresponds to the posterior probability a58a62a196</definiens>
			</definition>
</paper>

		<paper id="1043">
</paper>

		<paper id="4003">
			<definition id="0">
				<sentence>Conventional statistical machine translation ( SMT ) approaches might not be able to find a good translation due to problems in its statistical models ( due to data sparseness during the estimation of the model parameters ) as well as search errors during the decoding process .</sentence>
				<definiendum id="0">Conventional statistical machine translation</definiendum>
				<definiendum id="1">SMT</definiendum>
				<definiens id="0">due to data sparseness during the estimation of the model parameters ) as well as search errors during the decoding process</definiens>
			</definition>
			<definition id="1">
				<sentence>The statistical machine translation framework ( SMT ) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability : TM LM = argmaxT p ( SjT ) p ( T ) , ( 1 ) where p ( SjT ) is called a translation model ( TM ) , representing the generation probability from T into S , p ( T ) is called a language model ( LM ) and represents the likelihood of the target language ( Brown et al. , 1993 ) .</sentence>
				<definiendum id="0">SMT</definiendum>
				<definiendum id="1">p ( SjT</definiendum>
				<definiendum id="2">TM</definiendum>
				<definiens id="0">formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability : TM LM = argmaxT p ( SjT ) p ( T )</definiens>
				<definiens id="1">called a language model ( LM ) and represents the likelihood of the target language</definiens>
			</definition>
			<definition id="2">
				<sentence>Translation examples consist of pairs of pre-translated sentences , either by humans ( high quality ) or automatically using MT systems ( reduced quality ) .</sentence>
				<definiendum id="0">Translation examples</definiendum>
				<definiens id="0">consist of pairs of pre-translated sentences , either by humans ( high quality ) or automatically using MT systems ( reduced quality )</definiens>
			</definition>
			<definition id="3">
				<sentence>The distance is defined as the sum of the costs of insertion ( INS ) , deletion ( DEL ) , and substitution ( SUB ) operations required to map one word sequence into the other .</sentence>
				<definiendum id="0">substitution ( SUB</definiendum>
				<definiens id="0">the sum of the costs of insertion ( INS ) , deletion ( DEL ) , and</definiens>
			</definition>
			<definition id="4">
				<sentence>TM LM EDW ( d ) = TM LM ( d ) exp ( scale ED ( s d ; d ) ) ( 2 ) The second rescoring function assigns a probability to each decoder output that combines the exponential of the sum of log probabilities of TM and LM and the scaled negative ED scores of all translation candidates TC as follows .</sentence>
				<definiendum id="0">TM LM EDW</definiendum>
			</definition>
			<definition id="5">
				<sentence>The Basic Travel Expression Corpus ( BTEC ) contains 157K sentence pairs and the average lengths in words of Japanese and English sentences are 7.7 and 5.5 , respectively .</sentence>
				<definiendum id="0">Basic Travel Expression Corpus ( BTEC</definiendum>
				<definiens id="0">contains 157K sentence pairs and the average lengths in words of Japanese and English sentences are 7.7 and 5.5 , respectively</definiens>
			</definition>
			<definition id="6">
				<sentence>Word Error Rate ( WER ) , which penalizes the edit distance against reference translations ( Su et al. , 1992 ) BLEU : the geometric mean of n-gram precision for the translation results found in reference translations ( Papineni et al. , 2002 ) Translation Accuracy ( ACC ) : subjective evaluation ranks ranging from A to D ( A : perfect , B : fair , C : acceptable and D : nonsense ) , judged blindly by a native speaker ( Sumita et al. , 1999 ) In contrast to WER , higher BLEU and ACC scores indicate better translations .</sentence>
				<definiendum id="0">B</definiendum>
				<definiendum id="1">ACC scores</definiendum>
				<definiens id="0">the geometric mean of n-gram precision for the translation results found in reference translations ( Papineni et al. , 2002 ) Translation Accuracy ( ACC ) : subjective evaluation ranks ranging from A to D ( A : perfect ,</definiens>
			</definition>
			<definition id="7">
				<sentence>Three of them were in-house EBMT systems which differ in the translation unit ( sentence-based vs. phrase-based ) .</sentence>
				<definiendum id="0">EBMT</definiendum>
				<definiens id="0">systems which differ in the translation unit ( sentence-based vs. phrase-based )</definiens>
			</definition>
</paper>

		<paper id="2008">
			<definition id="0">
				<sentence>The Greek CHILDES corpus ( Stephany , 1995 ) is a database of conversations between children and caretakers , broadly transcribed , currently with no notations for lexical stress .</sentence>
				<definiendum id="0">Greek CHILDES corpus</definiendum>
				<definiens id="0">a database of conversations between children and caretakers , broadly transcribed</definiens>
			</definition>
			<definition id="1">
				<sentence>The test corpus consists of utterances by adults to the same child as in the training corpus .</sentence>
				<definiendum id="0">test corpus</definiendum>
				<definiens id="0">consists of utterances by adults to the same child as in the training corpus</definiens>
			</definition>
</paper>

		<paper id="1034">
</paper>

		<paper id="4022">
			<definition id="0">
				<sentence>For a target word x and each context word y , we calculate p ( x ) = c ( x ) /n and p ( y ) =c ( y ) /n , where c ( x ) and c ( y ) are the total corpus counts for x and y and where n is the total number of words in the training corpus ( 1,638,224 ) .</sentence>
				<definiendum id="0">n</definiendum>
				<definiens id="0">p ( x ) = c ( x ) /n and p ( y ) =c ( y ) /n , where c ( x ) and c ( y ) are the total corpus counts for x and y and where</definiens>
				<definiens id="1">the total number of words in the training corpus ( 1,638,224 )</definiens>
			</definition>
</paper>

		<paper id="1008">
			<definition id="0">
				<sentence>After a search engine uses the formulated query to return the N most relevant documents from the Web , an answer to the given question is found by computing an answer language model probability ( indicating how similar the proposed answer is to answers seen in the training corpus ) , and an answer/question translation model probability ( indicating how similar the proposed answer/question pair is to pairs seen in the training corpus ) .</sentence>
				<definiendum id="0">answer language model probability</definiendum>
				<definiens id="0">to answers seen in the training corpus</definiens>
				<definiens id="1">to pairs seen in the training corpus )</definiens>
			</definition>
			<definition id="1">
				<sentence>We express this probability using the following formula : ) ) | ( 1 1 ) ) | ( ) | ( ( 1 ( ) | ( ) | ( 11 NULLqt n aacaqt n n nmaqp j ii n i j m j + + +‚ãÖ + = ‚àë‚àè == œà ( 3 ) where t ( q j | a i ) are the probabilities of ‚Äútranslating‚Äù answer terms into question terms , and c ( a i |a ) are the relative counts of the answer terms .</sentence>
				<definiendum id="0">c</definiendum>
				<definiens id="0">the relative counts of the answer terms</definiens>
			</definition>
			<definition id="2">
				<sentence>The ONG-AE algorithm gives an accurate estimate of the performance ceiling induced by the set of potential answers available to the AnswerExtraction Module .</sentence>
				<definiendum id="0">ONG-AE algorithm</definiendum>
				<definiens id="0">gives an accurate estimate of the performance ceiling induced by the set of potential answers available to the AnswerExtraction Module</definiens>
			</definition>
			<definition id="3">
				<sentence>A : The Ameripass includes all of USA excluding Alaska .</sentence>
				<definiendum id="0">Ameripass</definiendum>
				<definiens id="0">includes all of USA excluding Alaska</definiens>
			</definition>
</paper>

		<paper id="4016">
			<definition id="0">
				<sentence>The CAMMIA system is a client-server dialog management system based on VoiceXML .</sentence>
				<definiendum id="0">CAMMIA system</definiendum>
				<definiens id="0">a client-server dialog management system based on VoiceXML</definiens>
			</definition>
</paper>

		<paper id="1028">
			<definition id="0">
				<sentence>The Port Authority of Allegheny County , which manages the buses in Pittsburgh provided the full database of bus routes and schedules .</sentence>
				<definiendum id="0">Port Authority of Allegheny County</definiendum>
				<definiens id="0">manages the buses in Pittsburgh provided the full database of bus routes and schedules</definiens>
			</definition>
</paper>

		<paper id="4021">
			<definition id="0">
				<sentence>The modeling of p ( AjS1 ; : : : ; SM ) ( where M is the number of features ) is a significant problem in its own right .</sentence>
				<definiendum id="0">M</definiendum>
				<definiens id="0">the number of features</definiens>
			</definition>
</paper>

		<paper id="4008">
			<definition id="0">
				<sentence>Among these , the Berkeley FrameNet database is a semantic lexical resource consisting of frame-semantic descriptions of more than 7000 English lexical items , together with example sentences annotated with semantic roles ( Baker et al. , 1998 ) .</sentence>
				<definiendum id="0">Berkeley FrameNet database</definiendum>
			</definition>
			<definition id="1">
				<sentence>FrameNet is a collection of lexical entries grouped by frame semantics .</sentence>
				<definiendum id="0">FrameNet</definiendum>
				<definiens id="0">a collection of lexical entries grouped by frame semantics</definiens>
			</definition>
			<definition id="2">
				<sentence>Each lexical entry represents an individual word sense , and is associated with semantic roles and some annotated sentences .</sentence>
				<definiendum id="0">lexical entry</definiendum>
				<definiens id="0">represents an individual word sense , and is associated with semantic roles and some annotated sentences</definiens>
			</definition>
			<definition id="3">
				<sentence>lingual FrameNet ( Dorr et al. 2002 ) uses a manual seed mapping of semantic roles between FrameNet and LVD .</sentence>
				<definiendum id="0">lingual FrameNet</definiendum>
				<definiens id="0">uses a manual seed mapping of semantic roles between FrameNet and LVD</definiens>
			</definition>
			<definition id="4">
				<sentence>Œ± is an adjusting parameter , which controls the curvature of the similarity score .</sentence>
				<definiendum id="0">Œ±</definiendum>
				<definiens id="0">controls the curvature of the similarity score</definiens>
			</definition>
			<definition id="5">
				<sentence>Lexical entries test set lexical entry Precision best/baseline Recall best/baseline F-measure best/baseline beat .</sentence>
				<definiendum id="0">Lexical entries test</definiendum>
				<definiens id="0">set lexical entry Precision best/baseline Recall best/baseline F-measure best/baseline beat</definiens>
			</definition>
</paper>

		<paper id="4006">
			<definition id="0">
				<sentence>The training set consists of N weighted word lattices produced by the baseline recognizer , and a gold-standard transcription for each of the N lattices .</sentence>
				<definiendum id="0">training set</definiendum>
				<definiens id="0">consists of N weighted word lattices produced by the baseline recognizer</definiens>
			</definition>
			<definition id="1">
				<sentence>Let GL be the gold-standard transcription for L. Let Œ¶ ( pi ) be the K-dimensional feature vector for pi , which contains the count within the path pi of each feature .</sentence>
				<definiendum id="0">pi</definiendum>
				<definiendum id="1">pi</definiendum>
			</definition>
</paper>

		<paper id="4030">
			<definition id="0">
				<sentence>For a collection of articles from the journal Machine Learning , the top level cluster is labeled learn , paper , base , model , new , train and the second level clusters are labeled process , experi , knowledge , develop , inform , design and algorithm , function , present , result , problem , model .</sentence>
				<definiendum id="0">top level cluster</definiendum>
				<definiens id="0">labeled process , experi , knowledge , develop , inform , design and algorithm , function , present , result , problem , model</definiens>
			</definition>
			<definition id="1">
				<sentence>The Word Space algorithm ( Schutze , 1993 ) uses linear regression on term co-occurrence statistics to create groups of semantically related words .</sentence>
				<definiendum id="0">Word Space algorithm</definiendum>
				<definiens id="0">uses linear regression on term co-occurrence statistics to create groups of semantically related words</definiens>
			</definition>
			<definition id="2">
				<sentence>A vector is defined as the sum of all fourgrams in a window of 1001 fourgrams centered around the word .</sentence>
				<definiendum id="0">vector</definiendum>
				<definiens id="0">the sum of all fourgrams in a window of 1001 fourgrams centered around the word</definiens>
			</definition>
			<definition id="3">
				<sentence>The unsupervised method ( document clustering ) imposes an initial organization on a personal information collection which the user can then modify .</sentence>
				<definiendum id="0">document clustering )</definiendum>
				<definiens id="0">imposes an initial organization on a personal information collection which the user can then modify</definiens>
			</definition>
			<definition id="4">
				<sentence>WordNet is a manually built lexical system where words are organized into synonym sets ( synsets ) linked by different relations ( Fellbaum , 1998 ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
			</definition>
			<definition id="5">
				<sentence>Figure 2 shows partial results obtained using the WordNet algorithm ( where compression reduced the number of nodes by a43 a14a45a44 ) and Word Space ( Schutze , 1993 ) .</sentence>
				<definiendum id="0">WordNet algorithm</definiendum>
			</definition>
</paper>

		<paper id="4035">
</paper>

		<paper id="4007">
			<definition id="0">
				<sentence>A key component of this program is the Interactive Book , which combines real-time speech recognition , facial animation , and natural language understanding capabilities to teach children to read and comprehend text .</sentence>
				<definiendum id="0">Interactive Book</definiendum>
				<definiens id="0">combines real-time speech recognition</definiens>
			</definition>
			<definition id="1">
				<sentence>The reading tracking module determines the current reading location by aligning each partial hypothesis with the book text using a Dynamic Programming search .</sentence>
				<definiendum id="0">reading tracking module</definiendum>
				<definiens id="0">determines the current reading location by aligning each partial hypothesis with the book text using a Dynamic Programming search</definiens>
			</definition>
			<definition id="2">
				<sentence>Text normalization consists primarily of punctuation removal and determination of sentence-like units .</sentence>
				<definiendum id="0">Text normalization</definiendum>
				<definiens id="0">consists primarily of punctuation removal and determination of sentence-like units</definiens>
			</definition>
			<definition id="3">
				<sentence>Word Error Rate ( % ) Experimental Configuration MFCC PMVDR ( A ) Baseline : single n-gram language model 17.7 % 17.4 % ( B ) ( A ) + Begin/End Sentence Context Modeling 14.0 % 13.5 % ( C ) ( B ) + between utterance word history modeling 13.0 % 12.7 % ( D ) ( C ) + dynamic n-gram language model 11.0 % 10.7 % ( E ) ( D ) + VTLN 10.9 % 10.6 % ( F ) ( E ) + VTLN/SAT + SMAPLR ( iteration 1 ) 8.2 % 8.2 % ( G ) ( E ) + VTLN/SAT + SMAPLR ( iteration 2 ) 8.0 % 8.0 % Table 1 : Recognition of children‚Äôs read out-loud data .</sentence>
				<definiendum id="0">Experimental Configuration MFCC PMVDR</definiendum>
				<definiendum id="1">B )</definiendum>
				<definiens id="0">% ( C ) ( B ) + between utterance word history modeling 13.0 % 12.7 % ( D ) ( C ) + dynamic n-gram language model 11.0 % 10.7 % ( E ) ( D ) + VTLN 10.9 % 10.6 % ( F ) ( E ) + VTLN/SAT + SMAPLR ( iteration 1 ) 8.2 % 8.2 % ( G ) ( E ) + VTLN/SAT + SMAPLR ( iteration 2</definiens>
			</definition>
			<definition id="4">
				<sentence>‚ÄúMultilingual Human-Computer Interactions : From Information Acess to Language Learning , ‚Äù ICSLP-96 , Philadelphia , PA J. Mostow , S. Roth , A. G. Hauptmann , and M. Kane ( 1994 ) .</sentence>
				<definiendum id="0">‚ÄúMultilingual Human-Computer Interactions</definiendum>
				<definiens id="0">From Information Acess to Language Learning , ‚Äù ICSLP-96 , Philadelphia , PA J. Mostow</definiens>
			</definition>
</paper>

		<paper id="4001">
</paper>

		<paper id="1020">
			<definition id="0">
				<sentence>Next , in the training phase , we estimate the probabilities Pa0 aa10 M a11ia12 a7 t j a2 and Pa0 aa10 S a11ia12 a7 t j a2 by simply counting the occurrence of the features aa10 M a11ia12 and aa10 S a11ia12 with marker t. For features with zero counts , we use add-k smoothing ( Johnson , 1932 ) , where k is a small number less than one .</sentence>
				<definiendum id="0">k</definiendum>
				<definiens id="0">a small number less than one</definiens>
			</definition>
			<definition id="1">
				<sentence>By selecting features that represent , albeit indirectly and imperfectly , ( S1 ( S ( NP ( DT The ) ( NN company ) ) ( VP ( VBD said ) ( S ( NP ( NNS employees ) ) ( VP ( MD will ) ( VP ( VB lose ) ( NP ( PRP their ) ( NNS jobs ) ) ( SBAR-TMP ( IN after ) ( S ( NP ( DT the ) ( NN sale ) ) ( VP ( AUX is ) ( VP ( VBN completed ) ) ) ) ) ) ) ) ) ) ) Figure 1 : Extraction of main and subordinate clause from parse tree these knowledge sources , we aim to empirically assess their contribution to the temporal inference task .</sentence>
				<definiendum id="0">AUX</definiendum>
				<definiens id="0">Extraction of main and subordinate clause from parse tree these knowledge sources</definiens>
			</definition>
			<definition id="2">
				<sentence>In Table 2 we show the relative frequencies in our corpus for finiteness ( FIN ) , past tense ( PAST ) , active voice ( ACT ) , and negation ( NEG ) for main and subordinate clauses conjoined with the markers once and since .</sentence>
				<definiendum id="0">NEG</definiendum>
				<definiens id="0">past tense ( PAST ) , active voice ( ACT ) , and negation (</definiens>
			</definition>
			<definition id="3">
				<sentence>Position ( P ) This feature simply records the position of the two clauses in the parse tree , i.e. , whether the subordinate clause precedes or follows the main clause .</sentence>
				<definiendum id="0">Position</definiendum>
				<definiens id="0">the position of the two clauses in the parse tree , i.e. , whether the subordinate clause precedes or follows the main clause</definiens>
			</definition>
			<definition id="4">
				<sentence>feature combinations where k is the arity of the combination ( unary , binary , ternary , etc. ) .</sentence>
				<definiendum id="0">k</definiendum>
			</definition>
			<definition id="5">
				<sentence>An important finding in Machine Learning is that a set of classifiers whose individual deInterpretation Fusion TMark Feat Acc Feat Acc after NPRSTV 69.9 AVWV 77.9 as ANNWPSV 57.0 AV 75.8 before SV 42.1 ANSTV 85.4 once PRS 40.7 RT 100 since PRST 25.1 T 85.2 when VLPS 85.5 RST 86.9 while PST 49.0 VWS 79.4 until VLVWRT 69.4 TV 90.5 Table 4 : Best feature combinations for individual markers ( development set ) Interpretation Fusion E SV E ARSTV TMark Prec Rec Prec Rec Prec Prec after 61.5 66.5 51.6 55.2 96.7 75.2 as 61.5 62.6 57.0 52.8 93.2 70.5 before 50.0 51.5 32.0 39.1 96.8 84.1 once 60.0 25.0 12.7 15.0 100 88.3 since 69.4 26.3 25.4 12.0 98.2 81.0 when 83.0 91.1 84.7 85.0 99.3 83.8 while 71.5 28.9 38.0 25.8 97.7 82.8 until 57.8 52.4 38.5 47.7 97.8 87.8 Acc 70.7 62.6 97.3 80.1 Baseline 42.6 42.6 42.6 42.6 50.0 50.0 Table 5 : Results on interpreation and fusion ( test set ) cisions are combined in some way ( an ensemble ) can be more accurate than any of its component classifiers if the errors of the individual classifiers are sufficiently uncorrelated ( Dietterich , 1997 ) .</sentence>
				<definiendum id="0">Interpretation Fusion E SV E ARSTV TMark Prec Rec Prec Rec</definiendum>
			</definition>
			<definition id="6">
				<sentence>A decision tree ( Quinlan , 1993 ) was used for selecting the models with the least overlap and for combining their output .</sentence>
				<definiendum id="0">decision tree</definiendum>
				<definiens id="0">selecting the models with the least overlap and for combining their output</definiens>
			</definition>
			<definition id="7">
				<sentence>The ensemble ( consisting of the 20 selected models ) attained an accuracy of 97.4 % on the test .</sentence>
				<definiendum id="0">ensemble</definiendum>
				<definiens id="0">consisting of the 20 selected models ) attained an accuracy of 97.4 % on the test</definiens>
			</definition>
			<definition id="8">
				<sentence>100 volunteers participated in the fusion study , again all native speakers of English .</sentence>
				<definiendum id="0">volunteers</definiendum>
				<definiens id="0">participated in the fusion study , again all native speakers of English</definiens>
			</definition>
</paper>

		<paper id="4033">
			<definition id="0">
				<sentence>a man turnstileleft s‚ó¶/c ( np\cs‚ó¶ ) Alice turnstileleft np saw turnstileleft ( np\s‚ó¶ ) /np Axiomnp turnstileleft np ‚Äôs mother turnstileleft np\np \Enp ‚ó¶ ‚Äôs mother turnstileleft np /Esaw ‚ó¶ ( np ‚ó¶ ‚Äôs mother ) turnstileleft np\s‚ó¶ \EAlice ‚ó¶ ( saw ‚ó¶ ( np ‚ó¶ ‚Äôs mother ) ) turnstileleft s‚ó¶ DiamondIDiamondparenleftbigAlice ‚ó¶ ( saw ‚ó¶ ( np ‚ó¶ ‚Äôs mother ) ) parenrightbig turnstileleft Diamonds‚ó¶ UnquoteDiamondparenleftbigAlice ‚ó¶ ( saw ‚ó¶ ( np ‚ó¶ ‚Äôs mother ) ) parenrightbig turnstileleft s‚ó¶ Kprime thriceDiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( Diamondnp ‚ó¶Diamond‚Äôs mother ) ) turnstileleft s‚ó¶ TDiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( np ‚ó¶Diamond‚Äôs mother ) ) turnstileleft s‚ó¶ RootparenleftbigDiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( np ‚ó¶Diamond‚Äôs mother ) ) parenrightbig ‚ó¶ c 1 turnstileleft s‚ó¶ Rightparenleftbig Diamondsaw ‚ó¶ ( np ‚ó¶Diamond‚Äôs mother ) parenrightbig ‚ó¶c ( 1 ‚ó¶DiamondAlice ) turnstileleft s‚ó¶ Right ( np ‚ó¶Diamond‚Äôs mother ) ‚ó¶c parenleftbig ( 1 ‚ó¶DiamondAlice ) ‚ó¶Diamondsawparenrightbig turnstileleft s‚ó¶ Left np ‚ó¶c parenleftbigDiamond‚Äôs mother ‚ó¶ ( ( 1 ‚ó¶DiamondAlice ) ‚ó¶Diamondsaw ) parenrightbig turnstileleft s‚ó¶ \ cIDiamond‚Äôs mother ‚ó¶ ( ( 1 ‚ó¶DiamondAlice ) ‚ó¶Diamondsaw ) turnstileleft np\ cs‚ó¶ / cEa man ‚ó¶ parenleftbigDiamond‚Äôs mother ‚ó¶ ( ( 1 ‚ó¶DiamondAlice ) ‚ó¶Diamondsaw ) parenrightbig turnstileleft s‚ó¶ Left ( a man ‚ó¶Diamond‚Äôs mother ) ‚ó¶ c parenleftbig ( 1 ‚ó¶DiamondAlice ) ‚ó¶Diamondsawparenrightbig turnstileleft s‚ó¶ RightparenleftbigDiamondsaw ‚ó¶ ( a man ‚ó¶Diamond‚Äôs mother ) parenrightbig ‚ó¶ c ( 1 ‚ó¶DiamondAlice ) turnstileleft s‚ó¶ Rightparenleftbig DiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( a man ‚ó¶Diamond‚Äôs mother ) ) parenrightbig ‚ó¶c 1 turnstileleft s‚ó¶ Root DiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( a man ‚ó¶Diamond‚Äôs mother ) ) turnstileleft s‚ó¶ T thrice Alice ‚ó¶ ( saw ‚ó¶ ( a man ‚ó¶ ‚Äôs mother ) ) turnstileleft s‚ó¶ Figure 3 : In-situ quantification : deriving Alice saw a man‚Äôs mother Quantifier Type a man s‚ó¶/c ( np\cs‚ó¶ ) nobody s‚ó¶/c ( np\cs‚àí ) anybody s‚àí/c ( np\cs‚àí ) somebody s+/c ( np\cs+ ) everybody s‚ó¶/c ( np\cs+ ) d71d70d69d68d64d65d66d67s+ Œµ d31d31d63d63 d63d63d63 d63d63d63 d63d63 somebody d31d31d47d47 d71d70d69d68d64d65d66d67 s‚àí Œµ d127d127 d127d127 d127d127d127 d127d127d127 d127 anybody d31d31 d79d78d77d76d72d73d74d75d71d70d69d68d64d65d66d67s‚ó¶ nobody d78d78 a man d95d95 everybodyd110d110 d47d47 Figure 4 : Quantifier type assignments , and a corresponding finitestate machine The pmodemediatespolaritysensitivity .</sentence>
				<definiendum id="0">man ‚ó¶Diamond‚Äôs mother</definiendum>
				<definiens id="0">‚Äôs mother ) ) turnstileleft s‚ó¶ DiamondIDiamondparenleftbigAlice ‚ó¶ ( saw ‚ó¶ ( np ‚ó¶ ‚Äôs mother ) ) parenrightbig turnstileleft Diamonds‚ó¶ UnquoteDiamondparenleftbigAlice ‚ó¶ ( saw ‚ó¶ ( np ‚ó¶ ‚Äôs mother ) ) parenrightbig turnstileleft s‚ó¶ Kprime thriceDiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( Diamondnp ‚ó¶Diamond‚Äôs mother ) ) turnstileleft s‚ó¶ TDiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( np ‚ó¶Diamond‚Äôs mother ) ) turnstileleft s‚ó¶ RootparenleftbigDiamondAlice ‚ó¶ ( Diamondsaw ‚ó¶ ( np ‚ó¶Diamond‚Äôs mother ) ) parenrightbig ‚ó¶ c 1 turnstileleft s‚ó¶ Rightparenleftbig Diamondsaw ‚ó¶ ( np ‚ó¶Diamond‚Äôs mother</definiens>
				<definiens id="1">a man ‚ó¶Diamond‚Äôs mother ) ) turnstileleft s‚ó¶ T thrice Alice ‚ó¶ ( saw ‚ó¶ ( a man ‚ó¶ ‚Äôs mother</definiens>
				<definiens id="2">Quantifier type assignments</definiens>
			</definition>
			<definition id="1">
				<sentence>Second , thanks to the unary modes in the structural nobody turnstileleft s‚ó¶/c ( np\cs‚àí ) anybody turnstileleft s‚àí/c ( np\cs‚àí ) ¬∑¬∑ ¬∑ Diamondnp ‚ó¶ ( Diamondsaw ‚ó¶ np ) turnstileleft s‚ó¶ Root , Right , Right np ‚ó¶c parenleftbig ( 1 ‚ó¶Diamondnp ) ‚ó¶Diamondsawparenrightbig turnstileleft s‚ó¶ Diamond pIDiamond p parenleftbignp ‚ó¶ c ( ( 1 ‚ó¶Diamondnp ) ‚ó¶Diamondsaw ) parenrightbig turnstileleft Diamond ps‚ó¶ square‚Üì pInp ‚ó¶ c parenleftbig ( 1 ‚ó¶Diamondnp ) ‚ó¶Diamondsawparenrightbig turnstileleft s‚àí \cI ( 1 ‚ó¶Diamondnp ) ‚ó¶Diamondsaw turnstileleft np\ cs‚àí / cEanybody ‚ó¶ c parenleftbig ( 1 ‚ó¶Diamondnp ) ‚ó¶Diamondsawparenrightbig turnstileleft s‚àí Right , Right , LeftDiamondnp ‚ó¶ c parenleftbig ( Diamondsaw ‚ó¶ anybody ) ‚ó¶ 1parenrightbig turnstileleft s‚àí T twicenp ‚ó¶ c parenleftbig ( saw ‚ó¶ anybody ) ‚ó¶ 1parenrightbig turnstileleft s‚àí \cI ( saw ‚ó¶ anybody ) ‚ó¶ 1 turnstileleft np\ cs‚àí / cEnobody ‚ó¶ c parenleftbig ( saw ‚ó¶ anybody ) ‚ó¶ 1parenrightbig turnstileleft s‚ó¶ Left , Rootnobody ‚ó¶ ( saw ‚ó¶ anybody ) turnstileleft s‚ó¶ Figure 5 : Polarity licensing : deriving Nobody saw anybody ¬∑¬∑ ¬∑ anybody ‚ó¶ ( saw ‚ó¶ np ) turnstileleft s‚àí DiamondI Diamondparenleftbiganybody ‚ó¶ ( saw ‚ó¶ np ) parenrightbig turnstileleft Diamonds‚àí¬∑ ¬∑¬∑ ? ? ?</sentence>
				<definiendum id="0">LeftDiamondnp ‚ó¶ c parenleftbig</definiendum>
				<definiendum id="1">turnstileleft s‚àí DiamondI Diamondparenleftbiganybody ‚ó¶ ( saw ‚ó¶ np</definiendum>
				<definiens id="0">the unary modes in the structural nobody turnstileleft s‚ó¶/c ( np\cs‚àí ) anybody turnstileleft s‚àí/c ( np\cs‚àí ) ¬∑¬∑ ¬∑ Diamondnp ‚ó¶ ( Diamondsaw ‚ó¶ np</definiens>
			</definition>
</paper>

		<paper id="3008">
			<definition id="0">
				<sentence>SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach .</sentence>
				<definiendum id="0">SenseClusters</definiendum>
				<definiens id="0">a freely available word sense discrimination system that takes a purely unsupervised clustering approach</definiens>
			</definition>
			<definition id="1">
				<sentence>SenseClusters creates clusters made up of the contexts in which a given target word occurs .</sentence>
				<definiendum id="0">SenseClusters</definiendum>
			</definition>
			<definition id="2">
				<sentence>SenseClusters allows for the selection of lexical features either from a held out corpus of training data , or from the same data that is to be clustered , which we refer to as the test data .</sentence>
				<definiendum id="0">SenseClusters</definiendum>
				<definiens id="0">a held out corpus of training data , or from the same data that is to be clustered</definiens>
			</definition>
			<definition id="3">
				<sentence>To combat these problems , SenseClusters follows the lead of LSI ( Deerwester et al. , 1990 ) and LSA ( Landauer et al. , 1998 ) and allows for the conversion of word level feature spaces into a concept level semantic space by carrying out dimensionality reduction with Singular Value Decomposition ( SVD ) .</sentence>
				<definiendum id="0">SenseClusters</definiendum>
			</definition>
			<definition id="4">
				<sentence>LSA Support SenseClusters provides all of the functionality needed to carry out Latent Semantic Analysis .</sentence>
				<definiendum id="0">LSA Support SenseClusters</definiendum>
			</definition>
			<definition id="5">
				<sentence>LSA converts a word level feature space into a concept level semantic space that smoothes over differences due to polysemy and synonymy among words .</sentence>
				<definiendum id="0">LSA</definiendum>
				<definiens id="0">converts a word level feature space into a concept level semantic space that smoothes over differences due to polysemy and synonymy among words</definiens>
			</definition>
			<definition id="6">
				<sentence>Availability SenseClusters is an open source software project that is freely distributed under the GNU Public License ( GPL ) via http : //senseclusters.sourceforge.net/ SenseClusters is an ongoing project , and there are already a number of published papers based on its use ( e.g. , ( Purandare , 2003 ) , ( Purandare and Pedersen , 2004 ) ) .</sentence>
				<definiendum id="0">Availability SenseClusters</definiendum>
				<definiendum id="1">SenseClusters</definiendum>
				<definiens id="0">an open source software project that is freely distributed under the GNU Public License ( GPL ) via http</definiens>
			</definition>
</paper>

		<paper id="4024">
			<definition id="0">
				<sentence>This query produces a retrieval status value ( RSV ) for each document that is a monotonically increasing function of the probability of relevance , in accord with the probability ranking principle ( Robertson , 1977 ) .</sentence>
				<definiendum id="0">RSV</definiendum>
				<definiens id="0">a monotonically increasing function of the probability of relevance , in accord with the probability ranking principle</definiens>
			</definition>
			<definition id="1">
				<sentence>If the document corpus and set of terms is held xed , the average precision calculation can be considered a function f : &lt; N ! [ 0 ; 1 ] mapping to a single average precision value. Finding the weight vectors in this 5 10 15 20 number of terms average precision Figure 1 : Average precision by query size as generated by the maximum entropy model for TREC topic 307. context is then the familiar problem of nding maxima in an N-dimensional landscape. One general approach to this problem of searching a multi-dimensional space is to decompose the problem into a series of iterated searches along single directions within the space. Perhaps the most basic technique , credited to Powell , is simply a round-robin-style iteration along a set of unchanging direction vectors , until convergence is reached ( Press et al. , 1992 , pp. 412-420 ) . This is the approach used in this study. Formally , the procedure is as follows. You are given a set of direction vectors ! 1 ; ! 2 ; ; ! N and a starting point 0. First move 0 to the maximum along ! 1 and call this 1 , i.e. 1 = 0 + 1 ! 1 for some scalar 1. Next move 1 to the maximum along ! 2 and call this 2 , and so on , until the nal point N . Finally , replace 0 with N and repeat the entire process , starting again with ! 1. Do this until some convergence criterion is met. This procedure has no guaranteed rate of convergence , although more sophisticated versions of Powell‚Äôs algorithm do. In practice this has not been a problem. Powell‚Äôs algorithm can make use of any one-dimensional search technique. Rather than applying a completely general hill-climbing search , however , in the case where document scores are calculated by a linear equation on the terms , i.e. j = NX i=1 i ji = j as they are in the tf-idf formula , we can exhaustively search in a single direction of the weight space in an ef cient manner. This potentially yields better solutions and potentially converges more quickly than a general hillclimbing heuristic. scale document score a a b b e c c f1 f2 f f d d Figure 2 : Sample plot of versus for a given direction. The insight behind the algorithm is as follows. Given a direction ! in weight space and a starting point , the score of each document is a linear function of the scale along ! from : j = j = ( + ! ) j = j + ( ! j ) : i.e. document di‚Äôs score , plotted against , is a line with slope ! i and y-intercept j. Consider the graph of lines for all documents , such as the example in Figure 2. Each vertical slice of the graph , at some point on the x axis , represents the order of the documents when = ; speci cally , the order of the documents is given by the order of the intersections of the lines with the vertical line at x = . Now consider the set of intersections of the document lines. Given two documents dr and ds , their intersection , if it exists , lies at point rs = ( xrs ; yrs ) where xrs = ( s r ) ! ( r s ) ; and yrs = r + xrs ( ! r ) ( Note that this is unde ned if ! r = ! s , i.e. , if the document lines are parallel. ) Let be the set of all such document intersection points for a given direction , document set and term vector. Note that more than two lines may intersect at the same point , and that two intersections may share the same x component while having different y components. Now consider the set x , de ned as the projection of onto the x axis , i.e. x = f j 9 2 s.t. x = g. The points in x represent precisely those values of where two or more documents are tied in score. Therefore , the document ordering changes at and only at these points of intersection ; in other words , the points in x partition the range of into at most M ( M 1 ) =2+1 regions , where M is the total number of documents. Within a given region , document ordering is invariant and hence average precision is constant. As we can calculate the boundaries of , and the document ordering and average precision within , each region , we now have a way of nding the maximum across the entire space by evaluating a nite number of regions. Each of the O ( M 2 ) regions requires an O ( M log M ) sort , yielding a total computational bound of O ( M 3 log M ) . In fact , we can further reduce the computation by exploiting the fact that the change in document ordering between any two regions is known and is typically small. The weight search algorithm functions in this manner. It sorts the documents completely to determine the ordering in the left-most region. Then , it traverses the regions from left to right and updates the document ordering in each , which does not require a sort. Average precision can be incrementally updated based on the document ordering changes. This reduces the computational bound to O ( M2 log M ) , the requirement for the initial sort of the O ( M2 ) intersection points. In order to compare the results of the weight search algorithm to those of the maximum entropy model , we employed the same experiment setup. We ran on 15 topics , which were manually selected from the TREC 6 , 7 , and 8 collections ( Voorhees and Harman , 2000 ) , with the objective of creating a representative subset. The document sets were divided into randomly selected training , validation and test splits , comprising 25 % , 25 % , and 50 % , respectively , of the complete set. For each query , a set of candidate terms was selected based on mutual information between ( binary ) term occurrence and document relevance. From this set , terms were chosen individually to be included in the query , and coef cients for all terms were calculated using LBFGS , a quasi-Newton unconstrained optimization algorithm ( Zhu et al. , 1994 ) . For experimenting with the weight search algorithm , we investigated queries of length 1 through 20 for each topic , so each topic involved 20 experiments. The rst term weight was xed at 1.0. The single-term queries did not require a weight search , as the weight of a single term does not affect the average precision score. For the remaining 19 experiments for each topic , the direction vectors ! were chosen such that the algorithm searched a single term weight at a time. For example , a query with 5 10 15 20 number of terms average precision average precision average precision average precision average precision average precision average precision average precision average precision average precision average precision average precision average precision average precision average precision 301 302 307 330 332 347 352 375 383 384 388 391 407 425 439 Figure 3 : Average precision versus query size for the weight search algorithm. Each line represents a topic. i terms used the i 1 directions ! i ; 1 = h0 1 0 0 0i ; ! i ; 2 = h0 0 1 0 0i ; ... ! i ; i 1 = h0 0 0 0 1i : The two-term query for a topic started the search from the point 2 ; 0 = h1 0i , and each successive experiment for that topic was initialized with the starting point 0 equal to the nal point in the previous iteration , concatenated with a 0. The value vectors j used in all experiments were Okapi tf scores. The average precision scores obtained by the maximum entropy and weight search algorithm experiments are listed in Table 1. The Best AP and No. Terms columns describe the query size at which average precision was best and the score at that point. These columns show that the maximum entropy approach performs just as well as the average precision hill-climber , and in some cases actually performs slightly better. This suggests that the metric divergence as seen in Figure 1 did not prohibit the maximum entropy approach from maximizing average precision in the course of maximizing likelihood. The 5 term AP column compares the performance of the algorithms on smaller queries. The weight search algorithm shows a slight advantage over the maximum entropy model on 10 of the 15 topics and equal performance on the others , but de nitive conclusions are dif cult at this stage. Figure 3 shows the average precision achieved by the weight search algorithm , for all 20 query sizes and for all 15 topics. Unlike the maximum entropy results , the algorithm is guaranteed to yield monotonically nondecreasing scores. Topic 5 term AP Best AP No. Terms WS ME WS ME WS ME 301 0.68 0.67 0.90 0.90 &gt; 20 &gt; 20 302 0.88 0.86 1.00 1.00 10 10 307 0.57 0.56 0.98 0.89 &gt; 20 &gt; 20 330 0.65 0.61 1.00 1.00 10 10 332 0.74 0.72 0.99 1.00 &gt; 20 18 347 0.78 0.78 1.00 1.00 17 14 352 0.55 0.55 0.94 0.93 &gt; 20 &gt; 20 375 0.92 0.92 1.00 1.00 9 9 383 0.89 0.89 1.00 1.00 9 9 384 0.77 0.73 1.00 1.00 8 8 388 0.82 0.80 1.00 1.00 7 7 391 0.64 0.63 0.99 0.98 &gt; 20 &gt; 20 407 0.83 0.83 1.00 1.00 9 9 425 0.75 0.73 1.00 1.00 12 12 439 0.53 0.51 1.00 1.00 17 16 Table 1 : Average precision achieved for weight search ( WS ) and maximum entropy ( ME ) algorithms .</sentence>
				<definiendum id="0">M</definiendum>
				<definiendum id="1">document ordering</definiendum>
				<definiendum id="2">maximum entropy</definiendum>
				<definiens id="0">in the case where document scores are calculated by a linear equation on the terms</definiens>
				<definiens id="1">the set of all such document intersection points for a given direction , document set and term vector. Note that more than two lines may intersect at the same point , and that two intersections may share the same x component while having different y components. Now consider the set x , de ned as the projection of onto the x axis</definiens>
				<definiens id="2">The weight search algorithm functions in this manner. It sorts the documents completely to determine the ordering in the left-most region. Then , it traverses the regions from left to right and updates the document ordering in each , which does not require a sort. Average precision can be incrementally updated based on the document ordering changes. This reduces the computational bound</definiens>
				<definiens id="3">with the objective of creating a representative subset. The document sets were divided into randomly selected training , validation and test splits</definiens>
			</definition>
</paper>

		<paper id="1004">
			<definition id="0">
				<sentence>Formally , each gesture , reference , and binding are defined as g =htgs ; tge ; Gtypei r =htrs ; tre ; wi b =hg ; ri ( 1 ) wherets , te describe the start and ending time of a gesture or reference , w2S is the word corresponding to r , and Gtype is the type of gesture ( e.g. deictic or trajectory ) .</sentence>
				<definiendum id="0">w2S</definiendum>
				<definiendum id="1">Gtype</definiendum>
				<definiens id="0">g =htgs ; tge ; Gtypei r =htrs ; tre ; wi b =hg ; ri ( 1 ) wherets , te describe the start and ending time of a gesture or reference</definiens>
				<definiens id="1">the word corresponding to r , and</definiens>
			</definition>
</paper>

		<paper id="1029">
			<definition id="0">
				<sentence>The sentence selection algorithm consists of the following steps : We take N top-ranked documents , retrieved in response to query terms from the topic title .</sentence>
				<definiendum id="0">sentence selection algorithm</definiendum>
				<definiens id="0">consists of the following steps : We take N top-ranked documents , retrieved in response to query terms from the topic title</definiens>
			</definition>
			<definition id="1">
				<sentence>Sentence selection score 1 ( S1 ) is the sum of idf of all query terms present in the sentence .</sentence>
				<definiendum id="0">S1</definiendum>
				<definiens id="0">the sum of idf of all query terms present in the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>UWAThard2 is a final run using query expansion method 1 , outlined earlier , plus the granularity and known relevant documents metadata .</sentence>
				<definiendum id="0">UWAThard2</definiendum>
				<definiens id="0">a final run using query expansion method 1 , outlined earlier , plus the granularity and known relevant documents metadata</definiens>
			</definition>
			<definition id="3">
				<sentence>The fact that the query expansion method 1 ( UWAThard2 ) produced no improvement over the baseline ( UWAThard1 ) was a surprise , and did not correspond to our training runs with the Financial Times and Los Angeles Times collections , which showed 21 % improvement over the original title-only query run .</sentence>
				<definiendum id="0">UWAThard2</definiendum>
				<definiens id="0">produced no improvement over the baseline</definiens>
			</definition>
</paper>

		<paper id="1002">
			<definition id="0">
				<sentence>Precision is the proportion of mentions in HC that are also in TC and recall is the proportion of mentions in TC that are also in HC .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiendum id="1">recall</definiendum>
				<definiens id="0">the proportion of mentions in HC that are also in TC</definiens>
				<definiens id="1">the proportion of mentions in TC that are also in HC</definiens>
			</definition>
			<definition id="1">
				<sentence>Terms are weighted by a tf-idf weight as tf¬∑log ( N/df ) , where tf is the number of times that a term occurs in the summary , N is the total number of documents in the collection , and df is the number of documents that contain the term .</sentence>
				<definiendum id="0">tf</definiendum>
				<definiendum id="1">N</definiendum>
				<definiendum id="2">df</definiendum>
				<definiens id="0">the number of times that a term occurs in the summary ,</definiens>
				<definiens id="1">the total number of documents in the collection</definiens>
			</definition>
			<definition id="2">
				<sentence>The KL divergence is a classic measure of the ‚Äúdistance‚Äù between two probability distributions .</sentence>
				<definiendum id="0">KL divergence</definiendum>
				<definiens id="0">a classic measure of the ‚Äúdistance‚Äù between two probability distributions</definiens>
			</definition>
</paper>

		<paper id="3003">
			<definition id="0">
				<sentence>When a successful parse is obtained , Gemini creates a quasi-logical form representing the meaning of the sentence .</sentence>
				<definiendum id="0">Gemini</definiendum>
				<definiens id="0">creates a quasi-logical form representing the meaning of the sentence</definiens>
			</definition>
</paper>

		<paper id="1003">
			<definition id="0">
				<sentence>E denotes the collection of all possible entities in the world and Ed = fedigld1 is the set of entities mentioned in document d. M denotes the collection of all possible mentions and Md = fmdignd1 is the set of mentions in document d. Mdi ( 1 ‚Ä¢ i ‚Ä¢ ld ) is the set of mentions that refer to entity edi 2 Ed .</sentence>
				<definiendum id="0">E</definiendum>
				<definiens id="0">the collection of all possible entities in the world and Ed = fedigld1 is the set of entities mentioned in document d. M denotes the collection of all possible mentions and Md = fmdignd1 is the set of mentions in document d. Mdi</definiens>
			</definition>
			<definition id="1">
				<sentence>Assuming conditional independency between Md and Ed given Rd , the probability distribution over documents is therefore P ( d ) = P ( Ed ; Rd ; Md ) = P ( d ) P ( RdjEd ) P ( MdjRd ) ; and the probability of the document collection D is : P ( D ) = Y d2D P ( d ) : Given a mention m in a document d ( Md is the set of observed mentions in d ) , the key inference problem is to determine the most likely entity e‚ÅÑm that corresponds to it .</sentence>
				<definiendum id="0">Md</definiendum>
				<definiens id="0">the probability distribution over documents is therefore P ( d ) = P ( Ed ; Rd ; Md ) = P ( d ) P ( RdjEd ) P ( MdjRd ) ; and the probability of the document collection D is : P ( D ) = Y d2D P ( d )</definiens>
				<definiens id="1">the set of observed mentions in d ) , the key inference problem is to determine the most likely entity e‚ÅÑm that corresponds to it</definiens>
			</definition>
			<definition id="2">
				<sentence>Entity Identity : Given two mentions m1 2 d1 ; m2 2 d2 , determine whether they correspond to the same entity by : m1 ¬ª m2 ( ) argmaxe2EP ( e ; m1 ) = argmaxe2EP ( e ; m2 ) for Model I and m1 ¬ª m2 ( ) argmaxe2EP ( Ed1 ; Rd1 ; Md1 ) = argmaxe2EP ( Ed2 ; Rd2 ; Md2 ) : for Model II and III .</sentence>
				<definiendum id="0">Entity Identity</definiendum>
				<definiendum id="1">m2</definiendum>
				<definiens id="0">determine whether they correspond to the same entity by : m1 ¬ª m2 ( ) argmaxe2EP ( e ; m1 ) = argmaxe2EP ( e ;</definiens>
			</definition>
			<definition id="3">
				<sentence>Given a set of labelled entitymention pairs f ( ei ; mi ) gn1 , P ( e ) = freq ( e ) n where freq ( e ) denotes the number of pairs containing entity e. ‚Ä† Given all the entities appearing in D , the transitive probability P ( eje ) is estimated by P ( e2je1 ) ¬ª P ( wrt ( e2 ) jwrt ( e1 ) ) = doc # ( wrt ( e2 ) ; wrt ( e1 ) ) doc # ( wrt ( e1 ) ) : Here , the conditional probability between two realworld entities P ( e2je1 ) is backed off to the one between the identifying writings of the two entities P ( wrt ( e2 ) jwrt ( e1 ) ) in the document set D to avoid 3Note that the performance of the initialization algorithm is 97:3 % precision and 10:1 % recall ( measures are defined later . )</sentence>
				<definiendum id="0">freq ( e )</definiendum>
				<definiendum id="1">transitive probability P ( eje</definiendum>
				<definiens id="0">the number of pairs containing entity e. ‚Ä† Given all the entities appearing in D</definiens>
				<definiens id="1">estimated by P ( e2je1 ) ¬ª P ( wrt ( e2 ) jwrt ( e1 ) ) = doc # ( wrt ( e2 ) ; wrt ( e1 ) ) doc # ( wrt ( e1 ) ) : Here , the conditional probability between two realworld entities P ( e2je1 ) is backed off to the one between the identifying writings of the two entities P ( wrt ( e2 ) jwrt ( e1 )</definiens>
			</definition>
			<definition id="4">
				<sentence>5copy denotes v0 k is exactly the same as vk ; missing denotes ‚Äúmissing value‚Äù for v0k ; typical denotes v0k is a typical variation of vk , for example , ‚ÄúProf.‚Äù for ‚ÄúProfessor‚Äù , ‚ÄúAndy‚Äù for ‚ÄúAndrew‚Äù ; non-typical denotes a non-typical transformation .</sentence>
				<definiendum id="0">non-typical</definiendum>
				<definiens id="0">a typical variation of vk , for example</definiens>
				<definiens id="1">a non-typical transformation</definiens>
			</definition>
			<definition id="5">
				<sentence>We measure Precision ( P ) ‚Äì Percentage of correctly predicted pairs , Recall ( R ) ‚Äì Percentage of correct pairs that were predicted , and F1 = 2PRP+R. Comparisons : The appearance model induces a ‚Äúsimilarity‚Äù measure between names , which is estimated during the training process .</sentence>
				<definiendum id="0">Recall</definiendum>
				<definiens id="0">is estimated during the training process</definiens>
			</definition>
</paper>

		<paper id="4032">
			<definition id="0">
				<sentence>The data consists of 816 hand-transcribed conversation sides ( 566K words ) , of which we reserve 128 conversation sides ( 61K words ) for evaluation testing according to the 1993 NIST evaluation choices .</sentence>
				<definiendum id="0">data</definiendum>
				<definiens id="0">consists of 816 hand-transcribed conversation sides ( 566K words ) , of which we reserve 128 conversation sides ( 61K words ) for evaluation testing according to the 1993 NIST evaluation choices</definiens>
			</definition>
</paper>

		<paper id="1026">
			<definition id="0">
				<sentence>STUDENTa0 : The engine ( EMOTION = POSITIVE ) TUTORa2 : Uh well engine is part of the car , so how can it exert force on itself ?</sentence>
				<definiendum id="0">STUDENTa0</definiendum>
				<definiens id="0">The engine ( EMOTION = POSITIVE ) TUTORa2 : Uh well engine is part of the car</definiens>
			</definition>
			<definition id="1">
				<sentence>P ( A ) is the proportion of times the annotators agree , and P ( E ) is the proportion of agreement expected by chance .</sentence>
				<definiendum id="0">P</definiendum>
				<definiens id="0">the proportion of times the annotators agree , and P ( E ) is the proportion of agreement expected by chance</definiens>
			</definition>
</paper>

		<paper id="4027">
			<definition id="0">
				<sentence>The centroid vector is the average of the TF-IDF vectors of all the sentences in t. The IDF component is derived from the ACM Corpus .</sentence>
				<definiendum id="0">centroid vector</definiendum>
				<definiens id="0">the average of the TF-IDF vectors of all the sentences in t. The IDF component is derived from the ACM Corpus</definiens>
			</definition>
</paper>

		<paper id="4036">
</paper>

		<paper id="2007">
			<definition id="0">
				<sentence>For the purposes of this paper , a token can be defined as the smallest discrete unit of meaning in a document relevant to the task at hand , the smallest entity of information that can not be further reduced in form and still carry that information .</sentence>
				<definiendum id="0">token</definiendum>
				<definiens id="0">the smallest discrete unit of meaning in a document relevant to the task at hand , the smallest entity of information that can not be further reduced in form and still carry that information</definiens>
			</definition>
			<definition id="1">
				<sentence>Normalization acts as a special case of tokenization by deciding which instances of punctuation break a token , and removing all punctuation that does not break the token in order to bring it to a normalized form .</sentence>
				<definiendum id="0">Normalization</definiendum>
				<definiens id="0">a special case of tokenization by deciding which instances of punctuation break a token , and removing all punctuation that does not break the token in order to bring it to a normalized form</definiens>
			</definition>
			<definition id="2">
				<sentence>Casting tokenization , and by extension normalization , as a classification problem motivates the creation of BAccHANT , a machine learning system designed to normalize bioscience text .</sentence>
				<definiendum id="0">extension normalization</definiendum>
				<definiens id="0">a machine learning system designed to normalize bioscience text</definiens>
			</definition>
			<definition id="3">
				<sentence>Values for TL/TR are as follows : * lower : Character is lowercase * cap : Character is a capital letter * num : Character is a number * space : Character is whitespace ( space , tab , etc. ) * other : Character is none of the above Values for Class are as follows : * remove : The punctuation should be removed * break : The punctuation should break the token The 'remove ' class is of chief importance for the normalization task , since classifying a piece of punctuation as 'remove ' means the punctuation will be removed for normalization .</sentence>
				<definiendum id="0">Character</definiendum>
				<definiendum id="1">Character</definiendum>
				<definiens id="0">a number * space</definiens>
			</definition>
			<definition id="4">
				<sentence>This achieves an accuracy of 92.73 % , where accuracy is the percentage of correctly classified punctuation .</sentence>
				<definiendum id="0">accuracy</definiendum>
				<definiens id="0">the percentage of correctly classified punctuation</definiens>
			</definition>
			<definition id="5">
				<sentence>The statistically significant degradation in performance of BAccHANT inside NEs vs. performance both inside and outside NEs indicates that data inside named entities is more difficult to normalize than data outside named entities .</sentence>
				<definiendum id="0">NEs</definiendum>
				<definiens id="0">indicates that data inside named entities is more difficult to normalize than data outside named entities</definiens>
			</definition>
</paper>

		<paper id="1011">
			<definition id="0">
				<sentence>( F-score is a standard measure of parse accuracy , see e.g. , Manning and Schcurrency1utze ( 1999 ) for details ) .</sentence>
				<definiendum id="0">F-score</definiendum>
				<definiens id="0">a standard measure of parse accuracy</definiens>
			</definition>
			<definition id="1">
				<sentence>The Switchboard Corpus is a corpus of telephone conversations between adult speakers of varying dialects .</sentence>
				<definiendum id="0">Switchboard Corpus</definiendum>
				<definiens id="0">a corpus of telephone conversations between adult speakers of varying dialects</definiens>
			</definition>
			<definition id="2">
				<sentence>While Ferrer ( 2002 ) should be consulted for full details , PAU DUR N is pause duration normalized by the speaker‚Äôs mean sentence-internal pause duration , NORM LAST RHYME DUR is the duration of the phone minus the mean phone duration normalized by the standard deviation of the phone duration for each phone in the rhyme , FOK WRD DIFF MNMN NG is the log of the mean f0 of the current word , divided by the log mean f0 of the following word , normalized by the speakers mean range , FOK LR MEAN KBASELN is the log of the mean f0 of the word normalized by speaker‚Äôs baseline , and SLOPE MEAN DIFF N is the difference in the f0 slope normalized by the speaker‚Äôs mean f0 slope .</sentence>
				<definiendum id="0">NORM LAST RHYME DUR</definiendum>
				<definiendum id="1">FOK WRD DIFF MNMN NG</definiendum>
				<definiendum id="2">FOK LR MEAN KBASELN</definiendum>
				<definiendum id="3">SLOPE MEAN DIFF N</definiendum>
				<definiens id="0">the duration of the phone minus the mean phone duration normalized by the standard deviation of the phone duration for each phone in the rhyme</definiens>
				<definiens id="1">the log of the mean f0 of the current word , divided by the log mean f0 of the following word , normalized by the speakers mean range ,</definiens>
				<definiens id="2">the log of the mean f0 of the word normalized by speaker‚Äôs baseline , and</definiens>
			</definition>
			<definition id="3">
				<sentence>In general , a pseudo-punctuation symbol is the conjunction of the binned values of all of the prosodic features used in a particular run .</sentence>
				<definiendum id="0">pseudo-punctuation symbol</definiendum>
				<definiens id="0">the conjunction of the binned values of all of the prosodic features used in a particular run</definiens>
			</definition>
</paper>

		<paper id="4017">
</paper>

		<paper id="4013">
			<definition id="0">
				<sentence>Formally , if Q is the set of queries to our search engine and D is the set of indexed documents , let R be a binary relation on Q D where qRd if and only if d is in the return set for the query q. It is likely that the set of related queries is quite large for a given q ( in practice the size is on the order of ten thousand ; for our dataset , y shing has 29 ; 698 related queries ) .</sentence>
				<definiendum id="0">D</definiendum>
				<definiens id="0">the set of indexed documents</definiens>
			</definition>
			<definition id="1">
				<sentence>In vanilla MSN search , we can ascribe a cost of 1 to reading each URL in the list : having a relevant URL as the 8th position results in a cost of 8 .</sentence>
				<definiendum id="0">relevant URL</definiendum>
				<definiens id="0">having a</definiens>
			</definition>
			<definition id="2">
				<sentence>In the table , MSN is vanilla MSN search and QDSE is the system described in this paper .</sentence>
				<definiendum id="0">QDSE</definiendum>
				<definiens id="0">the system described in this paper</definiens>
			</definition>
			<definition id="3">
				<sentence>Query : Soldering iron URL : www.siliconsolar.com/accessories.htm Intent : looking for accessories for soldering irons ( but not soldering irons themselves ) Query : Whole Foods URL : www.wholefoodsmarket.com/company/communitygiving.html Intent : looking for the Whole Foods Market‚Äôs community giving policy Query : nal fantasy URL : www.playonline.com/ff11/home/ Intent : looking for a webforum for nal fantasy games Query : online computer course URL : www.microsoft.com/traincert/ Intent : looking for information on Microsoft Certi ed Technical Education centers Table 1 : Four random hquery ; URL ; intenti triples MSN QDSE Prob .</sentence>
				<definiendum id="0">Query</definiendum>
				<definiendum id="1">www.siliconsolar.com/accessories.htm Intent</definiendum>
				<definiendum id="2">www.playonline.com/ff11/home/ Intent</definiendum>
				<definiens id="0">online computer course URL : www.microsoft.com/traincert/ Intent : looking for information on Microsoft Certi ed Technical</definiens>
			</definition>
</paper>

		<paper id="1025">
			<definition id="0">
				<sentence>A unigram language model is defined by a list of types ( words ) and their individual probabilities .</sentence>
				<definiendum id="0">unigram language model</definiendum>
				<definiens id="0">a list of types ( words ) and their individual probabilities</definiens>
			</definition>
			<definition id="1">
				<sentence>The probability of T given model G i is therefore : where C ( w ) is the count of the type w in T. Our goal is to find the most likely grade language model given the text T , or equivalently , the model G i that maximizes .</sentence>
				<definiendum id="0">C ( w )</definiendum>
				<definiens id="0">the text T , or equivalently , the model G i that maximizes</definiens>
			</definition>
			<definition id="2">
				<sentence>We use a technique called Simple Good-Turing smoothing , which is a popular method for natural language applications .</sentence>
				<definiendum id="0">Good-Turing smoothing</definiendum>
				<definiens id="0">a popular method for natural language applications</definiens>
			</definition>
			<definition id="3">
				<sentence>In this study , we used an estimate which is a function of type length : where w is a type , i is a grade index between 1 and 12 , |w| is w‚Äôs length in characters , and C = -13 , D = 10 based on statistics from the Web corpus .</sentence>
				<definiendum id="0">w</definiendum>
				<definiens id="0">w‚Äôs length in characters</definiens>
			</definition>
			<definition id="4">
				<sentence>Feature selection is an important step in text classification : it can lessen the computational burden by reducing the number of features and increase accuracy by removing ‚Äònoise‚Äô words having low predictive power .</sentence>
				<definiendum id="0">Feature selection</definiendum>
				<definiens id="0">an important step in text classification : it can lessen the computational burden by reducing the number of features</definiens>
			</definition>
			<definition id="5">
				<sentence>The Reading A-Z files were converted from PDF files using optical character recognition ; spelling errors were corrected but sentence boundary errors were left intact to simulate the kinds of problems encountered with Web documents .</sentence>
				<definiendum id="0">Reading A-Z</definiendum>
			</definition>
			<definition id="6">
				<sentence>We also included a fourth predictor : the FleschKincaid score ( Kincaid et al. 1975 ) , which is a linear combination of the text‚Äôs average sentence length ( in tokens ) , and the average number of syllables per token .</sentence>
				<definiendum id="0">FleschKincaid score</definiendum>
				<definiens id="0">a linear combination of the text‚Äôs average sentence length ( in tokens ) , and the average number of syllables per token</definiens>
			</definition>
			<definition id="7">
				<sentence>However , Reading A-Z documents were written to pre-established criteria which includes objective factors such as type/ token ratio ( Reading A-Z .</sentence>
				<definiendum id="0">Reading A-Z</definiendum>
			</definition>
</paper>

		<paper id="1006">
</paper>

		<paper id="1032">
			<definition id="0">
				<sentence>For each ( non-aux/non-copula ) verb in each sentence , our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label .</sentence>
				<definiendum id="0">classifier</definiendum>
				<definiens id="0">examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label</definiens>
			</definition>
			<definition id="1">
				<sentence>VV '' indicates that the constituent is an `` NP '' which is the subject of the predicate verb .</sentence>
				<definiendum id="0">VV</definiendum>
				<definiens id="0">the subject of the predicate verb</definiens>
			</definition>
</paper>

		<paper id="4040">
			<definition id="0">
				<sentence>One of the major goals of the DARPA program for Effective , Affordable , Reusable Speech-to-Text ( EARS ) ( Wayne , 2003 ) is to provide a rich transcription of speech recognition output , including speaker identification , sentence boundary detection and the annotation of disfluencies in the transcript ( This collection of additional features is also known as Metadata ) .</sentence>
				<definiendum id="0">Affordable , Reusable Speech-to-Text</definiendum>
				<definiens id="0">to provide a rich transcription of speech recognition output , including speaker identification , sentence boundary detection and the annotation of disfluencies in the transcript</definiens>
			</definition>
			<definition id="1">
				<sentence>TBL is a technique for learning a set of rules that transform an initial hypothesis for the purpose of reducing the error rate of the hypothesis .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">a technique for learning a set of rules that transform an initial hypothesis for the purpose of reducing the error rate of the hypothesis</definiens>
			</definition>
			<definition id="2">
				<sentence>The rules learned by the system are conditioned on several features of each of the words including the lexeme ( the word itself ) , a POS tag for the word , whether the word is followed by a silence and whether the word is a high frequency word .</sentence>
				<definiendum id="0">POS tag</definiendum>
				<definiens id="0">the word itself ) , a</definiens>
			</definition>
			<definition id="3">
				<sentence>2We use a POS tagger ( Ratnaparkhi , 1996 ) trained on switchboard data with the additional tags of FP ( filled pause ) and FRAG ( word fragment ) .</sentence>
				<definiendum id="0">FRAG</definiendum>
				<definiens id="0">data with the additional tags of FP ( filled pause</definiens>
			</definition>
			<definition id="4">
				<sentence>The error rate is the number of disfluency errors ( insertions and deletions ) divided by the number of disfluent tokens in the reference transcript .</sentence>
				<definiendum id="0">error rate</definiendum>
				<definiens id="0">the number of disfluency errors ( insertions and deletions</definiens>
			</definition>
</paper>

		<paper id="4038">
			<definition id="0">
				<sentence>Base Phrase ( BP ) Chunking is the process of creating non-recursive base phrases such as noun phrases , adjectival phrases , verb phrases , preposition phrases , etc .</sentence>
				<definiendum id="0">Base Phrase ( BP ) Chunking</definiendum>
				<definiens id="0">the process of creating non-recursive base phrases such as noun phrases , adjectival phrases , verb phrases , preposition phrases , etc</definiens>
			</definition>
			<definition id="1">
				<sentence>Arabic is a Semitic language with rich templatic morphology .</sentence>
				<definiendum id="0">Arabic</definiendum>
				<definiens id="0">a Semitic language with rich templatic morphology</definiens>
			</definition>
			<definition id="2">
				<sentence>The current standard approach to Arabic tokenization and POS tagging ‚Äî adopted in theArabic TreeBank‚Äî relies on manually choosing the appropriate analysis from among the multiple analyses rendered by AraMorph , a sophisticated rule based morphological analyzer by Buckwalter.3 Morphological analysis may be characterized as the process of segmenting a surface word form into its component derivational and inflectional morphemes .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">the process of segmenting a surface word form into its component derivational and inflectional morphemes</definiens>
			</definition>
			<definition id="3">
				<sentence>APT is a two-step hybrid system with rules and a Viterbi algorithm for statistically determining the appropriate POS tag .</sentence>
				<definiendum id="0">APT</definiendum>
				<definiens id="0">a two-step hybrid system with rules and a Viterbi algorithm for statistically determining the appropriate POS tag</definiens>
			</definition>
			<definition id="4">
				<sentence>Context : A fixed-size window of -5/+5 characters centered at the character in focus .</sentence>
				<definiendum id="0">Context</definiendum>
				<definiens id="0">A fixed-size window of -5/+5 characters centered at the character in focus</definiens>
			</definition>
			<definition id="5">
				<sentence>Tag Set : The tag set is a36 B-PRE1 , B-PRE2 , B-WRD , IWRD , B-SUFF , I-SUFFa37 where I denotes inside a segment , B denotes beginning of a segment , PRE1 and PRE2 are proclitic tags , SUFF is an enclitic , and WRD is the stem plus any affixes and/or the determiner Al .</sentence>
				<definiendum id="0">SUFF</definiendum>
				<definiendum id="1">WRD</definiendum>
				<definiens id="0">The tag set is a36 B-PRE1 , B-PRE2 , B-WRD , IWRD , B-SUFF , I-SUFFa37 where I denotes inside a segment</definiens>
				<definiens id="1">the stem plus any affixes and/or the determiner Al</definiens>
			</definition>
			<definition id="6">
				<sentence>Input : A sequence of tokens processed from left-to-right .</sentence>
				<definiendum id="0">Input</definiendum>
				<definiens id="0">A sequence of tokens processed from left-to-right</definiens>
			</definition>
			<definition id="7">
				<sentence>Context : A window of -2/+2 tokens centered at the focus token .</sentence>
				<definiendum id="0">Context</definiendum>
				<definiens id="0">A window of -2/+2 tokens centered at the focus token</definiens>
			</definition>
			<definition id="8">
				<sentence>Features : Every character a0 -gram , a0 a1a2a1 that occurs in the focus token , the 5 tokens themselves , their ‚Äòtype‚Äô from the set a36 alpha , numerica37 , and POS tag decisions for previous tokens within context .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">Every character a0 -gram</definiens>
			</definition>
			<definition id="9">
				<sentence>ChunkLink flattens the tree to a sequence of base ( non-recursive ) phrase chunks with their IOB labels .</sentence>
				<definiendum id="0">ChunkLink</definiendum>
			</definition>
			<definition id="10">
				<sentence>Context : A window of -2/+2 tokens centered at the focus token .</sentence>
				<definiendum id="0">Context</definiendum>
				<definiens id="0">A window of -2/+2 tokens centered at the focus token</definiens>
			</definition>
			<definition id="11">
				<sentence>We use a standard SVM with a polynomial kernel , of degree 2 and C=1.7 Standard metrics of Accuracy ( Acc ) , Precision ( Prec ) , Recall ( Rec ) , and the F-measure , a1 a2a5a4a3a6 , on the test set are utilized.8 Results : Table 2 presents the results obtained using the current SVM based approach , SVM-TOK , compared against two rule-based baseline approaches , RULE and RULE+DICT .</sentence>
				<definiendum id="0">RULE</definiendum>
				<definiens id="0">SVM-TOK , compared against two rule-based baseline approaches</definiens>
			</definition>
			<definition id="12">
				<sentence>RULE marks a prefix if a word starts with one of five proclitic letters described in Section 4.1 .</sentence>
				<definiendum id="0">RULE</definiendum>
			</definition>
</paper>

		<paper id="3012">
			<definition id="0">
				<sentence>WordNet : :Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts ( or synsets ) .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiendum id="1">:Similarity</definiendum>
				<definiens id="0">a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts ( or synsets )</definiens>
			</definition>
			<definition id="1">
				<sentence>Three of the six measures of similarity are based on the information content of the least common subsumer ( LCS ) of concepts A and B. Information content is a measure of the specificity of a concept , and the LCS of concepts A and B is the most specific concept that is an ancestor of both A and B. These measures include res ( Resnik , 1995 ) , lin ( Lin , 1998 ) , and jcn ( Jiang and Conrath , 1997 ) .</sentence>
				<definiendum id="0">B. Information content</definiendum>
				<definiendum id="1">B</definiendum>
				<definiendum id="2">lin</definiendum>
				<definiens id="0">the information content of the least common subsumer ( LCS ) of concepts A and</definiens>
				<definiens id="1">a measure of the specificity of a concept</definiens>
			</definition>
			<definition id="2">
				<sentence>The measure path is a baseline that is equal to the inverse of the shortest path between two concepts .</sentence>
				<definiendum id="0">measure path</definiendum>
				<definiens id="0">a baseline that is equal to the inverse of the shortest path between two concepts</definiens>
			</definition>
			<definition id="3">
				<sentence>pm is the super class of all modules , and provides general services used by all of the measures such as validation of synset identifier input , tracing , and caching of results .</sentence>
				<definiendum id="0">pm</definiendum>
				<definiens id="0">the super class of all modules , and provides general services used by all of the measures such as validation of synset identifier input , tracing , and caching of results</definiens>
			</definition>
			<definition id="4">
				<sentence>pm provides getAllPaths ( ) , which finds all of the paths and their lengths between two input synsets , and getShortestPath ( ) which determines the length of the shortest path between two concepts .</sentence>
				<definiendum id="0">getAllPaths ( )</definiendum>
				<definiendum id="1">getShortestPath ( )</definiendum>
				<definiens id="0">finds all of the paths and their lengths between two input synsets</definiens>
				<definiens id="1">determines the length of the shortest path between two concepts</definiens>
			</definition>
			<definition id="5">
				<sentence>pm includes the method IC ( ) , which gets the information content value of a synset .</sentence>
				<definiendum id="0">IC ( )</definiendum>
				<definiens id="0">gets the information content value of a synset</definiens>
			</definition>
			<definition id="6">
				<sentence>getLCSbyIC ( ) chooses the LCS for a pair of concepts that has the highest information content , getLCSbyDepth ( ) selects the LCS with the greatest depth , and getLCSbyPath ( ) selects the LCS that results in the shortest path .</sentence>
				<definiendum id="0">getLCSbyIC ( )</definiendum>
				<definiendum id="1">getLCSbyDepth ( )</definiendum>
				<definiendum id="2">getLCSbyPath ( )</definiendum>
				<definiens id="0">chooses the LCS for a pair of concepts that has the highest information content</definiens>
				<definiens id="1">selects the LCS with the greatest depth</definiens>
				<definiens id="2">selects the LCS that results in the shortest path</definiens>
			</definition>
</paper>

		<paper id="4009">
			<definition id="0">
				<sentence>The training corpus consists of 2773 annotated third-person pronouns from the newspaper and newswire segments of the Automatic Content Extraction ( ACE ) program training corpus .</sentence>
				<definiendum id="0">training corpus</definiendum>
				<definiens id="0">consists of 2773 annotated third-person pronouns from the newspaper and newswire segments of the Automatic Content Extraction ( ACE ) program training corpus</definiens>
			</definition>
			<definition id="1">
				<sentence>The self-trained system embeds the MaxEnt algorithm in an iterative loop during which the training examples are acquired .</sentence>
				<definiendum id="0">MaxEnt algorithm</definiendum>
				<definiens id="0">in an iterative loop during which the training examples are acquired</definiens>
			</definition>
			<definition id="2">
				<sentence>Number of Pronouns Blind Test Performance 55 71.4 % 138 72.3 % 277 72.5 % 554 72.6 % 1386 73.5 % 2773 73.4 % 5546 73.5 % Full Segment 73.7 % Table 1 : Effect of Training Data Size on Blind Test Performance analysis showed that the iterative phase contributed agradual ( althoughagainnotcompletelymonotonic ) improvement in performance during the course of learning .</sentence>
				<definiendum id="0">agradual</definiendum>
				<definiens id="0">Effect of Training Data Size on Blind Test Performance analysis showed that the iterative phase contributed</definiens>
			</definition>
</paper>

		<paper id="1001">
			<definition id="0">
				<sentence>An entity consists of all the mentions ( of any level ) which refer to one conceptual entity .</sentence>
				<definiendum id="0">entity</definiendum>
				<definiens id="0">consists of all the mentions ( of any level ) which refer to one conceptual entity</definiens>
			</definition>
			<definition id="1">
				<sentence>The mention detection system identifies the named , nominal and pronominal mentions introduced in the previous section .</sentence>
				<definiendum id="0">mention detection system</definiendum>
			</definition>
			<definition id="2">
				<sentence>Given these weights , the model computes the probability distribution of a particular examplea26 as follows : a91 a31 a5 a48a14a92a26a17a34 a1 a23 a93 a25 a94 a57a38a50 a7 a89a96a95a98a97a13a99a51a100a12a101 a48a54a57 a8 a93 a1 a62 a48 a94 a57 a89a96a95a98a97a11a99a90a100a6a101 a48a63a57 where a93 is a normalization factor .</sentence>
				<definiendum id="0">a93</definiendum>
				<definiens id="0">a normalization factor</definiens>
			</definition>
			<definition id="3">
				<sentence>Arabic , a highly inflected language , has linguistic peculiarities that affect any mention detection system .</sentence>
				<definiendum id="0">Arabic</definiendum>
				<definiens id="0">a highly inflected language , has linguistic peculiarities that affect any mention detection system</definiens>
			</definition>
			<definition id="4">
				<sentence>The following is a list of additional features ( again , a56a65a48 is the current token ) : a3 Shallow parsing information associated with the tokens in window of 3 ; a3 Prefixes/suffixes of length up to 4 ; a3 A capitalization/word-type flag ( similar to the ones described by Bikel et al. ( 1997 ) ) ; a3 Gazetteer information : a handful of location ( 55k entries ) person names ( 30k ) and organizations ( 5k ) dictionaries ; a3 A combination of gazetteer , POS and capitalization information , obtained as follows : if the word is a closed-class word ‚Äî select its class , else if it‚Äôs in a dictionary ‚Äî select that class , otherwise back-off to its capitalization information ; we call this feature gap ; a3 WordNet information ( the synsets and hypernyms of the two most frequent senses of the word ) ; a3 The outputs of three systems ( HMM , RRM and MaxEnt ) trained on a 32-category named entity data , the output of an RRM system trained on the MUC-6 data , and the output of RRM model identifying 49 categories .</sentence>
				<definiendum id="0">a56a65a48</definiendum>
				<definiendum id="1">a3 Gazetteer information</definiendum>
				<definiendum id="2">a3 WordNet information</definiendum>
				<definiens id="0">the current token ) : a3 Shallow parsing information associated with the tokens in window of 3 ; a3 Prefixes/suffixes of length up to 4</definiens>
				<definiens id="1">a handful of location ( 55k entries ) person names ( 30k ) and organizations ( 5k ) dictionaries ; a3 A combination of gazetteer</definiens>
			</definition>
			<definition id="5">
				<sentence>In this work , a binary modela91 a31a32a31 a1 a23 a92a22 a15 a8a33a0 a15 a8a33a34 a1 a27 a34 is used to compute the link probability , where a27 a27 a14a35a15 , a31 is a23 iff a0 a15 links with a23a1a25 ; the random variable a34 is the index of the partial entity to which a0 a15 is linking .</sentence>
				<definiendum id="0">a31</definiendum>
				<definiens id="0">the index of the partial entity to which a0 a15 is linking</definiens>
			</definition>
			<definition id="6">
				<sentence>This number is also quantized ; a3 editing distance ‚Äì quantized editing distance between the two mentions ; a3 mention information ‚Äì spellings of the two mentions and other information ( such as POS tags ) if available ; If a mention is a pronoun , the feature also computes gender , plurality , possessiveness and reflexiveness ; a3 acronym ‚Äì whether or not one mention is the acronym of the other mention ; a3 syntactic features ‚Äì whether or not the two mentions appear in apposition .</sentence>
				<definiendum id="0">mention</definiendum>
				<definiens id="0">editing distance ‚Äì quantized editing distance between the two mentions ; a3 mention information ‚Äì spellings of the two mentions and other information ( such as POS tags ) if available</definiens>
				<definiens id="1">a pronoun , the feature also computes gender , plurality , possessiveness and reflexiveness ; a3 acronym ‚Äì whether or not one</definiens>
				<definiens id="2">the acronym of the other mention ; a3 syntactic features ‚Äì whether or not the two mentions appear in apposition</definiens>
			</definition>
			<definition id="7">
				<sentence>To better assert the contribution of the different types of features to the final performance , we have grouped them into 4 categories : from investigating the words : words , morphs , prefix/suffix , capitalization/word-form flags techniques : POS tags , text chunks , word segmentation , etc. classifiers ( with different tag sets ) : HMM , MaxEnt and RRM output on the 32-category , 49-category and MUC data sets.9 Table 3 presents the mention detection comparative results , F-measure and ACE value , on Arabic and Chinese data .</sentence>
				<definiendum id="0">MUC</definiendum>
				<definiens id="0">from investigating the words : words , morphs , prefix/suffix , capitalization/word-form flags techniques : POS tags , text chunks , word segmentation , etc. classifiers ( with different tag sets ) : HMM , MaxEnt and RRM output on the 32-category , 49-category and</definiens>
			</definition>
</paper>

		<paper id="3010">
</paper>

		<paper id="1019">
			<definition id="0">
				<sentence>Evaluation involves comparison of a peer summary ( baseline , or produced by human or system ) by comparing its content to a gold standard , or model .</sentence>
				<definiendum id="0">Evaluation</definiendum>
				<definiens id="0">involves comparison of a peer summary ( baseline , or produced by human or system ) by comparing its content to a gold standard , or model</definiens>
			</definition>
			<definition id="1">
				<sentence>The selection of units with the same content is facilitated by the use of the Summary Evaluation Environment ( SEE ) 2 developed at ISI , which displays the model and peer summary side by side and allows the user to make selections by using a mouse .</sentence>
				<definiendum id="0">Summary Evaluation Environment</definiendum>
				<definiendum id="1">ISI</definiendum>
				<definiens id="0">displays the model</definiens>
			</definition>
			<definition id="2">
				<sentence>Each SCU has a weight corresponding to the number of summaries it appears in ; SCU1 has weight=4 and SCU2 has weight=3 3 : 3 The grammatical constituents contributing to an SCU are bracketed and coindexed with the SCU ID .</sentence>
				<definiendum id="0">SCU</definiendum>
				<definiens id="0">a weight corresponding to the number of summaries it appears in</definiens>
			</definition>
			<definition id="3">
				<sentence>An SCU consists of a set of contributors that , in their sentential contexts , express the same semantic content .</sentence>
				<definiendum id="0">SCU</definiendum>
				<definiens id="0">consists of a set of contributors that , in their sentential contexts , express the same semantic content</definiens>
			</definition>
			<definition id="4">
				<sentence>n=3 ) or use the largest pyramid ( n=9 ) , the PAL A score goes down to .46 or .52 , respectively .</sentence>
				<definiendum id="0">PAL A score</definiendum>
				<definiens id="0">goes down to .46 or .52 , respectively</definiens>
			</definition>
			<definition id="5">
				<sentence>First , an SCU is a set of contributors that are largely similar in meaning , thus SCUs differ from each other in both meaning and weight ( number of contributors ) .</sentence>
				<definiendum id="0">SCU</definiendum>
				<definiens id="0">a set of contributors that are largely similar in meaning , thus SCUs differ from each other in both meaning and weight ( number of contributors )</definiens>
			</definition>
			<definition id="6">
				<sentence>Evaluation data consists of human relevance judgments on a scale from 0 to 10 on for all sentences in the original documents .</sentence>
				<definiendum id="0">Evaluation data</definiendum>
				<definiens id="0">consists of human relevance judgments on a scale from 0 to 10 on for all sentences in the original documents</definiens>
			</definition>
			<definition id="7">
				<sentence>The reference summary consists of the sentences with highest relevance judgements that satisfy the compression constraints .</sentence>
				<definiendum id="0">reference summary</definiendum>
				<definiens id="0">consists of the sentences with highest relevance judgements that satisfy the compression constraints</definiens>
			</definition>
</paper>

		<paper id="4002">
			<definition id="0">
				<sentence>1 Introduction Text categorization is the problem of automatically assigning predefined categories to free text documents .</sentence>
				<definiendum id="0">Text categorization</definiendum>
				<definiens id="0">the problem of automatically assigning predefined categories to free text documents</definiens>
			</definition>
			<definition id="1">
				<sentence>The native feature space consists of the unique terms that occur in documents , which can be tens or hundreds of thousands of terms for even a moderatesized text collection .</sentence>
				<definiendum id="0">native feature space</definiendum>
				<definiens id="0">consists of the unique terms that occur in documents , which can be tens or hundreds of thousands of terms for even a moderatesized text collection</definiens>
			</definition>
			<definition id="2">
				<sentence>Na√Øve Bayes , which is a popular learning algorithm , is commonly justified using assumptions of conditional independence or linked dependence [ 12 ] .</sentence>
				<definiendum id="0">Na√Øve Bayes</definiendum>
			</definition>
			<definition id="3">
				<sentence>‚àí‚àí‚ãÖ = ‚àà‚àà ) , ( max ) 1 ( ) , ( max 21 \ ji SD i SRD DDSimQDSimArg MMR ji ŒªŒª where C= { D1 , ‚Ä¶ , Di , ‚Ä¶ } is a document collection ( or document stream ) ; Q is a query or user profile ; R = IR ( C , Q , Œ∏ ) , i.e. , the ranked list of documents retrieved by an IR system , given C and Q and a relevance threshold Œ∏ , below which it will not retrieve documents ( Œ∏ can be degree of match or number of documents ) ; S is the subset of documents in R which is already selected ; R\S is the set difference , i.e. the set of as yet unselected documents in R ; Sim 1 is the similarity metric used in document retrieval and relevance ranking between documents ( passages ) and a query ; and Sim 2 can be the same as Sim 1 or a different metric .</sentence>
				<definiendum id="0">Q</definiendum>
				<definiendum id="1">; S</definiendum>
				<definiendum id="2">R\S</definiendum>
				<definiens id="0">a document collection ( or document stream )</definiens>
				<definiens id="1">a query or user profile ; R = IR ( C , Q , Œ∏ ) , i.e. , the ranked list of documents retrieved by an IR system , given C and Q and a relevance threshold Œ∏ , below which it will not retrieve documents ( Œ∏ can be degree of match or number of documents )</definiens>
				<definiens id="2">the subset of documents in R which is already selected ;</definiens>
				<definiens id="3">the set difference , i.e. the set of as yet unselected documents in R</definiens>
				<definiens id="4">the similarity metric used in document retrieval and relevance ranking between documents ( passages ) and a query</definiens>
			</definition>
			<definition id="4">
				<sentence>We define MMR-based feature selection as follows : Ô£∫ Ô£ª Ô£π Ô£Ø Ô£∞ Ô£Æ ‚àí‚àí‚ãÖ = ‚àà‚àà ) | ; ( max ) 1 ( ) ; ( max _ \ CwwIGpairCwIGArg FSMMR ji Sw i SRw ji ŒªŒª where C is the set of class labels , R is the set of candidate features , S is the subset of features in R which was already selected , R\S is the set difference , i.e. the set of as yet unselected features in R , IG is the information gain scores , and IGpair is the information gain scores of co-occurrence of the word ( feature ) pairs .</sentence>
				<definiendum id="0">MMR-based feature selection</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">R</definiendum>
				<definiendum id="3">S</definiendum>
				<definiendum id="4">R\S</definiendum>
				<definiendum id="5">IG</definiendum>
				<definiendum id="6">IGpair</definiendum>
				<definiens id="0">the set of class labels</definiens>
				<definiens id="1">the set of candidate features</definiens>
				<definiens id="2">the subset of features in R which was already selected ,</definiens>
				<definiens id="3">the set difference , i.e. the set of as yet unselected features in R</definiens>
				<definiens id="4">the information gain scores , and</definiens>
				<definiens id="5">the information gain scores of co-occurrence of the word ( feature ) pairs</definiens>
			</definition>
			<definition id="5">
				<sentence>IG and IGpair are defined as follows : ‚àë ‚àë ‚àë + + ‚àí= k ikiki k ikiki k kki wCpwCpwp wCpwCpwp CpCpCwIG ) | ( log ) | ( ) ( ) | ( log ) | ( ) ( ) ( log ) ( ) ; ( ‚àë ‚àë ‚àë + + ‚àí= k jikjikji k jikjikji k kkji wCpwCpwp wCpwCpwp CpCpCwwIGpair ) | ( log ) | ( ) ( ) | ( log ) | ( ) ( ) ( log ) ( ) | ; ( , , , , , , where p ( w i ) is the probability that word w i occurred , i w means that word w i doesn‚Äôt occur , p ( C k ) is the probability of the k-th class value , p ( C k |w i ) is the conditional probability of the k-th class value given that w i occurred , p ( w i , j ) is the probability that w i and w j co-occurred , and i w means that w i and w j doesn‚Äôt co-occur but w i or w j can occur ( i.e. ) ( 1 ) ( , , jiji wpwp ‚àí= ) .</sentence>
				<definiendum id="0">p ( w i )</definiendum>
				<definiendum id="1">, p ( C k |w i )</definiendum>
				<definiendum id="2">p</definiendum>
				<definiens id="0">follows : ‚àë ‚àë ‚àë + + ‚àí= k ikiki k ikiki k kki wCpwCpwp wCpwCpwp CpCpCwIG ) | ( log ) | ( ) ( ) | ( log ) | ( ) ( ) ( log ) ( ) ; ( ‚àë ‚àë ‚àë + + ‚àí= k jikjikji k jikjikji k kkji wCpwCpwp wCpwCpwp CpCpCwwIGpair ) | ( log ) | ( ) ( ) | ( log ) | ( ) ( ) ( log )</definiens>
				<definiens id="1">the probability that word w i occurred , i w means that word w i doesn‚Äôt occur , p ( C k ) is the probability of the k-th class value</definiens>
				<definiens id="2">the probability that w i and w j co-occurred , and i w means that w i and w j doesn‚Äôt co-occur but w i or w j can occur ( i.e. ) ( 1 ) ( , , jiji wpwp ‚àí= )</definiens>
			</definition>
			<definition id="6">
				<sentence>A disadvantage in using MMR-based feature selection is that the computational cost of computing the pairwise information gain ( i.e. IGpair ) is quadratic time with respect to the number of features .</sentence>
				<definiendum id="0">pairwise information gain</definiendum>
				<definiens id="0">quadratic time with respect to the number of features</definiens>
			</definition>
</paper>

		<paper id="4015">
			<definition id="0">
				<sentence>Bleu : a Method for Automatic Evaluation of Machine Translation .</sentence>
				<definiendum id="0">Bleu</definiendum>
			</definition>
</paper>

		<paper id="4037">
			<definition id="0">
				<sentence>A base phrase is a phrase that does not dominate another phrase .</sentence>
				<definiendum id="0">base phrase</definiendum>
				<definiens id="0">a phrase that does not dominate another phrase</definiens>
			</definition>
			<definition id="1">
				<sentence>The lower levels consist of the part of speech tags and the words .</sentence>
				<definiendum id="0">lower levels</definiendum>
			</definition>
</paper>

		<paper id="1005">
			<definition id="0">
				<sentence>MATCH ( Multimodal Access To City Help ) is a working city guide and navigation system that enables mobile users to access restaurant and subway information for New York City ( NYC ) ( Johnston et al. , 2002b ; Johnston et al. , 2002a ) .</sentence>
				<definiendum id="0">MATCH ( Multimodal Access To City Help</definiendum>
			</definition>
			<definition id="1">
				<sentence>Once a command or query is complete , it is passed to the multimodal generation component ( MMGEN ) , which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts .</sentence>
				<definiendum id="0">MMGEN</definiendum>
				<definiens id="0">builds a multimodal score indicating a coordinated sequence of graphical actions</definiens>
			</definition>
			<definition id="2">
				<sentence>The Multimodal UI coordinates presentation of graphical content with synthetic speech output using the AT &amp; T Natural Voices TTS engine ( Beutnagel et al. , 1999 ) .</sentence>
				<definiendum id="0">Multimodal UI</definiendum>
			</definition>
			<definition id="3">
				<sentence>The grammar consists of a set of context-free rules .</sentence>
				<definiendum id="0">grammar</definiendum>
				<definiens id="0">consists of a set of context-free rules</definiens>
			</definition>
			<definition id="4">
				<sentence>Models The problem of speech recognition can be succinctly represented as a search for the most likely word sequence ( a34 ) through the network created by the composition of a language of acoustic observations ( a35 ) , an acoustic model which is a transduction from acoustic observations to phone sequences ( a36 ) , a pronounciation model which is a transduction from phone sequences to word sequences ( a37 ) , and a language model acceptor ( a38 ) ( Pereira and Riley , 1997 ) .</sentence>
				<definiendum id="0">acoustic model</definiendum>
				<definiens id="0">a search for the most likely word sequence ( a34 ) through the network created by the composition of a language of acoustic observations</definiens>
			</definition>
			<definition id="5">
				<sentence>A mixture model ( a62a4a79a1a0a3a2 ) with mixture weight ( a4 ) is built by interpolating the model trained on the corpus of extracted sentences ( a62a6a5a8a7a3a9a11a10a13a12 ) and the model trained on the collected corpus ( a62a4a67 a68a70a69a4a71a73a72 ) .</sentence>
				<definiendum id="0">mixture model</definiendum>
				<definiendum id="1">a62a4a79a1a0a3a2</definiendum>
			</definition>
			<definition id="6">
				<sentence>We used the Switchboard corpus ( a59a60a10 a46a30a61 a60 ) as an example of a large vocabulary conversational speech corpus .</sentence>
				<definiendum id="0">Switchboard corpus</definiendum>
				<definiens id="0">an example of a large vocabulary conversational speech corpus</definiens>
			</definition>
			<definition id="7">
				<sentence>An LTAG consists of a set of elementary trees ( Supertags ) ( Bangalore and Joshi , 1999 ) each associated with a lexical item .</sentence>
				<definiendum id="0">LTAG</definiendum>
			</definition>
</paper>

		<paper id="1024">
			<definition id="0">
				<sentence>CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements ( e.g. , thesis statements ) .</sentence>
				<definiendum id="0">CriterionSM Online Essay Evaluation Service</definiendum>
				<definiens id="0">includes a capability that labels sentences in student writing with essay-based discourse elements ( e.g. , thesis statements )</definiens>
			</definition>
			<definition id="1">
				<sentence>CriterionSM Online Essay Evaluation Service is an application for writing instruction which includes a capability to annotate sentences in student essays with discourse element labels .</sentence>
				<definiendum id="0">CriterionSM Online Essay Evaluation Service</definiendum>
				<definiens id="0">an application for writing instruction which includes a capability to annotate sentences in student essays with discourse element labels</definiens>
			</definition>
			<definition id="2">
				<sentence>More specifically , a sentence is considered to be low on this dimension if it contains frequent patterns of error , defined as follows : ( a ) contains 2 errors in grammar , word usage or mechanics ( i.e. , spelling , capitalization or punctuation ) , ( b ) is an incomplete sentence , or ( c ) is a run-on sentence ( i.e. , 4 or more independent clauses within a sentence ) .</sentence>
				<definiendum id="0">b )</definiendum>
				<definiendum id="1">( c )</definiendum>
				<definiens id="0">an incomplete sentence , or</definiens>
				<definiens id="1">a run-on sentence ( i.e. , 4 or more independent clauses within a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>DimS , which concerns whether the target sentence relates to another sentence within the same discourse segment , seems another good candidate for applying our semantic similarity score to the task of establishing coherence .</sentence>
				<definiendum id="0">DimS</definiendum>
				<definiens id="0">concerns whether the target sentence relates to another sentence within the same discourse segment</definiens>
			</definition>
			<definition id="4">
				<sentence>For the two global coherence dimensions , DimP and DimT , a support vector machine provides a coherence ranking of sentences based on features related to essay-based discourse information , and semantic similarity values derived from the RI algorithm .</sentence>
				<definiendum id="0">DimP</definiendum>
				<definiens id="0">a coherence ranking of sentences based on features related to essay-based discourse information , and semantic similarity values derived from the RI algorithm</definiens>
			</definition>
</paper>

		<paper id="2001">
			<definition id="0">
				<sentence>Integrated language identification should be possible which helps later stages like speech synthesis to interact in user‚Äôs native language .</sentence>
				<definiendum id="0">Integrated language identification</definiendum>
				<definiens id="0">helps later stages like speech synthesis to interact in user‚Äôs native language</definiens>
			</definition>
			<definition id="1">
				<sentence>Front-end features consist of 39 dimensional Melscale cepstral coefficients .</sentence>
				<definiendum id="0">Front-end features</definiendum>
				<definiens id="0">consist of 39 dimensional Melscale cepstral coefficients</definiens>
			</definition>
			<definition id="2">
				<sentence>Vocal Tract Length Normalization ( VTLN ) is used to reduce inter and intraspeaker variability .</sentence>
				<definiendum id="0">Vocal Tract Length Normalization ( VTLN</definiendum>
				<definiens id="0">used to reduce inter and intraspeaker variability</definiens>
			</definition>
			<definition id="3">
				<sentence>LDHLDTLIML G¬®G¬®G=Y where LDTG is the set of Tamil dependent models .</sentence>
				<definiendum id="0">LDTG</definiendum>
				<definiens id="0">the set of Tamil dependent models</definiens>
			</definition>
			<definition id="4">
				<sentence>These algorithms are costlier in terms of memory space and execution time , which makes them difficult to handle real-time speech .</sentence>
				<definiendum id="0">execution time</definiendum>
				<definiens id="0">makes them difficult to handle real-time speech</definiens>
			</definition>
			<definition id="5">
				<sentence>Automatic language identification ( LID ) has received increased interest with the development of multilingual spoken language systems .</sentence>
				<definiendum id="0">Automatic language identification</definiendum>
				<definiendum id="1">LID</definiendum>
				<definiens id="0">has received increased interest with the development of multilingual spoken language systems</definiens>
			</definition>
</paper>

		<paper id="1018">
			<definition id="0">
				<sentence>Word and rhyme duration were normalized by phone duration statistics ( mean and variance ) calculated over all available training data .</sentence>
				<definiendum id="0">duration statistics</definiendum>
			</definition>
			<definition id="1">
				<sentence>The SRI Language Modeling Toolkit ( SRILM ) ( Stolcke , 2002 ) was used to train a trigram open-vocabulary language model with Kneser-Ney discounting ( Kneser and Ney , 1995 ) on data that had boundary events ( SU , ISU , and IP ) inserted in the word stream .</sentence>
				<definiendum id="0">SRI Language Modeling Toolkit</definiendum>
				<definiens id="0">used to train a trigram open-vocabulary language model with Kneser-Ney discounting ( Kneser and Ney , 1995 ) on data that had boundary events ( SU , ISU , and IP ) inserted in the word stream</definiens>
			</definition>
			<definition id="2">
				<sentence>TBL is an automatic rule learning technique that has been successfully applied to a variety of problems in natural language processing , including part-of-speech tagging ( Brill , 1995 ) , spelling correction ( Mangu and Brill , 1997 ) , error correction in automatic speech recognition ( Mangu and Padmanabhan , 2001 ) , and named entity detection ( Kim and Woodland , 2000 ) .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">an automatic rule learning technique that has been successfully applied to a variety of problems in natural language processing , including part-of-speech tagging</definiens>
			</definition>
			<definition id="3">
				<sentence>TBL is an iterative technique for inducing rules from training data .</sentence>
				<definiendum id="0">TBL</definiendum>
			</definition>
			<definition id="4">
				<sentence>A TBL system consists of a baseline predictor , a set of rule templates , and an objective function for scoring potential rules .</sentence>
				<definiendum id="0">TBL system</definiendum>
				<definiens id="0">consists of a baseline predictor , a set of rule templates , and an objective function for scoring potential rules</definiens>
			</definition>
			<definition id="5">
				<sentence>TBL produces concise , comprehensible rules , and uses the entire corpus to train all of the rules .</sentence>
				<definiendum id="0">TBL</definiendum>
				<definiens id="0">produces concise , comprehensible rules , and uses the entire corpus to train all of the rules</definiens>
			</definition>
			<definition id="6">
				<sentence>The input to our TBL system consists of text divided into utterances , with IPs and SUs inserted as if they were extra words .</sentence>
				<definiendum id="0">TBL system</definiendum>
				<definiens id="0">consists of text divided into utterances , with IPs and SUs inserted as if they were extra words</definiens>
			</definition>
			<definition id="7">
				<sentence>Is the word commonly used as : lled pause ( FP ) , backchannel ( BC ) , explicit editing term ( EET ) , discourse marker ( DM ) ?</sentence>
				<definiendum id="0">DM</definiendum>
				<definiens id="0">lled pause ( FP ) , backchannel ( BC ) , explicit editing term ( EET ) , discourse marker (</definiens>
			</definition>
			<definition id="8">
				<sentence>Transformation-based learning is an effective way to tag llers and edit regions after boundary events are tagged , but the best performance is obtained when training with automatically predicted SU and IP boundary events .</sentence>
				<definiendum id="0">Transformation-based learning</definiendum>
				<definiens id="0">an effective way to tag llers and edit regions after boundary events</definiens>
			</definition>
</paper>

		<paper id="1027">
			<definition id="0">
				<sentence>The WOT setup consists of two major phases that begin after subjects have been given a set of tasks to be solved with the telephone-based dialogue system : a0 in Phase 1 the human assistant ( a1 ) is acting as a wizard who is simulating the dialogue system , much like in WoZ experiments , by operating a speech synthesis interface , a0 in Phase 2 , which starts immediately after a system breakdown has been simulated by means of beeping noises transmitted via the telephone , the human assistant is acting as a human operator asking the subject to continue with the tasks .</sentence>
				<definiendum id="0">WOT setup</definiendum>
				<definiens id="0">consists of two major phases that begin after subjects have been given a set of tasks to be solved with the telephone-based dialogue system : a0 in Phase 1 the human assistant</definiens>
			</definition>
			<definition id="1">
				<sentence>Overlaps HCI-G HHI-G HCI-E HHI-E Number total 7 49 4 88 per dialogue 0.35 3.06 0.2 4.4 per turn 0.03 0.18 0.01 0.1 Table 5 : Overlaps in Phase 1 versus Phase 2 Lastly , our experiments yielded negative findings concerning differences in the type-token ratio ( denoting the lexical variation of forms ) , speech production errors ( false starts , repetitions etc. ) and syntax .</sentence>
				<definiendum id="0">false</definiendum>
				<definiens id="0">starts , repetitions etc. ) and syntax</definiens>
			</definition>
</paper>

		<paper id="4010">
			<definition id="0">
				<sentence>Named Entity Recognition ( NER ) is the first step for many tasks in the fields of natural language processing and information retrieval .</sentence>
				<definiendum id="0">Named Entity Recognition</definiendum>
				<definiendum id="1">NER )</definiendum>
				<definiens id="0">the first step for many tasks in the fields of natural language processing and information retrieval</definiens>
			</definition>
			<definition id="1">
				<sentence>It is a designated task in a number of conferences , including the Message Understanding Conference ( MUC ) , the Information Retrieval and Extraction Conference ( IREX ) , the Conferences on Natural Language Learning ( CoNLL ) and the recent Automatic Content Extraction Conference ( ACE ) .</sentence>
				<definiendum id="0">Information Retrieval</definiendum>
				<definiendum id="1">Extraction Conference ( IREX</definiendum>
			</definition>
			<definition id="2">
				<sentence>Natural language can be viewed as a stochastic process .</sentence>
				<definiendum id="0">Natural language</definiendum>
				<definiens id="0">a stochastic process</definiens>
			</definition>
			<definition id="3">
				<sentence>The Maximum Entropy principle can be stated as follows : given some set of constrains from observations , find the most uniform probability distribution ( Maximum Entropy ) p ( y|x ) that satisfies these constrains : 0 00 *argmax ( | ) 1 ( | ) exp ( ( , ) ( ) ( ) exp ( ( , ) yi i i m ii j jii j i lm ijjik kj yPyx Py x f x y Zx Zx f x y Œª Œª = == = =‚ãÖ =‚ãÖ ‚àë ‚àë‚àë In the above equations , f j ( x i , y k ) is a binary valued feature function , and Œª j is a weight that indicates how important feature f j is for the model .</sentence>
				<definiendum id="0">Maximum Entropy principle</definiendum>
				<definiendum id="1">Œª j</definiendum>
				<definiens id="0">given some set of constrains from observations , find the most uniform probability distribution ( Maximum Entropy ) p ( y|x ) that satisfies these constrains : 0 00 *argmax ( | ) 1 ( | ) exp ( ( , ) ( ) ( ) exp ( ( , ) yi i i m ii j jii j i lm ijjik kj yPyx Py x f x y Zx Zx f x y Œª Œª = == = =‚ãÖ =‚ãÖ ‚àë ‚àë‚àë In the above equations , f j ( x i , y k ) is a binary valued feature function</definiens>
				<definiens id="1">a weight that indicates how important feature f j is for the model</definiens>
			</definition>
			<definition id="4">
				<sentence>This corpus consists of about 20k sentences , annotated with word segmentation , part-of-speech tags and three namedentity tags including person ( PER ) , location ( LOC ) and organization ( ORG ) .</sentence>
				<definiendum id="0">ORG</definiendum>
				<definiens id="0">annotated with word segmentation , part-of-speech tags and three namedentity tags including person ( PER ) , location ( LOC</definiens>
			</definition>
			<definition id="5">
				<sentence>In particular , since most errors in Chinese ASR seem to be for person names , using NER score on the n-best hypotheses can improve recognition results by a relative 6.7 % in precision and PER LOC ORG Results F P F P F P One best n-best simple vote n-best weighted vote ( NE score ) n-best weighted vote ( all scores ) Table 4 .</sentence>
				<definiendum id="0">NER score</definiendum>
				<definiens id="0">n-best simple vote n-best weighted vote ( NE score ) n-best weighted vote ( all scores</definiens>
			</definition>
			<definition id="6">
				<sentence>We apply a maximum entropy ( MaxEnt ) based system to the n-best output of the BBN LVCSR system on Chinese Broadcast News utterances .</sentence>
				<definiendum id="0">maximum entropy</definiendum>
			</definition>
</paper>

		<paper id="1038">
			<definition id="0">
				<sentence>BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources .</sentence>
				<definiendum id="0">BABAR</definiendum>
			</definition>
			<definition id="1">
				<sentence>BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data .</sentence>
				<definiendum id="0">BABAR</definiendum>
				<definiens id="0">uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data</definiens>
			</definition>
			<definition id="2">
				<sentence>The second case involves existential noun phrases ( Allen , 1995 ) , which are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse .</sentence>
				<definiendum id="0">existential noun phrases</definiendum>
				<definiens id="0">are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse</definiens>
			</definition>
			<definition id="3">
				<sentence>Ex : The regime gives itself the right ... Ex : The brigade , which attacked ... Ex : Mr. Cristiani is the president ... Ex : The government said it ... Ex : He was found in San Jose , where ... Ex : Mr. Cristiani , president of the country ... Ex : Mr. Bush disclosed the policy by reading it ... Table 1 : Syntactic Seeding Heuristics BABAR‚Äôs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge .</sentence>
				<definiendum id="0">Ex</definiendum>
				<definiendum id="1">Ex</definiendum>
				<definiens id="0">The regime gives itself the right ... Ex : The brigade , which attacked ... Ex : Mr. Cristiani is the president</definiens>
				<definiens id="1">Syntactic Seeding Heuristics BABAR‚Äôs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge</definiens>
			</definition>
			<definition id="4">
				<sentence>The first type of contextual role knowledge that BABAR learns is the Caseframe Network ( CFNet ) , which identifies caseframes that co-occur in anaphor/antecedent resolutions .</sentence>
				<definiendum id="0">CFNet )</definiendum>
				<definiens id="0">identifies caseframes that co-occur in anaphor/antecedent resolutions</definiens>
			</definition>
			<definition id="5">
				<sentence>Terrorism Caseframe : engaged in &lt; NP &gt; NPs : activity , battle , clash , dialogue , effort , fight , group , shoot-out , struggle , village , violence Caseframe : ambushed &lt; patient &gt; NPs : company , convoy , helicopter , member , motorcade , move , Ormeno , patrol , position , response , soldier , they , troops , truck , vehicle , which Natural Disasters Caseframe : battled through &lt; NP &gt; NPs : flame , night , smoke , wall Caseframe : braced for &lt; NP &gt; NPs : arrival , battering , catastrophe , crest , Dolly , epidemics , evacuate , evacuation , flood , flooding , front , Hortense , hurricane , misery , rains , river , storm , surge , test , typhoon .</sentence>
				<definiendum id="0">vehicle</definiendum>
				<definiens id="0">activity , battle , clash , dialogue , effort , fight , group</definiens>
				<definiens id="1">company , convoy , helicopter , member , motorcade , move , Ormeno , patrol , position , response , soldier , they , troops , truck</definiens>
			</definition>
			<definition id="6">
				<sentence>Figure 2 : Lexical Caseframe Expectations To illustrate how lexical expectations are used , suppose we want to determine whether noun phrase X is the antecedent for noun phrase Y. If they are coreferent , then X and Y should be substitutable for one another in the story .</sentence>
				<definiendum id="0">X</definiendum>
				<definiens id="0">Lexical Caseframe Expectations To illustrate how lexical expectations are used , suppose we want to determine whether noun phrase</definiens>
			</definition>
			<definition id="7">
				<sentence>For each caseframe , BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe .</sentence>
				<definiendum id="0">BABAR</definiendum>
				<definiens id="0">collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe</definiens>
			</definition>
			<definition id="8">
				<sentence>For example , BABAR learned that agents that ‚Äúassassinate‚Äù or ‚Äúinvestigate a cause‚Äù are usually humans or groups ( i.e. , organizations ) .</sentence>
				<definiendum id="0">BABAR</definiendum>
				<definiens id="0">learned that agents that ‚Äúassassinate‚Äù or ‚Äúinvestigate a cause‚Äù are usually humans or groups ( i.e. , organizations )</definiens>
			</definition>
			<definition id="9">
				<sentence>Given an anaphor and candidate , BABAR checks ( 1 ) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate , and ( 2 ) whether the semantic classes of the candidate intersect with the semantic expectations of the caseframe that extracts the anaphor .</sentence>
				<definiendum id="0">BABAR</definiendum>
				<definiens id="0">checks ( 1 ) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate</definiens>
			</definition>
			<definition id="10">
				<sentence>BABAR uses the log-likelihood statistic ( Dunning , 1993 ) to evaluate the strength of a co-occurrence relationship .</sentence>
				<definiendum id="0">BABAR</definiendum>
				<definiens id="0">uses the log-likelihood statistic ( Dunning , 1993 ) to evaluate the strength of a co-occurrence relationship</definiens>
			</definition>
			<definition id="11">
				<sentence>Second , BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1 .</sentence>
				<definiendum id="0">BABAR</definiendum>
			</definition>
			<definition id="12">
				<sentence>Finally , a Dempster-Shafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision .</sentence>
				<definiendum id="0">Dempster-Shafer probabilistic model</definiendum>
				<definiens id="0">evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision</definiens>
			</definition>
			<definition id="13">
				<sentence>The semantic agreement KS eliminates some candidates , but also provides positive evidence in one case : if the candidate and anaphor both have semantic tags human , company , date , or location that were assigned via NER or the manually labeled dictionary entries .</sentence>
				<definiendum id="0">semantic agreement KS</definiendum>
				<definiens id="0">if the candidate and anaphor both have semantic tags human , company , date , or location that were assigned via NER or the manually labeled dictionary entries</definiens>
			</definition>
			<definition id="14">
				<sentence>Recency computes the relative distance between the candidate and the anaphor .</sentence>
				<definiendum id="0">Recency</definiendum>
				<definiens id="0">computes the relative distance between the candidate and the anaphor</definiens>
			</definition>
			<definition id="15">
				<sentence>SynRole computes relative frequency with which the candidate‚Äôs syntactic role occurs in resolutions .</sentence>
				<definiendum id="0">SynRole</definiendum>
				<definiens id="0">computes relative frequency with which the candidate‚Äôs syntactic role occurs in resolutions</definiens>
			</definition>
			<definition id="16">
				<sentence>The Recency KS computes the distance between the candidate and the anaphor relative to its scope .</sentence>
				<definiendum id="0">Recency KS</definiendum>
				<definiens id="0">computes the distance between the candidate and the anaphor relative to its scope</definiens>
			</definition>
			<definition id="17">
				<sentence>The SynRole KS computes the relative frequency with which the candidates‚Äô syntactic role ( subject , direct object , PP object ) appeared in resolutions in the training set .</sentence>
				<definiendum id="0">SynRole KS</definiendum>
				<definiens id="0">computes the relative frequency with which the candidates‚Äô syntactic role ( subject , direct object , PP object ) appeared in resolutions in the training set</definiens>
			</definition>
			<definition id="18">
				<sentence>BABAR uses a Dempster-Shafer decision model ( Stefik , 1995 ) to combine the evidence provided by the knowledge sources .</sentence>
				<definiendum id="0">BABAR</definiendum>
			</definition>
			<definition id="19">
				<sentence>Formally , the Dempster-Shafer theory defines a probability density function m ( S ) , where S is a set of hypotheses .</sentence>
				<definiendum id="0">Dempster-Shafer theory</definiendum>
				<definiendum id="1">S</definiendum>
				<definiens id="0">a set of hypotheses</definiens>
			</definition>
			<definition id="20">
				<sentence>m ( S ) represents the belief that the correct hypothesis is included in S. The model assumes that evidence also arrives as a probability density function ( pdf ) over sets of hypotheses .</sentence>
				<definiendum id="0">m ( S )</definiendum>
				<definiens id="0">a probability density function ( pdf ) over sets of hypotheses</definiens>
			</definition>
			<definition id="21">
				<sentence>In this situation , BABAR takes the conservative approach and declines to make a resolution .</sentence>
				<definiendum id="0">BABAR</definiendum>
				<definiens id="0">takes the conservative approach and declines to make a resolution</definiens>
			</definition>
</paper>

		<paper id="3005">
			<definition id="0">
				<sentence>The real-time speech recognition system produces a word error rate of roughly 20-30 % for English and Arabic news sources .</sentence>
				<definiendum id="0">real-time speech recognition system</definiendum>
			</definition>
</paper>

		<paper id="4025">
			<definition id="0">
				<sentence>Our corpus ( UAV-Corpus ) consists of 67 transcripts collected from 11 teams , who each completed 7 missions that simulate flight of an Uninhabited Air Vehicle ( UAV ) in the CERTT ( Cognitive Engineering Research on Team Tasks ) Lab 's synthetic team task environment ( CERTT UAV-STE ) .</sentence>
				<definiendum id="0">UAV-Corpus )</definiendum>
				<definiens id="0">consists of 67 transcripts collected from 11 teams , who each completed 7 missions that simulate flight of an Uninhabited Air Vehicle ( UAV ) in the CERTT ( Cognitive Engineering Research on Team Tasks ) Lab 's synthetic team task environment ( CERTT UAV-STE )</definiens>
			</definition>
			<definition id="1">
				<sentence>LSA is a fully automatic corpus-based statistical method for extracting and inferring relations of expected contextual usage of words in discourse ( Landauer et al. , 1998 ) .</sentence>
				<definiendum id="0">LSA</definiendum>
			</definition>
</paper>

		<paper id="1039">
			<definition id="0">
				<sentence>Maximum entropy models are a common modeling technique , but prone to overfitting .</sentence>
				<definiendum id="0">Maximum entropy models</definiendum>
				<definiens id="0">a common modeling technique , but prone to overfitting</definiens>
			</definition>
			<definition id="1">
				<sentence>Conditional maxent models are of the form P Œõ ( y|x ) = exp summationtext F i=1 Œª i f i ( x , y ) summationtext y prime exp summationtext i Œª i f i ( x , y prime ) where x is an input vector , y is an output , the f i are the socalled indicator functions or feature values that are true if a particular property of x , y is true , F is the number of such features , Œõ represents the parameter set Œª 1 ... Œª n , and Œª i is a weight for the indicator f i .</sentence>
				<definiendum id="0">Conditional maxent models</definiendum>
				<definiendum id="1">x</definiendum>
				<definiendum id="2">F</definiendum>
				<definiens id="0">the socalled indicator functions or feature values that are true if a particular property of x</definiens>
				<definiens id="1">the number of such features , Œõ represents the parameter set Œª 1 ... Œª n , and Œª i is a weight for the indicator f i</definiens>
			</definition>
			<definition id="2">
				<sentence>In particular , it has been suggested to use an update of the form Œª k : = Œª k + 1 f # log observed [ k ] ‚àó expected [ k ] where observed [ k ] ‚àó is the Good-Turing discounted value of observed [ k ] .</sentence>
				<definiendum id="0">‚àó</definiendum>
				<definiens id="0">suggested to use an update of the form Œª k : = Œª k + 1 f # log observed [ k ] ‚àó expected [ k ] where observed [ k ]</definiens>
			</definition>
			<definition id="3">
				<sentence>Tibshirani ( 1994 ) introduced Laplacian priors for linear models ( linear regressions ) and showed that an objective function that minimizes the absolute values of the parameters corresponds to a Laplacian prior .</sentence>
				<definiendum id="0">Laplacian priors</definiendum>
			</definition>
			<definition id="4">
				<sentence>‚àÇ ‚àÇŒ¥ k Ô£´ Ô£≠ summationdisplay j summationdisplay i Œ¥ i f i ( x j , y j ) + summationdisplay j 1 ‚àí summationdisplay y P Œõ ( y|x j ) summationdisplay i f i ( x j , y ) f # ( x j , y ) exp parenleftbig f # ( x j , y ) Œ¥ i parenrightbig ‚àí summationdisplay i Œ± i Œ¥ i parenrightBigg = summationdisplay j f k ( x j , y j ) + summationdisplay j ‚àí summationdisplay y P Œõ ( y|x j ) f k ( x j , y ) f # ( x j , y ) ‚àÇ ‚àÇŒ¥ k exp parenleftbig f # ( x j , y ) Œ¥ k parenrightbig ‚àíŒ± k = summationdisplay j f k ( x j , y j ) ‚àí summationdisplay j summationdisplay y P Œõ ( y|x j ) f k ( x j , y ) exp parenleftbig f # ( x j , y ) Œ¥ k parenrightbig ‚àí Œ± k =0 This gives us a version of Improved Iterative Scaling with an exponential Prior .</sentence>
				<definiendum id="0">f k</definiendum>
			</definition>
</paper>

		<paper id="1014">
			<definition id="0">
				<sentence>This is similar to an FST , which compactly represents a set of input/output string pairs , and in fact , R is a generalization of FST .</sentence>
				<definiendum id="0">FST</definiendum>
				<definiendum id="1">R</definiendum>
				<definiens id="0">a generalization of FST</definiens>
			</definition>
			<definition id="1">
				<sentence>For example , R is not closed under composition ( Rounds , 1970 ) , and neither are RL or F ( the ‚Äúfrontier-to-root‚Äù cousin of R ) , but the non-copying FL is closed under composition .</sentence>
				<definiendum id="0">F</definiendum>
				<definiens id="0">the ‚Äúfrontier-to-root‚Äù cousin of R ) , but the non-copying FL is closed under composition</definiens>
			</definition>
			<definition id="2">
				<sentence>R has surprising ability to change the structure of an input tree .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">has surprising ability to change the structure of an input tree</definiens>
			</definition>
			<definition id="3">
				<sentence>np VP ( x0 , x1 ) ‚Üí qnp x1 While general properties of R are understood , there are many algorithmic questions .</sentence>
				<definiendum id="0">np VP</definiendum>
				<definiens id="0">‚Üí qnp x1 While general properties of R are understood</definiens>
			</definition>
			<definition id="4">
				<sentence>TŒ£ ( X ) are the trees over alphabet Œ£ , indexed by X‚Äî the subset of TŒ£‚à™X where only leaves may be labeled by X. ( TŒ£ ( ‚àÖ ) = TŒ£ . )</sentence>
				<definiendum id="0">TŒ£ ( X )</definiendum>
				<definiens id="0">the trees over alphabet Œ£ , indexed by X‚Äî the subset of TŒ£‚à™X</definiens>
			</definition>
			<definition id="5">
				<sentence>The usual case ( the yield of t ) is yieldt‚â°yieldt ( Œ£ ) .</sentence>
				<definiendum id="0">usual case</definiendum>
				<definiens id="0">yieldt‚â°yieldt ( Œ£ )</definiens>
			</definition>
			<definition id="6">
				<sentence>Œ£ = { S , NP , VP , PP , PREP , DET , N , V , run , the , of , sons , daughters } N = { qnp , qpp , qdet , qn , qprep } S = q P = { q‚Üí1.0 S ( qnp , VP ( V ( run ) ) ) , qnp‚Üí0.6 NP ( qdet , qn ) , qnp‚Üí0.4 NP ( qnp , qpp ) , qpp‚Üí1.0 PP ( qprep , qnp ) , qdet‚Üí1.0 DET ( the ) , qprep‚Üí1.0 PREP ( of ) , qn‚Üí0.5 N ( sons ) , qn‚Üí0.5 N ( daughters ) } Figure 2 : A sample weighted regular tree grammar ( wRTG ) In this section , we describe the regular tree grammar , a common way of compactly representing a potentially infinite set of trees ( similar to the role played by the finitestate acceptor FSA for strings ) .</sentence>
				<definiendum id="0">DET</definiendum>
				<definiendum id="1">sample weighted regular tree grammar</definiendum>
				<definiens id="0">sons , daughters } N = { qnp , qpp , qdet , qn</definiens>
			</definition>
			<definition id="7">
				<sentence>A weighted regular tree grammar ( wRTG ) G is a quadruple ( Œ£ , N , S , P ) , where Œ£ is the alphabet , N is the finite set of nonterminals , S ‚ààN is the start ( or initial ) nonterminal , and P ‚äÜN√óTŒ£ ( N ) √óR+ is the finite set of weighted productions ( R+ ‚â° { r‚ààR|r &gt; 0 } ) .</sentence>
				<definiendum id="0">weighted regular tree grammar</definiendum>
				<definiendum id="1">wRTG ) G</definiendum>
				<definiendum id="2">Œ£</definiendum>
				<definiendum id="3">N</definiendum>
				<definiendum id="4">S ‚ààN</definiendum>
				<definiendum id="5">N ) √óR+</definiendum>
				<definiens id="0">a quadruple ( Œ£</definiens>
				<definiens id="1">the alphabet</definiens>
				<definiens id="2">the finite set of nonterminals</definiens>
				<definiens id="3">the start</definiens>
			</definition>
			<definition id="8">
				<sentence>We define xR , a convenience-oriented generalization of weighted R. Because of its good fit to natural language problems , xR is already briefly touched on , though not defined , in ( Rounds , 1970 ) .</sentence>
				<definiendum id="0">xR</definiendum>
				<definiendum id="1">xR</definiendum>
				<definiens id="0">a convenience-oriented generalization of weighted R. Because of its good fit to natural language problems</definiens>
			</definition>
			<definition id="9">
				<sentence>A weighted extended-lhs root-to-frontier tree transducer X is a quintuple ( Œ£ , ‚àÜ , Q , Qi , R ) where Œ£ is the input alphabet , and ‚àÜ is the output alphabet , Q is a finite set of states , Qi ‚ààQ is the initial ( or start , or root ) state , and R‚äÜQ√óXRPATŒ£√óT‚àÜ ( Q√ópaths ) √óR+ is a finite set of weighted transformation rules , written ( q , pattern ) ‚Üíw rhs , meaning that an input subtree matching pattern while in state q is transformed into rhs , with Q√ópaths leaves replaced by their ( recursive ) transformations .</sentence>
				<definiendum id="0">Œ£</definiendum>
				<definiendum id="1">‚àÜ</definiendum>
				<definiendum id="2">Q</definiendum>
				<definiendum id="3">Qi ‚ààQ</definiendum>
				<definiendum id="4">R‚äÜQ√óXRPATŒ£√óT‚àÜ</definiendum>
				<definiendum id="5">recursive</definiendum>
				<definiens id="0">a quintuple ( Œ£ , ‚àÜ , Q , Qi , R ) where</definiens>
				<definiens id="1">the output alphabet</definiens>
				<definiens id="2">a finite set of states</definiens>
				<definiens id="3">the initial ( or start , or root</definiens>
				<definiens id="4">a finite set of weighted transformation rules</definiens>
			</definition>
			<definition id="10">
				<sentence>xR is the set of all such transducers .</sentence>
				<definiendum id="0">xR</definiendum>
				<definiens id="0">the set of all such transducers</definiens>
			</definition>
			<definition id="11">
				<sentence>R , the set of conventional top-down transducers , is a subset of xR where the rules are restricted to use finite tree patterns that depend only on the root : RPATŒ£ ‚â° { pœÉ , r ( t ) } where pœÉ , r ( t ) ‚â° ( labelt ( ( ) ) = œÉ‚àßrankt ( ( ) ) = r ) .</sentence>
				<definiendum id="0">R</definiendum>
				<definiens id="0">a subset of xR where the rules</definiens>
			</definition>
			<definition id="12">
				<sentence>Derivation trees for a transducer X = ( Œ£ , ‚àÜ , Q , Qi , R ) are trees labeled by rules ( R ) that dictate the choice of rules in a complete X-derivation .</sentence>
				<definiendum id="0">Derivation trees</definiendum>
				<definiens id="0">Œ£ , ‚àÜ , Q , Qi , R ) are trees labeled by rules ( R ) that dictate the choice of rules in a complete X-derivation</definiens>
			</definition>
			<definition id="13">
				<sentence>Œ±G ( n ‚àà N ) ‚â° 1 if n = S , else : uses of n in productionsz } | { X p , ( n‚Ä≤ , r , w ) ‚ààP : labelr ( p ) =n w ¬∑Œ±G ( n‚Ä≤ ) ¬∑ Y p‚Ä≤‚ààpathsr ( N ) ‚àí { p } Œ≤G ( labelr ( p‚Ä≤ ) ) | { z } sibling nonterminals Algorithm 1 : DERIV Input : xR transducer X = ( Œ£ , ‚àÜ , Q , Qi , R ) and observed tree pair I‚ààTŒ£ , O‚ààT‚àÜ .</sentence>
				<definiendum id="0">Œ±G</definiendum>
				<definiens id="0">uses of n in productionsz } | { X p , ( n‚Ä≤ , r</definiens>
			</definition>
			<definition id="14">
				<sentence>Estimation-Maximization training ( Dempster , Laird , and Rubin , 1977 ) works on the principle that the corpus likelihood can be maximized subject to some normalization constraint on the parameters by repeatedly ( 1 ) estimating the expectation of decisions taken for all possible ways of generating the training corpus given the current parameters , accumulating parameter counts , and ( 2 ) maximizing by assigning the counts to the parameters and renormalizing .</sentence>
				<definiendum id="0">Estimation-Maximization training</definiendum>
				<definiens id="0">works on the principle that the corpus likelihood can be maximized subject to some normalization constraint on the parameters by repeatedly ( 1 ) estimating the expectation of decisions taken for all possible ways of generating the training corpus given the current parameters , accumulating parameter counts</definiens>
			</definition>
			<definition id="15">
				<sentence>Formally , a weighted extended-lhs root-to-frontier tree-to-string transducer X is a quintuple ( Œ£ , ‚àÜ , Q , Qi , R ) where Œ£ is the input alphabet , and ‚àÜ is the output alphabet , Q is a finite set of states , Qi ‚àà Q is the initial ( or start , or root ) state , and R ‚äÜ Q√ó XRPATŒ£√ó ( ‚àÜ‚à™ ( Q√ópaths ) ) ‚ãÜ√óR+ are a finite set of weighted transformation rules , written ( q , pattern ) ‚Üíw rhs .</sentence>
				<definiendum id="0">X</definiendum>
				<definiendum id="1">Œ£</definiendum>
				<definiendum id="2">‚àÜ</definiendum>
				<definiendum id="3">Q</definiendum>
				<definiens id="0">a quintuple ( Œ£ , ‚àÜ , Q , Qi , R ) where</definiens>
				<definiens id="1">the output alphabet</definiens>
				<definiens id="2">a finite set of states , Qi ‚àà Q is the initial ( or start , or root</definiens>
			</definition>
			<definition id="16">
				<sentence>xRS is the same as xR , except that the rhs are strings containing some nonterminals instead of trees containing nonterminal leaves ( so the intermediate derivation objects Algorithm 2 : TRAIN Input : xR transducer X = ( Œ£ , ‚àÜ , Q , Qd , R ) , observed weighted tree pairs T ‚ààTŒ£√óT‚àÜ√óR+ , normalization function Z ( { countr | r ‚àà R } , r‚Ä≤ ‚àà R ) , minimum relative log-likelihood change for convergence «´‚ààR+ , maximum number of iterations maxit ‚àà N , and prior counts ( for a so-called Dirichlet prior ) { priorr |r‚ààR } for smoothing each rule .</sentence>
				<definiendum id="0">xRS</definiendum>
				<definiens id="0">observed weighted tree pairs T ‚ààTŒ£√óT‚àÜ√óR+ , normalization function Z ( { countr | r ‚àà R }</definiens>
			</definition>
</paper>

		<paper id="1015">
			<definition id="0">
				<sentence>A content model is an HMM in which each state a1 corresponds to a distinct topic and generates sentences relevant to that topic according to a state-specific language model a2a4a3 ‚Äî note that standard a0 -gram language models can therefore be considered to be degenerate ( single-state ) content models .</sentence>
				<definiendum id="0">content model</definiendum>
				<definiens id="0">a distinct topic and generates sentences relevant to that topic according to a state-specific language model a2a4a3 ‚Äî note that standard a0 -gram language models</definiens>
			</definition>
			<definition id="1">
				<sentence>Determining states , emission probabilities , and transition probabilities Given a set a13 a9a15a14 a13 a12a16a14a18a17a19a17a18a17a19a14 a13a21a20 of a12 clusters , where a13a18a20 is the ‚Äúetcetera‚Äù cluster , we construct a content model with corresponding states a1 a9 a14 a1 a12 a14a18a17a19a17a18a17a19a14 a1 a20 ; we refer to a1 a20 as the insertion state .</sentence>
				<definiendum id="0">a13a18a20</definiendum>
				<definiens id="0">the ‚Äúetcetera‚Äù cluster</definiens>
			</definition>
			<definition id="2">
				<sentence>For each state a1 a1 , a22a24a23a25a12 , bigram probabilities ( which induce the state‚Äôs sentence-emission probabilities ) are estimated using smoothed counts from the corresponding cluster : a2 a3a27a26a36a21 a8 a8 a4 a8 a24 a26a11a28 a29a6 a28a16a29 a26a11a21 a8a10a8a9a8 a24a31a30a33a32 a9 a28a16a29 a26a11a21 a8 a24a34a30a33a32 a9 a4a35a36a4 a14 where a28 a29 a26 a21a7a37 a24 is the frequency with which word sequence a37 occurs within the sentences in cluster a13 a1 , and a35 is the vocabulary .</sentence>
				<definiendum id="0">a35</definiendum>
				<definiens id="0">the vocabulary</definiens>
			</definition>
			<definition id="3">
				<sentence>The fifth consists of narratives from the National Transportation Safety Board‚Äôs database previously employed by Jones and Thompson ( 2003 ) for event-identification experiments .</sentence>
				<definiendum id="0">fifth</definiendum>
				<definiens id="0">consists of narratives from the National Transportation Safety Board‚Äôs database previously employed by Jones and Thompson ( 2003 ) for event-identification experiments</definiens>
			</definition>
</paper>

		<paper id="1007">
			<definition id="0">
				<sentence>8This and the previous pattern frequently identify hyponymy relations ; typically , NP1 is the hypernym of NP2 .</sentence>
				<definiendum id="0">NP1</definiendum>
				<definiens id="0">the hypernym of NP2</definiens>
			</definition>
			<definition id="1">
				<sentence>10This pattern typically identifies an exemplification relationship , where NP2 is an instance of NP1 .</sentence>
				<definiendum id="0">NP2</definiendum>
				<definiens id="0">an instance of NP1</definiens>
			</definition>
			<definition id="2">
				<sentence>Previous factoid quesCopular A fractal is a pattern that is irregular , but self-similar at all size scales Become Althea Gibson became the first black tennis player to win a Wimbledon singles title Verb Francis Scott Key wrote ‚ÄúThe Star-Spangled Banner‚Äù Appositive The Aga Khan , Spiritual Leader of the Ismaili Muslims Occupation steel magnate Andrew Carnegie Parenthesis Alice Rivlin ( director of the Office of Management and Budget ) Also-known-as special proteins , known as enzymes // amitriptyline , also known as Elavil Also-called amino acid called phenylalanine Or caldera , or cauldron-like cavity on the summit Like prominent human rights leaders like Desmond Tutu Relative clause Solar cells which currently produce less than one percent of global power supplies Table 2 : Example nuggets for each pattern .</sentence>
				<definiendum id="0">Previous factoid quesCopular A fractal</definiendum>
			</definition>
			<definition id="3">
				<sentence>Redundancy presents a major challenge for integrating knowledge from multiple sources .</sentence>
				<definiendum id="0">Redundancy</definiendum>
			</definition>
			<definition id="4">
				<sentence>Finally , the answer merging component decides how many responses to return .</sentence>
				<definiendum id="0">answer merging component</definiendum>
				<definiens id="0">decides how many responses to return</definiens>
			</definition>
</paper>

		<paper id="4039">
			<definition id="0">
				<sentence>Topic marking postpositions ( or ‚Äútopic markers‚Äù ) , typically ‚Äúwa , ‚Äù mark a nominal phrase as the theme .</sentence>
				<definiendum id="0">Topic marking postpositions</definiendum>
				<definiens id="0">or ‚Äútopic markers‚Äù ) , typically ‚Äúwa , ‚Äù mark a nominal phrase as the theme</definiens>
			</definition>
			<definition id="1">
				<sentence>Another interesting aspect of information structure is that in English grammar , a whinterrogative ( what , how , etc. ) at the beginning of a sentence marks the theme and indicates that the content of the theme is the focus ( Halliday , 1967 ) .</sentence>
				<definiendum id="0">whinterrogative</definiendum>
				<definiens id="0">the beginning of a sentence marks the theme and indicates that the content of the</definiens>
			</definition>
			<definition id="2">
				<sentence>A gesture consists of preparation , stroke , and retraction ( McNeill , 1992 ) , and a stroke co-occurs with the most prominent syllable ( Kendon , 1972 ) .</sentence>
				<definiendum id="0">gesture</definiendum>
			</definition>
			<definition id="3">
				<sentence>As shown in Figure 2 , CAST consists of four main components : ( 1 ) the Agent Behavior Selection Module ( ABS ) , ( 2 ) the Language Tagging Module ( LTM ) , ( 3 ) the agent animation system , and ( 4 ) a text-to-speech engine ( TTS ) .</sentence>
				<definiendum id="0">CAST</definiendum>
				<definiens id="0">consists of four main components : ( 1 ) the Agent Behavior Selection Module ( ABS ) , ( 2 ) the Language Tagging Module ( LTM ) , ( 3 ) the agent animation system , and ( 4 ) a text-to-speech engine ( TTS )</definiens>
			</definition>
			<definition id="4">
				<sentence>The ABS selects appropriate gestures and facial expressions based on the linguistic information calculated by the LTM .</sentence>
				<definiendum id="0">ABS</definiendum>
				<definiens id="0">selects appropriate gestures and facial expressions based on the linguistic information calculated by the LTM</definiens>
			</definition>
			<definition id="5">
				<sentence>Tagging linguistic information : First , the LTM parses the input text and calculates the linguistic information described in Sec .</sentence>
				<definiendum id="0">LTM</definiendum>
				<definiens id="0">parses the input text and calculates the linguistic information described in Sec</definiens>
			</definition>
			<definition id="6">
				<sentence>Finally , the ABS transforms the XML into a time schedule by accessing the TTS engine and estimating the phoneme and bunsetsu boundary timings .</sentence>
				<definiendum id="0">ABS</definiendum>
				<definiens id="0">transforms the XML into a time schedule by accessing the TTS engine and estimating the phoneme and bunsetsu boundary timings</definiens>
			</definition>
</paper>

		<paper id="1016">
			<definition id="0">
				<sentence>1,2,3 N , P Sem Analysis Countability detection 1,2 N , Det Sem Analysis Table 1 : Overview of the tasks investigated in this paper ( n : size of n-gram ; POS : parts of speech ; Ling : linguistic knowledge ; Type : type of task ) tic , involving analysis ( e.g. , disambiguation ) and generation ( e.g. , selection among competing outputs ) .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">parts of speech ; Ling : linguistic knowledge ; Type : type of task ) tic , involving analysis ( e.g. , disambiguation</definiens>
			</definition>
			<definition id="1">
				<sentence>Translation Target word selection is a generation task that occurs in machine translation ( MT ) .</sentence>
				<definiendum id="0">Translation Target word selection</definiendum>
				<definiens id="0">a generation task that occurs in machine translation ( MT )</definiens>
			</definition>
			<definition id="2">
				<sentence>Grefenstette translates compounds from German and Spanish into English , and uses BNC frequencies as a filter for candidate translations .</sentence>
				<definiendum id="0">Grefenstette</definiendum>
				<definiens id="0">translates compounds from German and Spanish into English , and uses BNC frequencies as a filter for candidate translations</definiens>
			</definition>
			<definition id="3">
				<sentence>t2 ) denotes the modification of a category t2 by a category t1 .</sentence>
				<definiendum id="0">t2 )</definiendum>
			</definition>
			<definition id="4">
				<sentence>Countability is the semantic property that determines whether a noun can occur in singular and plural forms , and affects the range of permissible modifiers .</sentence>
				<definiendum id="0">Countability</definiendum>
				<definiens id="0">the semantic property that determines whether a noun can occur in singular and plural forms , and affects the range of permissible modifiers</definiens>
			</definition>
</paper>

		<paper id="4029">
			<definition id="0">
				<sentence>Spoken queries Process Spoken Input Respond to Input Initiate Interaction Desired Action Spoken query based Information Retrieval Initiate Interaction speech speech ( a ) Menu or dialog based spoken user interface ( b ) SILO Desired Action List Selection Figure 1 .</sentence>
				<definiendum id="0">Spoken</definiendum>
				<definiens id="0">queries Process Spoken Input Respond to Input Initiate Interaction Desired Action Spoken query based Information Retrieval Initiate Interaction speech speech ( a ) Menu or dialog based spoken user interface ( b ) SILO Desired Action List Selection Figure 1</definiens>
			</definition>
			<definition id="1">
				<sentence>Instead , it uses an N-gram language model that highlights the keywords and key phrases , but does not strictly disallow any particular sequence of words .</sentence>
				<definiendum id="0">N-gram language model</definiendum>
				<definiens id="0">highlights the keywords and key phrases , but does not strictly disallow any particular sequence of words</definiens>
			</definition>
			<definition id="2">
				<sentence>MediaFinder : retrieving music with spoken queries We now evaluate the effectiveness of SILO as a user interface .</sentence>
				<definiendum id="0">MediaFinder</definiendum>
				<definiens id="0">retrieving music with spoken queries We now evaluate the effectiveness of SILO as a user interface</definiens>
			</definition>
			<definition id="3">
				<sentence>The MediaFinder application is a spoken interface for retrieving music from digital collections , and represents a good example of an application where SILO can make a significant difference in the effectiveness of the UI .</sentence>
				<definiendum id="0">MediaFinder application</definiendum>
				<definiens id="0">a spoken interface for retrieving music from digital collections , and represents a good example of an application where SILO can make a significant difference in the effectiveness of the UI</definiens>
			</definition>
			<definition id="4">
				<sentence>The document indexer computes word count vectors for documents and stores them in an index .</sentence>
				<definiendum id="0">document indexer</definiendum>
				<definiens id="0">computes word count vectors for documents and stores them in an index</definiens>
			</definition>
</paper>

		<paper id="1035">
			<definition id="0">
				<sentence>Generally one defines an alignment as a relation between the words in the French sentence and the words in the English sentence .</sentence>
				<definiendum id="0">alignment</definiendum>
				<definiens id="0">a relation between the words in the French sentence and the words in the English sentence</definiens>
			</definition>
			<definition id="1">
				<sentence>For instance , the leaf labeled x2 means that when this rule is applied , x2 is replaced by the target subtree rooted at V B ( since V B is the second element of the input ) .</sentence>
				<definiendum id="0">x2</definiendum>
				<definiens id="0">V B is the second element of the input )</definiens>
			</definition>
			<definition id="2">
				<sentence>The span of a node n of the alignment graph is the subset of nodes from S that are reachable from n. Note that this definition is similar to , but not quite the same as , the definition of a span given by Fox ( 2002 ) .</sentence>
				<definiendum id="0">span of a node n of the alignment graph</definiendum>
				<definiens id="0">the subset of nodes from S that are reachable from n. Note that this definition is similar to , but not quite the same as , the definition of a span given by Fox ( 2002 )</definiens>
			</definition>
</paper>

		<paper id="4020">
			<definition id="0">
				<sentence>Mathematically , ) ( 1 ) ( ) ( Ep EpApK ‚àí ‚àí= where K is the kappa value , p ( A ) is the probability of the actual outcome and p ( E ) is the probability of the expected outcome as predicted by chance .</sentence>
				<definiendum id="0">K</definiendum>
				<definiendum id="1">p</definiendum>
				<definiens id="0">the probability of the actual outcome and p ( E ) is the probability of the expected outcome as predicted by chance</definiens>
			</definition>
			<definition id="1">
				<sentence>These annotation matrices , Mannotator , have N rows and M columns , where n is the number of messages and m is the number of labels .</sentence>
				<definiendum id="0">Mannotator</definiendum>
				<definiendum id="1">n</definiendum>
				<definiens id="0">the number of messages</definiens>
			</definition>
</paper>

		<paper id="3004">
			<definition id="0">
				<sentence>Introduction MiTAP ( Damianos et al. 2002 ) was originally developed by the MITRE Corporation under the Defense Advanced Research Projects Agency ( DARPA ) Translingual Information Detection Extraction and Summarization ( TIDES ) program .</sentence>
				<definiendum id="0">MiTAP</definiendum>
				<definiens id="0">originally developed by the MITRE Corporation under the Defense Advanced Research Projects Agency ( DARPA ) Translingual Information Detection Extraction and Summarization ( TIDES ) program</definiens>
			</definition>
			<definition id="1">
				<sentence>TIDES aims to revolutionize the way that information is obtained from human language by enabling people to find and interpret relevant information quickly and effectively , regardless of language or medium .</sentence>
				<definiendum id="0">TIDES</definiendum>
				<definiens id="0">aims to revolutionize the way that information is obtained from human language by enabling people to find and interpret relevant information quickly and effectively , regardless of language or medium</definiens>
			</definition>
			<definition id="2">
				<sentence>The Alembic natural language analyzer ( Aberdeen et al. 1995 ; Vilain and Day 1996 ) processes the zoned messages to identify paragraph , sentence , and word boundaries as well as part-of-speech tags .</sentence>
				<definiendum id="0">Alembic natural language analyzer</definiendum>
			</definition>
			<definition id="3">
				<sentence>HLID monitors a variety of sources including MiTAP articles , information events in RSS feeds , and other dynamically updated information on the World Wide Web .</sentence>
				<definiendum id="0">HLID</definiendum>
			</definition>
			<definition id="4">
				<sentence>HLID analyzes events from these sources in real time and generates an estimate of significance for each , complete with an audit trail of supporting and negating evidence .</sentence>
				<definiendum id="0">HLID</definiendum>
				<definiendum id="1">significance</definiendum>
				<definiens id="0">analyzes events from these sources in real time and generates an estimate of</definiens>
			</definition>
</paper>

		<paper id="4005">
			<definition id="0">
				<sentence>Automatic keyword indexing is the task of finding a small set of terms that describes the content of a specific document .</sentence>
				<definiendum id="0">Automatic keyword indexing</definiendum>
				<definiens id="0">the task of finding a small set of terms that describes the content of a specific document</definiens>
			</definition>
			<definition id="1">
				<sentence>RDS allows for the prediction to be treated as a regression task ( Breiman et al. , 1984 ) .</sentence>
				<definiendum id="0">RDS</definiendum>
			</definition>
</paper>

		<paper id="2004">
			<definition id="0">
				<sentence>voiceP DP John voice vDOP vDO p run ARGext ( John ; e ) ^ DO ( [ activity run ] ; e ) The entire voiceP is further embedded under a tense projection ( not shown here ) , and the verbal complex undergoes head movement and left adjoins to any overt tense markings .</sentence>
				<definiendum id="0">voiceP</definiendum>
				<definiens id="0">further embedded under a tense projection</definiens>
			</definition>
			<definition id="1">
				<sentence>Cyclic head movement ( incorporation ) of the verbal roots into the verbalizing heads up to the highest verbal projection accounts for the surface form of the sentence .</sentence>
				<definiendum id="0">Cyclic head movement</definiendum>
				<definiens id="0">the verbal roots into the verbalizing heads up to the highest verbal projection accounts for the surface form of the sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>This is shown below ( underline indicates canceled features , and the node label &lt; indicates that the left item projects ) : ( 16 ) &lt; the : : : =n d -k shelf : n The features &gt; x and &lt; x trigger head movement ( incorporation ) , i.e. , the phonetic content of the licensee is affixed to the left or right of the licensor‚Äôs phonetic content , respectively. These licensor features also cancel corresponding x licensee features : ( 17 ) &lt; book -s : : : &gt; n d -k book : n &lt; debone : : &lt; n V bone : n Finally , feature checking is implemented by +x/-x features. The +x denotes a need to discharge features , and the -x denotes a need for features. A simple example of this is the case assignment involved in building a prepositional phrase , i.e. , prepositions must assign case , and DPs much receive case. ( 18 ) &lt; on : : : =d : : : +k ploc &lt; the : : : =n : d : :-k shelf : n Niyogi ( 2001 ) has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm. I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework. As an example , a simplified derivation of the sentence ‚ÄúThe tire flattened.‚Äù is shown in Figure 1. The currently implemented system is still at the ‚Äútoy parser‚Äù stage. Although the effectiveness and coverage &lt; // : : &gt; s vbe‚Äöx : B E ( x ) /flat/ : s [ state flat ] &lt; /flat -en/ : : : : &gt; be =d‚Äöx : ‚Äöy : A RG‚Äì ( y ; e ) ^ BECOME ( x ; e ) &lt; : : &gt; s : : : vbeB E ( [ state flat ] ) : s &gt; /the tire/ : :dtire &lt; /flat -en/ : : : : &gt; be : :=d‚Äöy : A RG‚Äì ( y ; e ) ^ BECOME ( BE ( [ state tall ] ) ; e ) &lt; : : : &gt; s : : : :vbe : s ARG‚Äì ( he ; e ) ^ BECOME ( BE ( [ state tall ( 3cm ) ] ) ; e ) Figure 1 : Simplified derivation for the sentence ‚ÄúThe tire flattened.‚Äù</sentence>
				<definiendum id="0">RG‚Äì</definiendum>
				<definiendum id="1">RG‚Äì</definiendum>
				<definiens id="0">n The features &gt; x and &lt; x trigger head movement ( incorporation ) , i.e. , the phonetic content of the licensee is affixed to the left or right of the licensor‚Äôs phonetic content</definiens>
				<definiens id="1">:-k shelf : n Niyogi ( 2001 ) has developed an agenda-driven chart parser for the feature-driven formalism described above</definiens>
			</definition>
</paper>

		<paper id="2005">
			<definition id="0">
				<sentence>American Sign Language ( ASL ) is a visual/spatial natural language used primarily by the half million Deaf individuals in the U.S. and Canada .</sentence>
				<definiendum id="0">Sign Language ( ASL )</definiendum>
			</definition>
			<definition id="1">
				<sentence>ASL has a distinct grammar , vocabulary , and structure from English , and its visual modality allows it to use linguistic phenomena not seen in spoken languages ( Liddell , 2003 ; Neidle et al. , 2000 ) .</sentence>
				<definiendum id="0">ASL</definiendum>
				<definiens id="0">a distinct grammar , vocabulary , and structure from English , and its visual modality allows it to use linguistic phenomena not seen in spoken languages</definiens>
			</definition>
			<definition id="2">
				<sentence>To collect a corpus for statistical MT research , a movement annotation standard must be developed , ASL performances videotaped , and finally the videos manually transcribed ‚Äì a slow and expensive process ( Niedle , 2000 ) .</sentence>
				<definiendum id="0">ASL</definiendum>
			</definition>
			<definition id="3">
				<sentence>As opposed to spoken/written languages , ASL relies on the multiple simultaneous channels of handshape , hand location , palm orientation , hand/arm movement , facial expressions , and other non-manual signals to convey meaning .</sentence>
				<definiendum id="0">ASL</definiendum>
				<definiens id="0">relies on the multiple simultaneous channels of handshape , hand location , palm orientation , hand/arm movement , facial expressions</definiens>
			</definition>
			<definition id="4">
				<sentence>ASL signers use the space around them for several grammatical , discourse , and descriptive purposes .</sentence>
				<definiendum id="0">ASL signers</definiendum>
				<definiens id="0">use the space around them for several grammatical , discourse , and descriptive purposes</definiens>
			</definition>
			<definition id="5">
				<sentence>Interlingual systems take this analysis of the input text one step further : the source is analyzed and semantically processed to produce a typically languageindependent semantic representation called an ‚Äúinterlingua , ‚Äù and then a generation component produces the target-language surface form from there .</sentence>
				<definiendum id="0">Interlingual systems</definiendum>
				<definiens id="0">the source is analyzed and semantically processed to produce a typically languageindependent semantic representation called an ‚Äúinterlingua</definiens>
			</definition>
			<definition id="6">
				<sentence>NLI has been used in military training and equipment repair domains and can be extended by augmenting its library of Parameterized Action Representations ( PARs ) , to cover additional domains of English input texts .</sentence>
				<definiendum id="0">NLI</definiendum>
				<definiens id="0">used in military training and equipment repair domains and can be extended by augmenting its library of Parameterized Action Representations ( PARs ) , to cover additional domains of English input texts</definiens>
			</definition>
			<definition id="7">
				<sentence>PARs serve as 3D motion primitives and are used as hierarchical planning operators to produce a detailed animation specification ; so , they contain fields like preconditions and sub-actions used in NLI‚Äôs animation planning process ( Badler et al. , 2000 ) .</sentence>
				<definiendum id="0">PARs</definiendum>
			</definition>
			<definition id="8">
				<sentence>However , ASL linguists have identified discourse and other factors beyond the 3D scene model that can affect how classifier predicates are generated ( Liddell , 2003 ) .</sentence>
				<definiendum id="0">ASL</definiendum>
			</definition>
</paper>

		<paper id="4004">
</paper>

		<paper id="1009">
			<definition id="0">
				<sentence>Systems by S s , Outfit s , and SE s Maximum-likelihood estimates of the questions‚Äô difficulty and the systems‚Äô abilities were computed via Winsteps .</sentence>
				<definiendum id="0">Outfit</definiendum>
				<definiendum id="1">SE</definiendum>
				<definiens id="0">s Maximum-likelihood estimates of the questions‚Äô difficulty and the systems‚Äô abilities were computed via Winsteps</definiens>
			</definition>
			<definition id="1">
				<sentence>For questions , the Y-value gives the estimate of the questions‚Äô difficulty ( i.e. , Q q ) ; for systems , the Yvalue reflects the estimate of systems‚Äô ability ( S s ) .</sentence>
				<definiendum id="0">Yvalue</definiendum>
			</definition>
			<definition id="2">
				<sentence>As is also reflected by the size of the dots in Figure 3 , it can be seen that SE s is smaller for intermediate and high performing systems ( i.e. , S s between -3 and 1 Logits ) than for low performing systems ( S s &lt; -3 Logits ) .</sentence>
				<definiendum id="0">S</definiendum>
			</definition>
			<definition id="3">
				<sentence>The purpose of equating is to achieve equivalent test scores across different test sets .</sentence>
				<definiendum id="0">purpose of equating</definiendum>
				<definiens id="0">to achieve equivalent test scores across different test sets</definiens>
			</definition>
			<definition id="4">
				<sentence>The Winsteps software achieves this by shifting the locations in the Hard set to be consistent with the Equating set ‚Äì but without adjusting the spacing of the questions in the Hard or Easy sets .</sentence>
				<definiendum id="0">Winsteps software</definiendum>
			</definition>
</paper>

		<paper id="3002">
			<definition id="0">
				<sentence>ITSPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its back-end .</sentence>
				<definiendum id="0">ITSPOKE</definiendum>
				<definiens id="0">a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its back-end</definiens>
			</definition>
			<definition id="1">
				<sentence>A student rst types a natural language answer to a qualitative physics problem .</sentence>
				<definiendum id="0">student rst</definiendum>
				<definiens id="0">types a natural language answer to a qualitative physics problem</definiens>
			</definition>
			<definition id="2">
				<sentence>ITSPOKE is a speech-enabled version of the Why2Atlas ( VanLehn et al. , 2002 ) text-based dialogue tutoring system .</sentence>
				<definiendum id="0">ITSPOKE</definiendum>
				<definiens id="0">a speech-enabled version of the Why2Atlas ( VanLehn et al. , 2002 ) text-based dialogue tutoring system</definiens>
			</definition>
			<definition id="3">
				<sentence>STUDENT : they‚Äôre equal Figure 2 : ITSPOKE Excerpt ( 3.5 minutes into session ) Figure 3 : The Architecture of ITSPOKE stalled Sphinx2 and Cepstral on the ITSPOKE machine .</sentence>
				<definiendum id="0">STUDENT</definiendum>
				<definiendum id="1">ITSPOKE Excerpt</definiendum>
				<definiens id="0">The Architecture of ITSPOKE stalled Sphinx2 and Cepstral on the ITSPOKE machine</definiens>
			</definition>
</paper>

		<paper id="4012">
			<definition id="0">
				<sentence>Multimodal Functional Unification Grammar ( MUG ) is a unification-based formalism that uses rules to generate content that is coordinated across several communication modes .</sentence>
				<definiendum id="0">Multimodal Functional Unification Grammar ( MUG )</definiendum>
				<definiens id="0">a unification-based formalism that uses rules to generate content that is coordinated across several communication modes</definiens>
			</definition>
			<definition id="1">
				<sentence>SmartKom ( Wahlster , 2002 ) is a recent effort that produces a multimodal user interface , using XML/XSLT techniques to render the output .</sentence>
				<definiendum id="0">SmartKom</definiendum>
				<definiens id="0">a recent effort that produces a multimodal user interface</definiens>
			</definition>
			<definition id="2">
				<sentence>The representation uses the following types of dialogue acts at the top level : ask for missing information , ask for a confirmation of an action or data , inform the user about the state of objects , or give context-dependent help .</sentence>
				<definiendum id="0">top level</definiendum>
				<definiens id="0">a confirmation of an action or data , inform the user about the state of objects , or give context-dependent help</definiens>
			</definition>
			<definition id="3">
				<sentence>MUG is a collection of components .</sentence>
				<definiendum id="0">MUG</definiendum>
				<definiens id="0">a collection of components</definiens>
			</definition>
			<definition id="4">
				<sentence>For each output , the MUG identifies an utterance plan , consisting of separate constituents in the output .</sentence>
				<definiendum id="0">MUG</definiendum>
				<definiens id="0">identifies an utterance plan , consisting of separate constituents in the output</definiens>
			</definition>
			<definition id="5">
				<sentence>, the utterance consists of an instruction and an interaction section .</sentence>
				<definiendum id="0">utterance</definiendum>
				<definiens id="0">consists of an instruction and an interaction section</definiens>
			</definition>
			<definition id="6">
				<sentence>2 66 66 66 66 66 66 66 66 66 66 66 4 action 3 2 4Mode h cat 1 i type 1 3 5 instruction 2 66 4 action 3 Mode `` cat confirm-mod text 4 # 3 77 5 user-input 2 4Mode `` cat yesnolist text 5 # 3 5 Mode `` cat askconfirmation text concat ( [ 4 , 5 ] ) # 3 77 77 77 77 77 77 77 77 77 77 77 5 Figure 3 : A MUG component that handles the confirmation of tasks or user input .</sentence>
				<definiendum id="0">MUG component</definiendum>
				<definiens id="0">handles the confirmation of tasks or user input</definiens>
			</definition>
			<definition id="7">
				<sentence>Coherence is a key element in designing a multimodal user interface , where the potential for confusion is increased .</sentence>
				<definiendum id="0">Coherence</definiendum>
				<definiens id="0">a key element in designing a multimodal user interface</definiens>
			</definition>
			<definition id="8">
				<sentence>The function E returns a set of semantic entities in e ( substructures ) and their embedding depths in d. The function P penalizes the non-realization of requested ( attribute realize ) semantic entities , while rewarding the ( possibly redundant ) realization of an entity .</sentence>
				<definiendum id="0">function E</definiendum>
				<definiens id="0">returns a set of semantic entities in e ( substructures ) and their embedding depths in d. The function P penalizes the non-realization of requested ( attribute realize ) semantic entities</definiens>
			</definition>
</paper>

		<paper id="3007">
			<definition id="0">
				<sentence>Text summarization is the process of identifying salient concepts in text , conceptualizing the relationships that exist among them and generating concise representations of the input text that preserve the gist of its content .</sentence>
				<definiendum id="0">Text summarization</definiendum>
				<definiens id="0">the process of identifying salient concepts in text , conceptualizing the relationships that exist among them and generating concise representations of the input text that preserve the gist of its content</definiens>
			</definition>
			<definition id="1">
				<sentence>The Mead Demo is a web-based demonstration of MEAD .</sentence>
				<definiendum id="0">Mead Demo</definiendum>
				<definiens id="0">a web-based demonstration of MEAD</definiens>
			</definition>
</paper>

		<paper id="1012">
			<definition id="0">
				<sentence>An elementary dependency graph is a simplified abstraction on a full MRS structure which uses no underspecification and retains only the major semantic predicates and their relations to one another .</sentence>
				<definiendum id="0">elementary dependency graph</definiendum>
				<definiens id="0">a simplified abstraction on a full MRS structure which uses no underspecification and retains only the major semantic predicates and their relations to one another</definiens>
			</definition>
			<definition id="1">
				<sentence>Log-linear models are popular for their ability to incorporate a wide variety of features without making assumptions about their independence.1 For log-linear models , the conditional probability of an analysisa0 given a sentence with a set of analysesa1a3a2 a4 a0a6a5a7a5a8a5a10a9 is given as : a11a13a12 a0a8a14a15a17a16a19a18a21a20a23a22a24a2 a25a7a26a28a27 a12a30a29a32a31 a33a35a34a37a36a39a38 a33 a12 a0a40a22a42a41 a33 a22 a43a44a12 a15a45a22 ( 1 ) wherea38a33a12a0a40a22 returns the number of times featurea46 occurs in analysisa0 , a41 a33 is a weight , a43a47a12a15a45a22 is a normalization factor for the sentence , and a18 a20 is a model .</sentence>
				<definiendum id="0">a43a47a12a15a45a22</definiendum>
				<definiens id="0">popular for their ability to incorporate a wide variety of features without making assumptions about their independence.1 For log-linear models , the conditional probability of an analysisa0 given a sentence with a set of analysesa1a3a2 a4 a0a6a5a7a5a8a5a10a9 is given as : a11a13a12 a0a8a14a15a17a16a19a18a21a20a23a22a24a2 a25a7a26a28a27 a12a30a29a32a31 a33a35a34a37a36a39a38 a33 a12 a0a40a22a42a41 a33 a22 a43a44a12 a15a45a22 ( 1 ) wherea38a33a12a0a40a22 returns the number of times featurea46 occurs in analysisa0</definiens>
				<definiens id="1">a weight</definiens>
				<definiens id="2">a normalization factor for the sentence , and a18 a20 is a model</definiens>
			</definition>
			<definition id="2">
				<sentence>We create our ensemble model ( called a product model ) using the product-ofexperts formulation ( Hinton , 1999 ) : a11a13a12 a0a8a14a15a17a16a19a18 a36 a5a7a5a8a5a48a18a21a49a50a22a51a2 a52 a49a53 a34a54a36 a11a13a12 a0a8a14a15a28a16a48a18 a53 a22 a43a47a12 a15a45a22 ( 2 ) Note that each individual modela18 a53 is a well-defined distribution and is usually taken from a fixed set of models .</sentence>
				<definiendum id="0">ensemble model</definiendum>
				<definiens id="0">a well-defined distribution and is usually taken from a fixed set of models</definiens>
			</definition>
			<definition id="3">
				<sentence>The conglomerate feature set uses a mixture of features gleaned from phrase structures , MRS structures , and elementary dependency graphs .</sentence>
				<definiendum id="0">conglomerate feature set</definiendum>
				<definiens id="0">uses a mixture of features gleaned from phrase structures , MRS structures , and elementary dependency graphs</definiens>
			</definition>
			<definition id="4">
				<sentence>Such variation is crucial for use in ensembles , and indeed , LL-PROD reduces the error rate Model Perf .</sentence>
				<definiendum id="0">LL-PROD</definiendum>
				<definiens id="0">reduces the error rate Model Perf</definiens>
			</definition>
			<definition id="5">
				<sentence>In all our methods , a1 denotes the set of analyses produced by the ERG for the sentence and a18 a20 is some model .</sentence>
				<definiendum id="0">a1</definiendum>
				<definiens id="0">the set of analyses produced by the ERG for the sentence and a18 a20 is some model</definiens>
			</definition>
			<definition id="6">
				<sentence>a14 is the set of modelsa18 a36 a5a8a5a8a5a19a18 a49 .</sentence>
				<definiendum id="0">a14</definiendum>
			</definition>
			<definition id="7">
				<sentence>Uncertainty sampling ( also called tree entropy by Hwa ( 2000 ) ) , measures the uncertainty of a model over the set of parses of a given sentence , based on the conditional distribution it assigns to them .</sentence>
				<definiendum id="0">Uncertainty sampling</definiendum>
				<definiendum id="1">tree entropy</definiendum>
			</definition>
			<definition id="8">
				<sentence>One way of measuring this is with vote entropy ( Argamon-Engelson and Dagan , 1999 ) :4 a38a19a18a21a20 a22a24a23a26a25 a12 a15a17a16a35a1 a22a24a2a16a3 a46 log mina12a17 a16a55a14a1a39a14a22 a5a6a8a7 a9 a27 a12 a0a16a48a15a45a22 a17 log a27 a12 a0a16a19a15a55a22 a17 ( 5 ) where a27 a12a0a16a48a15a45a22 is the number of committee members that preferred parsea0 .</sentence>
				<definiendum id="0">vote entropy</definiendum>
				<definiendum id="1">a27 a12a0a16a48a15a45a22</definiendum>
			</definition>
			<definition id="9">
				<sentence>We call this new method lowest best probability ( LBP ) selection , and calculate it as follows : a38a1a28a29a23a31a30 a12 a15a28a16a40a1 a16a19a18 a20a22 a2 max a6a8a7 a9 a11a13a12 a0 a14a28a15a17a16a19a18 a20a22 ( 6 ) LBP can be extended for use with an ensemble model in the same manner as uncertainty sampling ( that is , replace the single model probability with a product ) .</sentence>
				<definiendum id="0">uncertainty sampling</definiendum>
				<definiens id="0">this new method lowest best probability ( LBP ) selection , and calculate it as follows : a38a1a28a29a23a31a30 a12 a15a28a16a40a1 a16a19a18 a20a22 a2 max a6a8a7 a9 a11a13a12 a0 a14a28a15a17a16a19a18 a20a22</definiens>
			</definition>
			<definition id="10">
				<sentence>Hwa et al. ( 2003 ) showed that for parsers , AL outperforms the closely related co-training , and that some of the labeling could be automated .</sentence>
				<definiendum id="0">AL</definiendum>
				<definiens id="0">outperforms the closely related co-training</definiens>
			</definition>
</paper>

		<paper id="1037">
			<definition id="0">
				<sentence>The desired values for these parameters are obtained by maximizing the likelihood of the training data with respect to the model.3 Thus , whereas L &amp; L‚Äôs RAP system uses an additive system of weights that is trained manually , the MaxEnt system learns a multiplicative system of weights automatically .</sentence>
				<definiendum id="0">MaxEnt system</definiendum>
				<definiens id="0">learns a multiplicative system of weights automatically</definiens>
			</definition>
			<definition id="1">
				<sentence>Number Agreement ( num ) : Includes features to test a strict match of number ( e.g. , a singular pronoun and singular antecedent ) , as well as mere compatibility ( e.g. , a singular pronoun with an antecedent of unknown number ) .</sentence>
				<definiendum id="0">Number Agreement ( num )</definiendum>
				<definiens id="0">Includes features to test a strict match of number ( e.g. , a singular pronoun and singular antecedent ) , as well as mere compatibility ( e.g. , a singular pronoun with an antecedent of unknown number )</definiens>
			</definition>
			<definition id="2">
				<sentence>The rea script to collect the number of pages that the AltaVista search engine found for each predicateargument combination and its variants per the following schema , modeled directly after K &amp; L : Subject-Verb : Search for occurrences of the combinations N V where N is the singular or plural form of the subject head noun and V is the infinitive , singular or plural present , past , perfect , or gerund of the head verb .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">V</definiendum>
				<definiens id="0">the singular or plural form of the subject head noun</definiens>
				<definiens id="1">the infinitive , singular or plural present , past , perfect , or gerund of the head verb</definiens>
			</definition>
			<definition id="3">
				<sentence>Verb-Object : Search for occurrences of the combinations V Det N , where V and N are as above for the verb and object head noun respectively , and Det is the determiner the , a ( n ) , or the empty string .</sentence>
				<definiendum id="0">Det</definiendum>
				<definiens id="0">the determiner the , a ( n ) , or the empty string</definiens>
			</definition>
			<definition id="4">
				<sentence>Possessive-Noun : Search for occurrences of the combinations Poss N , where Poss is the singular or plural form of the possessive and N is the singular or plural form of the noun .</sentence>
				<definiendum id="0">Possessive-Noun</definiendum>
				<definiendum id="1">Poss</definiendum>
				<definiendum id="2">N</definiendum>
				<definiens id="0">the singular or plural form of the noun</definiens>
			</definition>
</paper>

		<paper id="3006">
			<definition id="0">
				<sentence>The semantic parser attempts to solve this problem , and produces a syntax-independent representation of sentence meaning , so that semantic constituents can be accessed and processed in a more meaningful and flexible way , avoiding the sometimes rigid interpretations produced by a syntactic analyzer .</sentence>
				<definiendum id="0">semantic parser</definiendum>
				<definiens id="0">attempts to solve this problem , and produces a syntax-independent representation of sentence meaning</definiens>
			</definition>
			<definition id="1">
				<sentence>Semantics is the denotation of a string of symbols , either a sentence or a word .</sentence>
				<definiendum id="0">Semantics</definiendum>
				<definiens id="0">the denotation of a string of symbols , either a sentence or a word</definiens>
			</definition>
			<definition id="2">
				<sentence>FrameNet ( Johnson et al. , 2002 ) provides the knowledge needed to identify case frames and semantic roles .</sentence>
				<definiendum id="0">FrameNet</definiendum>
				<definiens id="0">provides the knowledge needed to identify case frames and semantic roles</definiens>
			</definition>
			<definition id="3">
				<sentence>The general procedure of semantic parsing consists of three main steps : ( 1 ) syntactic parsing into an intermediate format , using a feature-augmented syntactic parser , and assignment of shallow semantic features ; ( 2 ) semantic role assignment ; ( 3 ) application of default rules .</sentence>
				<definiendum id="0">semantic parsing</definiendum>
				<definiens id="0">consists of three main steps : ( 1 ) syntactic parsing into an intermediate format , using a feature-augmented syntactic parser , and assignment of shallow semantic features</definiens>
			</definition>
			<definition id="4">
				<sentence>Constituents are assigned with features , and the grammar consists of a set of rules defining how constituents can connect to each other , based on the values of their features .</sentence>
				<definiendum id="0">Constituents</definiendum>
			</definition>
			<definition id="5">
				<sentence>For example , for the sentence I like to eat Mexican food because it is spicy , the semantic parser produces the following encoding of sentence type , frames , semantic constituents and roles , and various attributes and modifiers : T = assertion P = [ [ experiencer , [ [ entity , [ i ] , reference ( first ) ] , [ modification ( attribute ) , quantity ( single ) ] ] ] , [ interaction ( experiencer\_subj ) , [ love ] ] , [ modification ( attribute ) , time ( present ) ] , [ content , [ [ interaction ( ingestion ) , [ eat ] ] , [ ingestibles , [ entity , [ food ] ] [ [ modification ( restriction ) , [ mexican ] ] , ] ] ] ] , [ reason , [ [ agent , [ [ entity , [ it ] , reference ( third ) ] , [ modification ( attribute ) , quantity ( single ) ] ] ] , [ description , [ modification ( attribute ) , time ( present ) ] ] , [ modification ( attribute ) , taste\_property ( spicy ) ] ] ] ] The corresponding semantic tree is shown in Figure 1 .</sentence>
				<definiendum id="0">quantity</definiendum>
				<definiens id="0">time ( present ) ] ] , [ modification ( attribute )</definiens>
			</definition>
			<definition id="6">
				<sentence>The parsing process relies on rules derived from a frame dataset ( FrameNet ) and a semantic network ( WordNet ) .</sentence>
				<definiendum id="0">parsing process</definiendum>
				<definiendum id="1">WordNet</definiendum>
				<definiens id="0">relies on rules derived from a frame dataset ( FrameNet ) and a semantic network</definiens>
			</definition>
</paper>

		<paper id="1031">
			<definition id="0">
				<sentence>UES is the shaded portion : unnatural expressions , and expressions used only in written language .</sentence>
				<definiendum id="0">UES</definiendum>
				<definiens id="0">the shaded portion : unnatural expressions , and expressions used only in written language</definiens>
			</definition>
			<definition id="1">
				<sentence>Paraphrases are different expressions which convey the same or almost the same meaning .</sentence>
				<definiendum id="0">Paraphrases</definiendum>
				<definiens id="0">different expressions which convey the same or almost the same meaning</definiens>
			</definition>
			<definition id="2">
				<sentence>For example , face-to-face communication is one of the typical situations in which spoken language is used .</sentence>
				<definiendum id="0">face-to-face communication</definiendum>
				<definiens id="0">one of the typical situations in which spoken language is used</definiens>
			</definition>
			<definition id="3">
				<sentence>The familiarity expression is one kind of interpersonal expressions , which implies the speaker‚Äôs familiarity toward the listener .</sentence>
				<definiendum id="0">familiarity expression</definiendum>
				<definiens id="0">implies the speaker‚Äôs familiarity toward the listener</definiens>
			</definition>
			<definition id="4">
				<sentence>The 240 pages consist of 125 pages collected as written language corpus and 115 pages collected as spoken language corpus .</sentence>
				<definiendum id="0">240 pages</definiendum>
				<definiens id="0">consist of 125 pages collected as written language corpus and 115 pages collected as spoken language corpus</definiens>
			</definition>
			<definition id="5">
				<sentence>The occurrence probability is defined as follows : if CT does not contain any nouns C7C8B4CTB5BPBYB4CTB5BP # of predicates in a corpus .</sentence>
				<definiendum id="0">occurrence probability</definiendum>
				<definiens id="0">follows : if CT does not contain any nouns C7C8B4CTB5BPBYB4CTB5BP # of predicates in a corpus</definiens>
			</definition>
			<definition id="6">
				<sentence>The data set consists of 200 paraphrase pairs ( 70 positive pairs and 130 negative pairs ) .</sentence>
				<definiendum id="0">data set</definiendum>
				<definiens id="0">consists of 200 paraphrase pairs ( 70 positive pairs and 130 negative pairs )</definiens>
			</definition>
			<definition id="7">
				<sentence>F-set1 is a feature set of all the four features , and F-set2 is that of only two features : OP of source in the spoken language corpus , and OP of target in the spoken language corpus .</sentence>
				<definiendum id="0">F-set1</definiendum>
				<definiens id="0">a feature set of all the four features , and F-set2 is that of only two features : OP of source in the spoken language corpus , and OP of target in the spoken language corpus</definiens>
			</definition>
</paper>

		<paper id="1042">
			<definition id="0">
				<sentence>A linear-chain CRF with parameters Œõ = { Œª , ... } defines a conditional probability for a state ( or label1 ) sequence y = y1 ... yT given an input sequence x = x1 ... xT to be PŒª ( y|x ) = 1Z x exp parenleftBigg Tsummationdisplay t=1 summationdisplay k Œªkfk ( yt‚àí1 , yt , x , t ) parenrightBigg , ( 1 ) where Zx is the normalization constant that makes the probability of all state sequences sum to one , fk ( yt‚àí1 , yt , x , t ) is a feature function which is often binary-valued , but can be real-valued , and Œªk is a learned weight associated with feature fk .</sentence>
				<definiendum id="0">linear-chain CRF</definiendum>
				<definiendum id="1">Zx</definiendum>
				<definiendum id="2">Œªk</definiendum>
				<definiens id="0">the normalization constant that makes the probability of all state sequences sum to one , fk ( yt‚àí1 , yt , x</definiens>
				<definiens id="1">a feature function which is often binary-valued , but can be real-valued</definiens>
				<definiens id="2">a learned weight associated with feature fk</definiens>
			</definition>
			<definition id="1">
				<sentence>The feature functions can measure any aspect of a state transition , yt‚àí1 ‚Üí yt , and the observation sequence , x , centered at the current time step , t. For example , one feature function might have value 1 when yt‚àí1 is the state TITLE , yt is the state AUTHOR , and xt is a word appearing in a lexicon of people‚Äôs first names .</sentence>
				<definiendum id="0">yt</definiendum>
				<definiendum id="1">xt</definiendum>
				<definiens id="0">a word appearing in a lexicon of people‚Äôs first names</definiens>
			</definition>
			<definition id="2">
				<sentence>The discounted value is set to be Œªk ceilingleftck/Nceilingright√óœÉ2 where ck is the count of features , N is the bin size , and ceilingleftaceilingright is the ceiling function .</sentence>
				<definiendum id="0">ck</definiendum>
				<definiendum id="1">N</definiendum>
				<definiens id="0">the count of features</definiens>
			</definition>
			<definition id="3">
				<sentence>positive words , B as the number of false negative words , C as the number of false positive words , D as the number of true negative words , and A+ B + C +D is the total number of words .</sentence>
				<definiendum id="0">B</definiendum>
				<definiens id="0">the number of false negative words</definiens>
				<definiens id="1">the number of false positive words</definiens>
				<definiens id="2">the total number of words</definiens>
			</definition>
			<definition id="4">
				<sentence>Precision = AA+C Recall = AA+B F1 = 2√óPrecision√óRecallPrecision+Recall is the percentage of words whose predicted labels equal their true labels .</sentence>
				<definiendum id="0">Precision</definiendum>
				<definiens id="0">the percentage of words whose predicted labels equal their true labels</definiens>
			</definition>
			<definition id="5">
				<sentence>Whole instance accuracy is the percentage of instances in which every word is correctly labeled .</sentence>
				<definiendum id="0">Whole instance accuracy</definiendum>
			</definition>
			<definition id="6">
				<sentence>Feature engineering is a key component of any machine learning solution‚Äîespecially in conditionally-trained models with such freedom to choose arbitrary features‚Äîand plays an even more important role than regularization .</sentence>
				<definiendum id="0">Feature engineering</definiendum>
				<definiens id="0">a key component of any machine learning solution‚Äîespecially in conditionally-trained models with such freedom to choose arbitrary features‚Äîand plays an even more important role than regularization</definiens>
			</definition>
</paper>

		<paper id="2009">
			<definition id="0">
				<sentence>Each SSP represents both usage information and semantic constrains and is manually acquired .</sentence>
				<definiendum id="0">SSP</definiendum>
				<definiens id="0">both usage information and semantic constrains and is manually acquired</definiens>
			</definition>
			<definition id="1">
				<sentence>VerbNet ( Kipper et al. , 2000 ) is a computational verb lexicon , based on Levin‚Äôs verb classes ( Levin , 1993 ) , that contains syntactic and semantic information for English verbs .</sentence>
				<definiendum id="0">VerbNet</definiendum>
			</definition>
			<definition id="2">
				<sentence>Each VerbNet class defines a list of members , a list of possible thematic roles , and a list of frames ( patterns ) of how these semantic roles can be realized in a sentence .</sentence>
				<definiendum id="0">VerbNet class</definiendum>
				<definiens id="0">defines a list of members , a list of possible thematic roles , and a list of frames ( patterns ) of how these semantic roles can be realized in a sentence</definiens>
			</definition>
			<definition id="3">
				<sentence>WordNet ( Fellbaum , 1998 ) is an English lexical database containing about 120 000 entries of nouns , verbs , adjectives and adverbs , hierarchically organized in synonym groups ( called synsets ) , and linked with relations , such as hypernym , hyponym , holonym and others .</sentence>
				<definiendum id="0">WordNet</definiendum>
				<definiens id="0">an English lexical database containing about 120 000 entries of nouns , verbs , adjectives and adverbs , hierarchically organized in synonym groups ( called synsets ) , and linked with relations , such as hypernym , hyponym , holonym and others</definiens>
			</definition>
			<definition id="4">
				<sentence>For example , if the constituent is a noun phrase consisting of a noun phrase , followed by a prepositional phrase , its head is the head of the noun phrase and the PP is a modifier .</sentence>
				<definiendum id="0">PP</definiendum>
				<definiens id="0">a modifier</definiens>
			</definition>
</paper>

		<paper id="2006">
			<definition id="0">
				<sentence>POS of words before head ( JJ ) The POS tags of words inside the NP that precede the head , excluding determiners .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">tags of words inside the NP that precede the head , excluding determiners</definiens>
			</definition>
			<definition id="1">
				<sentence>POS of words after head ( null ) The POS tags of words inside the NP that follow the head , excluding determiners .</sentence>
				<definiendum id="0">POS</definiendum>
				<definiens id="0">tags of words inside the NP that follow the head , excluding determiners</definiens>
			</definition>
			<definition id="2">
				<sentence>Contextual predicates that were true in less than 5 base NPs in the training sets were deemed unreliable and rejected .</sentence>
				<definiendum id="0">Contextual</definiendum>
				<definiens id="0">predicates that were true in less than 5 base NPs in the training sets were deemed unreliable and rejected</definiens>
			</definition>
</paper>

		<paper id="4034">
			<definition id="0">
				<sentence>In Switchboard-I , 6.83 % of the words are overlapped in time , where we de ne w1 and w2 as being overlapped if s ( w1 ) s ( w2 ) &lt; e ( w1 ) or s ( w2 ) s ( w1 ) &lt; e ( w2 ) , where s ( ) and e ( ) are the starting and ending time of a word .</sentence>
				<definiendum id="0">e ( )</definiendum>
				<definiens id="0">w1 ) s ( w2 ) &lt; e ( w1 ) or s ( w2 ) s ( w1 ) &lt; e ( w2 )</definiens>
			</definition>
			<definition id="1">
				<sentence>The ICSI Meeting Recorder corpus ( Janin et al. , 2003 ) consists of a number of meeting conversations with three or more participants .</sentence>
				<definiendum id="0">ICSI Meeting Recorder corpus</definiendum>
				<definiens id="0">a number of meeting conversations with three or more participants</definiens>
			</definition>
			<definition id="2">
				<sentence>While one possible explanation of this is just due to decreased counts , we found that for such phrases p ( wtjwt 1 ; wt 2 ; at ) minwt 32S p4 ( wtjwt 1 ; wt 2 ; wt 3 ) where p4 is a four-gram , S = fw : C ( wt ; wt 1 ; wt 2 ; w ) &gt; 0g , and C is the 4-gram word count function for the switchboard training and test sets .</sentence>
				<definiendum id="0">p4</definiendum>
				<definiendum id="1">C</definiendum>
				<definiens id="0">the 4-gram word count function for the switchboard training and test sets</definiens>
			</definition>
</paper>

		<paper id="1021">
			<definition id="0">
				<sentence>We are given a source ( ‚ÄòChinese‚Äô ) sentence f = fJ1 = f1 , ... , fj , ... , fJ , which is to be translated into a target ( ‚ÄòEnglish‚Äô ) sentence e = eI1 = e1 , ... , ei , ... , eI Among all possible target sentences , we will choose the sentence with the highest probability : ÀÜeI1 = argmax eI1 { Pr ( eI1|fJ1 ) } ( 1 ) As an alternative to the often used source-channel approach ( Brown et al. , 1993 ) , we directly model the posterior probability Pr ( eI1|fJ1 ) ( Och and Ney , 2002 ) using a log-linear combination of feature functions .</sentence>
				<definiendum id="0">, eI Among</definiendum>
				<definiens id="0">given a source ( ‚ÄòChinese‚Äô ) sentence f = fJ1 = f1 , ... , fj , ... , fJ , which is to be translated into a target ( ‚ÄòEnglish‚Äô ) sentence e = eI1 = e1 , ... , ei , ...</definiens>
			</definition>
			<definition id="1">
				<sentence>Our baseline MT system is the alignment template system described in detail by Och , Tillmann , and Ney ( 1999 ) and Och and Ney ( 2004 ) .</sentence>
				<definiendum id="0">Ney</definiendum>
				<definiens id="0">the alignment template system described in detail by Och , Tillmann , and</definiens>
			</definition>
			<definition id="2">
				<sentence>The phrase penalty feature counts the number of phrases produced , and can allow the model to prefer either short or long phrases .</sentence>
				<definiendum id="0">phrase penalty feature</definiendum>
				<definiens id="0">counts the number of phrases produced</definiens>
			</definition>
</paper>

		<paper id="1041">
			<definition id="0">
				<sentence>Lin and Pantel ( 2001 ) proposed a clustering algorithm , UNICON , which generates similar lists but discriminates between senses of words .</sentence>
				<definiendum id="0">UNICON</definiendum>
				<definiens id="0">generates similar lists but discriminates between senses of words</definiens>
			</definition>
			<definition id="1">
				<sentence>Using sets of representative elements called committees , CBC discovers cluster centroids that unambiguously describe the members of a possible class .</sentence>
				<definiendum id="0">CBC</definiendum>
				<definiens id="0">discovers cluster centroids that unambiguously describe the members of a possible class</definiens>
			</definition>
			<definition id="2">
				<sentence>We first construct a frequency count vector C ( e ) = ( c e1 , c e2 , ÔøΩ , c em ) , where m is the total number of features and c ef is the frequency count of feature f occurring in word e. Here , c ef is the number of times word e occurred in a grammatical context f. For example , if the word wave occurred 217 times as the object of the verb catch , then the feature vector for wave will have value 217 for its ÔøΩobject-of catchÔøΩ feature .</sentence>
				<definiendum id="0">m</definiendum>
				<definiendum id="1">c ef</definiendum>
				<definiens id="0">e ) = ( c e1 , c e2 , ÔøΩ , c em )</definiens>
				<definiens id="1">the total number of features</definiens>
				<definiens id="2">the frequency count of feature f occurring in word e. Here , c ef is the number of times word e occurred in a grammatical context f. For example , if the word wave occurred 217 times as the object of the verb catch</definiens>
			</definition>
			<definition id="3">
				<sentence>We then construct a mutual information vector MI ( e ) = ( mi e1 , mi e2 , ÔøΩ , mi em ) for each word e , where mi ef is the pointwise mutual information between word e and feature f , which is defined as : N c N c N c ef m j ej n i if ef mi ‚àë ‚àë = = √ó = 1 1 log ( 1 ) where n is the number of words and N = ‚àë‚àë == n i m j ij c 11 is the total frequency count of all features of all words .</sentence>
				<definiendum id="0">mi ef</definiendum>
				<definiendum id="1">n</definiendum>
				<definiendum id="2">ij c 11</definiendum>
				<definiens id="0">a mutual information vector MI ( e ) = ( mi e1 , mi e2 , ÔøΩ , mi em ) for each word e</definiens>
				<definiens id="1">the pointwise mutual information between word e and feature f</definiens>
				<definiens id="2">the number of words</definiens>
			</definition>
			<definition id="4">
				<sentence>A committee is a set of representative elements that unambiguously describe the members of a possible class .</sentence>
				<definiendum id="0">committee</definiendum>
				<definiens id="0">a set of representative elements that unambiguously describe the members of a possible class</definiens>
			</definition>
			<definition id="5">
				<sentence>For each class c , we construct a matrix containing the similarity between each pair of words e i and e j in c using the cosine coefficient of their mutual information vectors ( Salton and McGill 1983 ) : ( ) ‚àë‚àë ‚àë √ó √ó = f fe f fe f fefe ji ji ji mimi mimi eesim 22 , ( 3 ) For each word e , we then cluster its most similar instances using group-average clustering ( Han and Kamber 2001 ) and we store as a candidate committee the highest scoring cluster c ' according to the following metric : | c'| ÔøΩ avgsim ( c ' ) ( 4 ) where |c'| is the number of elements in c ' and avgsim ( c ' ) is the average pairwise similarity between words in c ' .</sentence>
				<definiendum id="0">|c'|</definiendum>
				<definiens id="0">a matrix containing the similarity between each pair of words e i and e j in c using the cosine coefficient of their mutual information vectors</definiens>
				<definiens id="1">the average pairwise similarity between words in c '</definiens>
			</definition>
			<definition id="6">
				<sentence>We used Minipar ( Lin 1994 ) , a broad coverage parser , to parse 3GB of newspaper text from the Aquaint ( TREC-9 ) collection .</sentence>
				<definiendum id="0">Minipar</definiendum>
				<definiens id="0">a broad coverage parser , to parse 3GB of newspaper text from the Aquaint ( TREC-9 ) collection</definiens>
			</definition>
			<definition id="7">
				<sentence>The Kappa statistic ( Siegel and Castellan Jr. 1988 ) measures the agreements between a set of judgesÔøΩ assessments correcting for chance agreements : ( ) ( ) ( ) EP EPAP K ‚àí ‚àí = 1 ( 5 ) where P ( A ) is the probability of agreement between the judges and P ( E ) is the probability that the judges agree Table 1 .</sentence>
				<definiendum id="0">Kappa statistic</definiendum>
				<definiendum id="1">P ( A )</definiendum>
				<definiendum id="2">P ( E )</definiendum>
				<definiens id="0">the probability of agreement between the judges</definiens>
			</definition>
			<definition id="8">
				<sentence>Information ( Passage ) Retrieval Passage retrieval is used in QA to supply relevant information to an answer pinpointing module .</sentence>
				<definiendum id="0">Information ( Passage</definiendum>
				<definiens id="0">used in QA to supply relevant information to an answer pinpointing module</definiens>
			</definition>
</paper>

		<paper id="4014">
			<definition id="0">
				<sentence>HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports .</sentence>
				<definiendum id="0">HITIQA</definiendum>
				<definiens id="0">an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports</definiens>
			</definition>
			<definition id="1">
				<sentence>HITIQA project is part of the ARDA AQUAINT program that aims to make significant advances in state of the art of automated question answering .</sentence>
				<definiendum id="0">HITIQA project</definiendum>
				<definiens id="0">part of the ARDA AQUAINT program that aims to make significant advances in state of the art of automated question answering</definiens>
			</definition>
			<definition id="2">
				<sentence>The general frame represents an event or a relation involving any number of entities , which make up the frame‚Äôs attributes , such as LOCATION , PERSON , COUNTRY , ORGANIZATION , etc .</sentence>
				<definiendum id="0">general frame</definiendum>
				<definiens id="0">represents an event or a relation involving any number of entities</definiens>
			</definition>
			<definition id="3">
				<sentence>Iraq possesses a few working centrifuges and the blueprints to build them .</sentence>
				<definiendum id="0">Iraq</definiendum>
			</definition>
</paper>

		<paper id="4019">
			<definition id="0">
				<sentence>Speech Graffiti is a structured interaction protocol that is designed to be universal .</sentence>
				<definiendum id="0">Speech Graffiti</definiendum>
				<definiens id="0">a structured interaction protocol that is designed to be universal</definiens>
			</definition>
			<definition id="1">
				<sentence>UER is the key to good system performance since even if the system has correctly decoded a word string , it must still match that string with the appropriate concepts in order to perform the desired action .</sentence>
				<definiendum id="0">UER</definiendum>
				<definiens id="0">the key to good system performance since even if the system has correctly decoded a word string , it must still match that string with the appropriate concepts in order to perform the desired action</definiens>
			</definition>
</paper>

		<paper id="1030">
			<definition id="0">
				<sentence>The arguments of a verb are labeled ARG0 to ARG5 , where ARG0 is the PROTOAGENT ( usually the subject of a transitive verb ) ARG1 is the PROTO-PATIENT ( usually its direct object ) , etc .</sentence>
				<definiendum id="0">ARG0</definiendum>
				<definiendum id="1">ARG1</definiendum>
				<definiens id="0">the PROTOAGENT ( usually the subject of a transitive verb</definiens>
			</definition>
			<definition id="1">
				<sentence>PropBank attempts to treat semantically related verbs consistently .</sentence>
				<definiendum id="0">PropBank</definiendum>
				<definiens id="0">attempts to treat semantically related verbs consistently</definiens>
			</definition>
			<definition id="2">
				<sentence>Our baseline system uses the same set of features introduced by G &amp; J .</sentence>
				<definiendum id="0">baseline system</definiendum>
			</definition>
			<definition id="3">
				<sentence>With this strategy only one classifier ( NULL vs NONNULL ) has to be trained on all of the data .</sentence>
				<definiendum id="0">NULL</definiendum>
			</definition>
			<definition id="4">
				<sentence>The clustering algorithm uses a database of verb-direct-object relations extracted by Lin ( 1998 ) .</sentence>
				<definiendum id="0">clustering algorithm</definiendum>
			</definition>
			<definition id="5">
				<sentence>To evaluate this scenario , we used the Charniak parser ( Chaniak , 2001 ) to generate parses for PropBank training and test data .</sentence>
				<definiendum id="0">Charniak parser</definiendum>
			</definition>
			<definition id="6">
				<sentence>CCG is a form of dependency grammar and is hoped to capture long distance relationships better than a phrase structure grammar .</sentence>
				<definiendum id="0">CCG</definiendum>
				<definiens id="0">a form of dependency grammar and is hoped to capture long distance relationships better than a phrase structure grammar</definiens>
			</definition>
			<definition id="7">
				<sentence>The Chen and Rambow ( C &amp; R ) System Chen and Rambow report on two different systems , also using a decision tree classifier .</sentence>
				<definiendum id="0">Rambow ( C</definiendum>
				<definiens id="0">&amp; R ) System Chen and Rambow report on two different systems</definiens>
			</definition>
			<definition id="8">
				<sentence>The second ‚ÄúC &amp; R System II‚Äù uses additional syntactic and semantic representations that are extracted from a Tree Adjoining Grammar ( TAG ) ‚Äì another grammar formalism that better captures the syntactic properties of natural languages .</sentence>
				<definiendum id="0">R System II‚Äù</definiendum>
			</definition>
			<definition id="9">
				<sentence>SVM does a very good job of generalizing in both stages of processing .</sentence>
				<definiendum id="0">SVM</definiendum>
				<definiens id="0">does a very good job of generalizing in both stages of processing</definiens>
			</definition>
</paper>

		<paper id="1022">
			<definition id="0">
				<sentence>BLEU score ( Papineni et al. , 2001 ) computes the geometric mean of the precision of a41 -grams of various lengths ( a41a43a42a45a44a47a46a49a48a40a48a50a52a51 ) between a hypothesis and a reference translation , and includes a brevity penalty ( a53a54a8a10a1a17a11a12a1a21a2a14a13a56a55a57a46 ) if the hypothesis is shorter than the reference .</sentence>
				<definiendum id="0">BLEU score</definiendum>
				<definiens id="0">computes the geometric mean of the precision of a41 -grams of various lengths ( a41a43a42a45a44a47a46a49a48a40a48a50a52a51 ) between a hypothesis and a reference translation , and includes a brevity penalty</definiens>
			</definition>
			<definition id="1">
				<sentence>The BLEU score is zero if any of the n-gram precisions a69 a75 a8a10a1a17a11a12a1a6a2a14a13 is zero for that sentence pair .</sentence>
				<definiendum id="0">BLEU score</definiendum>
				<definiens id="0">zero if any of the n-gram precisions a69 a75 a8a10a1a17a11a12a1a6a2a14a13 is zero for that sentence pair</definiens>
			</definition>
			<definition id="2">
				<sentence>Position-independent Word Error Rate ( PER ) measures the minimum number of edit operations needed to transform a word string to any permutation of the other word string .</sentence>
				<definiendum id="0">Position-independent Word Error Rate ( PER )</definiendum>
				<definiens id="0">measures the minimum number of edit operations needed to transform a word string to any permutation of the other word string</definiens>
			</definition>
			<definition id="3">
				<sentence>The Bitree loss function measures the distance between two trees in terms of distances between their corresponding subtrees .</sentence>
				<definiendum id="0">Bitree loss function</definiendum>
				<definiens id="0">measures the distance between two trees in terms of distances between their corresponding subtrees</definiens>
			</definition>
			<definition id="4">
				<sentence>a24a17a8a12a8a25a1a48a47a47a11a12a4a46a47a35a13a49a12a0a21a13a39a58 a24a17a8a25a1a48a47a47a11a12a4a46a47a49a11a23a0a21a13 a50 a72a51 a76a79a78 a24a17a8a25a1 a51 a11a12a4 a51 a11a23a0a21a13 a48 ( 4 ) The conventional Maximum A Posteriori ( MAP ) decoder can be derived as a special case of the MBR decoder by considering a loss function that assigns a equal cost ( say 1 ) to all misclassifications .</sentence>
				<definiendum id="0">Posteriori</definiendum>
				<definiens id="0">a special case of the MBR decoder by considering a loss function that assigns a equal cost</definiens>
			</definition>
			<definition id="5">
				<sentence>In future work we plan to extend the search space of MBR decoders to translation lattices produced by the baseline system .</sentence>
				<definiendum id="0">MBR</definiendum>
				<definiens id="0">decoders to translation lattices produced by the baseline system</definiens>
			</definition>
			<definition id="6">
				<sentence>MBR is a promising modeling framework for statistical machine translation .</sentence>
				<definiendum id="0">MBR</definiendum>
			</definition>
</paper>

		<paper id="1033">
			<definition id="0">
				<sentence>In statistical machine translation , we are given a source language ( ‚ÄòFrench‚Äô ) sentence fJ1 = f1 : : : fj : : : fJ , which is to be translated into a target language ( ‚ÄòEnglish‚Äô ) sentence eI1 = e1 : : : ei : : : eI : Among all possible target language sentences , we will choose the sentence with the highest probability : ÀÜeI1 = argmax eI1 'Pr ( eI 1jf J 1 ) ‚Äú ( 1 ) = argmax eI1 'Pr ( eI 1 ) ¬¢Pr ( f J 1 je I 1 ) ‚Äú ( 2 ) The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation ( Brown et al. , 1990 ) .</sentence>
				<definiendum id="0">eI1 'Pr ( eI 1 ) ¬¢Pr</definiendum>
				<definiens id="0">given a source language ( ‚ÄòFrench‚Äô ) sentence fJ1 = f1 : : : fj : : : fJ , which is to be translated into a target language ( ‚ÄòEnglish‚Äô ) sentence eI1 = e1 : : : ei : : : eI : Among all possible target language sentences , we will choose the sentence with the highest probability : ÀÜeI1 = argmax eI1 'Pr ( eI 1jf J 1 ) ‚Äú ( 1 ) = argmax</definiens>
			</definition>
			<definition id="1">
				<sentence>The translation model links the source language sentence to the target language sentence .</sentence>
				<definiendum id="0">translation model</definiendum>
				<definiens id="0">links the source language sentence to the target language sentence</definiens>
			</definition>
			<definition id="2">
				<sentence>The argmax operation denotes the search problem , i.e. the generation of the output sentence in the target language .</sentence>
				<definiendum id="0">argmax operation</definiendum>
				<definiens id="0">the search problem , i.e. the generation of the output sentence in the target language</definiens>
			</definition>
			<definition id="3">
				<sentence>¬¢Z ( fJ1 ) Here , Z ( fJ1 ) denotes the appropriate normalization constant .</sentence>
				<definiendum id="0">¬¢Z</definiendum>
				<definiens id="0">the appropriate normalization constant</definiens>
			</definition>
			<definition id="4">
				<sentence>This is done via relative frequencies : p ( ÀúfjÀúe ) = N ( Àúf ; Àúe ) P Àúf0 N ( Àúf0 ; Àúe ) ( 8 ) Here , N ( Àúf ; Àúe ) denotes the count of the event that Àúf has been seen as a translation of Àúe .</sentence>
				<definiendum id="0">Àúe )</definiendum>
				<definiendum id="1">Àúe )</definiendum>
				<definiens id="0">the count of the event that Àúf has been seen as a translation of Àúe</definiens>
			</definition>
			<definition id="5">
				<sentence>The symbol N ( e ) denotes the unigram count of a word e and N ( f ; e ) denotes the count of the event that the target language word e is aligned to the source language word f. If one occurrence of e has N &gt; 1 aligned source words , each of them contributes with a count of 1=N. The formula for fi ( e ) is : fi ( e ) = 1N ( e ) 0 @ X f : N ( f ; e ) &gt; d d+ X f : N ( f ; e ) ‚Ä¢d N ( f ; e ) 1 A = 1N ( e ) X f minfd ; N ( f ; e ) g This formula is a generalization of the one typically used in publications on language modeling .</sentence>
				<definiendum id="0">symbol N ( e</definiendum>
				<definiendum id="1">N ( f ; e )</definiendum>
				<definiens id="0">the unigram count of a word e</definiens>
				<definiens id="1">the count of the event that the target language word e is aligned to the source language word f. If one occurrence of e has N &gt; 1 aligned source words</definiens>
			</definition>
			<definition id="6">
				<sentence>The first one is a uniform distribution and the second one is the unigram distribution : fl1 ( f ) = 1V f ( 14 ) fl2 ( f ) = N ( f ) P f0 N ( f0 ) ( 15 ) Here , Vf denotes the vocabulary size of the source language and N ( f ) denotes the unigram count of a source word f. The monotone search can be efficiently computed with dynamic programming .</sentence>
				<definiendum id="0">Vf</definiendum>
				<definiendum id="1">N ( f )</definiendum>
				<definiendum id="2">monotone search</definiendum>
				<definiens id="0">the vocabulary size of the source language</definiens>
			</definition>
			<definition id="7">
				<sentence>Q ( 0 ; $ ) = 1 Q ( j ; e ) = max e0 ; ~e ; j¬°M‚Ä¢j0 &lt; j n p ( fjj0+1jÀúe ) ¬¢p ( Àúeje0 ) ¬¢Q ( j0 ; e0 ) o Q ( J +1 ; $ ) = max e0 fQ ( J ; e0 ) ¬¢p ( $ je0 ) g Here , M denotes the maximum phrase length in the source language .</sentence>
				<definiendum id="0">M</definiendum>
			</definition>
			<definition id="8">
				<sentence>Here , Ve denotes the vocabulary size of the target language and E denotes the maximum number of phrase translation candidates for a source language phrase .</sentence>
				<definiendum id="0">Ve</definiendum>
				<definiens id="0">the vocabulary size of the target language and E denotes the maximum number of phrase translation candidates for a source language phrase</definiens>
			</definition>
			<definition id="9">
				<sentence>‚Ä† WER ( word error rate ) : The WER is computed as the minimum number of substitution , insertion and deletion operations that have to be performed to convert the generated sentence into the reference sentence .</sentence>
				<definiendum id="0">‚Ä† WER</definiendum>
				<definiendum id="1">WER</definiendum>
				<definiens id="0">the minimum number of substitution , insertion and deletion operations that have to be performed to convert the generated sentence into the reference sentence</definiens>
			</definition>
			<definition id="10">
				<sentence>The PER compares the words in the two sentences ignoring the word order .</sentence>
				<definiendum id="0">PER</definiendum>
				<definiens id="0">compares the words in the two sentences ignoring the word order</definiens>
			</definition>
			<definition id="11">
				<sentence>The alignment template system ( Och et al. , 1999 ) is similar to the system described in this work .</sentence>
				<definiendum id="0">alignment template system</definiendum>
				<definiens id="0">similar to the system described in this work</definiens>
			</definition>
			<definition id="12">
				<sentence>In addition to the word-based trigram model , the alignment template system uses a classbased fivegram language model .</sentence>
				<definiendum id="0">alignment template system</definiendum>
				<definiens id="0">uses a classbased fivegram language model</definiens>
			</definition>
</paper>

		<paper id="1040">
			<definition id="0">
				<sentence>Story link detection , as defined in the Topic Detection and Tracking ( TDT ) competition sponsored by the DARPA TIDES program , is the task of determining whether two stories , such as news articles and/or radio broadcasts , are about the same event , or linked .</sentence>
				<definiendum id="0">Story link detection</definiendum>
				<definiens id="0">defined in the Topic Detection and Tracking ( TDT ) competition sponsored by the DARPA TIDES program , is the task of determining whether two stories , such as news articles and/or radio broadcasts</definiens>
			</definition>
			<definition id="1">
				<sentence>UMass ( Allan et al. , 2000 ) has examined a number of similarity measures in the link detection task , including weighted sum , language modeling and KullbackLeibler divergence , and found that the cosine similarity produced the best results .</sentence>
				<definiendum id="0">UMass</definiendum>
			</definition>
			<definition id="2">
				<sentence>When the a52a48a6a53 set of test documents , a54a34a55 , is added to the model , the document term counts are updated as : a11 a55a4a6a0a8a7a56a9 a11 a55a25a57a59a58a4a6a0a8a7a61a60 a11a59a62a64a63 a4a51a0a8a7 where a11 a62 a63 a46a49a48a51a50 denotes the document count for term a0 in the newly added set of documentsa54a34a55 .</sentence>
				<definiendum id="0">a46a49a48a51a50</definiendum>
				<definiens id="0">When the a52a48a6a53 set of test documents , a54a34a55 , is added to the model , the document term counts are updated as : a11 a55a4a6a0a8a7a56a9 a11 a55a25a57a59a58a4a6a0a8a7a61a60 a11a59a62a64a63 a4a51a0a8a7 where a11 a62 a63</definiens>
				<definiens id="1">the document count for term a0 in the newly added set of documentsa54a34a55</definiens>
			</definition>
			<definition id="3">
				<sentence>The document frequencies , a1a69a2a5a4a51a0a8a7 , the number of documents containing term a0 , and document term frequencies , a2a5a4a76a1a78a77a79a0a8a7 , are used to calculate TF-IDF based weights a80 a4a6a0a29a77a79a1a47a7 for the terms in a document ( or story ) a1 : a80 a4a6a1a81a77a8a0a8a7a10a9 a82 a83 a4a76a1a84a7 a2a5a4a6a1a81a77a8a0a8a7a5a85a21a38a86a39a73a41 a13 a11 a4a51a0a8a7 ( 1 ) where a13 is the total number of documents and a83 a4a6a1a47a7 is a normalization value .</sentence>
				<definiendum id="0">a13</definiendum>
				<definiens id="0">the number of documents containing term a0 , and document term frequencies , a2a5a4a76a1a78a77a79a0a8a7 , are used to calculate TF-IDF based weights a80 a4a6a0a29a77a79a1a47a7 for the terms in a document ( or story ) a1 : a80 a4a6a1a81a77a8a0a8a7a10a9 a82 a83 a4a76a1a84a7 a2a5a4a6a1a81a77a8a0a8a7a5a85a21a38a86a39a73a41 a13 a11 a4a51a0a8a7 ( 1 ) where</definiens>
			</definition>
			<definition id="4">
				<sentence>We used a symmetric version that is computed as : a20a33a30 a26a99a4a6a1a58a96a77a79a1 a97 a7 a9 a1 a2a4a3 a4 a80 a4a51a0a29a77a79a1 a58a94a7a6a5a7a5 a80 a4a6a0a29a77a79a1 a97 a7a79a7 a60 a2a4a3 a4 a80 a4a51a0a29a77a36a1a47a58a33a7a8a5a9a5a10a12a11 a7 a1 a2a4a3 a4 a80 a4a51a0a29a77a36a1 a97 a7a8a5a9a5 a80 a4a51a0a29a77a79a1a58a7a8a7 a60 a2a4a3 a4 a80 a4a51a0a29a77a36a1 a97 a5a7a5a10a12a11 a7 where a10a12a11 is the probability distribution of words for ‚Äúgeneral English‚Äù as derived from the training corpus , and KL is the Kullback-Leibler divergence : a2a4a3 a4a49a35a13a5a9a5a14a122a7 a9 a116a16a15 a35a115a4a18a17a37a7a20a19 a15a22a21a24a23 a46 a15 a50 a25a29a46 a15 a50a27a26 In computing the clarity measure , the term frequencies were smoothed with the General English model using a weight of 0.01 .</sentence>
				<definiendum id="0">a10a12a11</definiendum>
				<definiendum id="1">KL</definiendum>
				<definiens id="0">the probability distribution of words for ‚Äúgeneral English‚Äù as derived from the training corpus , and</definiens>
				<definiens id="1">the Kullback-Leibler divergence : a2a4a3 a4a49a35a13a5a9a5a14a122a7 a9 a116a16a15 a35a115a4a18a17a37a7a20a19 a15a22a21a24a23 a46 a15 a50 a25a29a46 a15 a50a27a26 In computing the clarity measure , the term frequencies were smoothed with the General English model using a weight of 0.01</definiens>
			</definition>
			<definition id="5">
				<sentence>The modality pairs that we considered are : asr : asr , asr : text , and text : text , where asr represents ‚Äúautomatic speech recognition‚Äù .</sentence>
				<definiendum id="0">asr</definiendum>
				<definiens id="0">‚Äúautomatic speech recognition‚Äù</definiens>
			</definition>
			<definition id="6">
				<sentence>The TDT3 corpus contains 40,000 news articles and broadcast news stories with 120 labeled events in English , Mandarin and Arabic from October through December 1998 .</sentence>
				<definiendum id="0">TDT3 corpus</definiendum>
				<definiens id="0">contains 40,000 news articles and broadcast news stories with 120 labeled events in English</definiens>
			</definition>
			<definition id="7">
				<sentence>The TDT tasks are evaluated by computing a ‚Äúdetection cost‚Äù : a54a1a0a3a2 a48 a9a118a54a5a4 a46a7a6a8a6 a85 a9 a4 a46a10a6a11a6 a85 a9 a48a13a12a15a14a17a16 a2 a48 a60a71a54a1a18a20a19 a85 a9 a18a20a19a74a85 a9 a45a22a21a79a45 a57 a48a13a12a15a14a17a16 a2 a48 wherea54a5a4 a46a7a6a8a6 is the cost of a miss , a9 a4 a46a7a6a8a6 is the estimated probability of a miss , a9 a48a13a12a15a14a17a16 a2a48 is the prior probability that a pair of stories are linked , a54 a18a23a19 is the cost of a false alarm , a9 a18a23a19 is the estimated probability of a false alarm , and a9 a45a24a21a8a45a57 a48a13a12a15a14a17a16 a2a48 is the prior probability that a pair of stories are not linked .</sentence>
				<definiendum id="0">a54 a18a23a19</definiendum>
				<definiens id="0">computing a ‚Äúdetection cost‚Äù : a54a1a0a3a2 a48 a9a118a54a5a4 a46a7a6a8a6 a85 a9 a4 a46a10a6a11a6 a85 a9 a48a13a12a15a14a17a16 a2 a48 a60a71a54a1a18a20a19 a85 a9 a18a20a19a74a85 a9 a45a22a21a79a45 a57 a48a13a12a15a14a17a16 a2 a48 wherea54a5a4 a46a7a6a8a6 is the cost of a miss</definiens>
				<definiens id="1">the estimated probability of a miss</definiens>
				<definiens id="2">the prior probability that a pair of stories are linked</definiens>
			</definition>
			<definition id="8">
				<sentence>A target is a pair of linked stories ; conversely , a non-target is a pair of stories that are not linked .</sentence>
				<definiendum id="0">target</definiendum>
				<definiendum id="1">non-target</definiendum>
				<definiens id="0">a pair of linked stories</definiens>
			</definition>
			<definition id="9">
				<sentence>The SVMs used the source-pair statistics , plus categorical source-pair information as input features .</sentence>
				<definiendum id="0">SVMs</definiendum>
				<definiens id="0">used the source-pair statistics , plus categorical source-pair information as input features</definiens>
			</definition>
</paper>

		<paper id="4026">
</paper>

		<paper id="1017">
			<definition id="0">
				<sentence>The acoustic models consist of decision tree state clustered triphones and the output distributions are mixtures of Gaussians .</sentence>
				<definiendum id="0">acoustic models</definiendum>
			</definition>
			<definition id="1">
				<sentence>For a lattice L we construct a ‚Äúcount‚Äù C ( l|L ) for a given label l using the information stored in the index I ( l ) as follows , C ( l|L ) = summationdisplay pi‚ààL p ( pi ) C ( l|pi ) = summationdisplay pi‚ààL parenleftBig p ( pi ) summationdisplay a‚ààpi Œ¥ ( a , l ) parenrightBig = summationdisplay a‚ààL parenleftBig Œ¥ ( a , l ) summationdisplay pi‚ààL : a‚ààpi p ( pi ) parenrightBig = summationdisplay a‚ààI ( l ) : L [ a ] =L p ( a ) = summationdisplay a‚ààI ( l ) : L [ a ] =L f ( k [ a ] ) p ( a|k [ a ] ) where C ( l|pi ) is the number of times l is seen on path pi and Œ¥ ( a , l ) is 1 if arc a has the label l and 0 otherwise .</sentence>
				<definiendum id="0">] =L f</definiendum>
				<definiendum id="1">C ( l|pi )</definiendum>
				<definiens id="0">a given label l using the information stored in the index I ( l ) as follows</definiens>
				<definiens id="1">the number of times l is seen on path pi</definiens>
			</definition>
			<definition id="2">
				<sentence>The probability of each match is defined as f ( k [ a1 ] ) p ( a1|k [ a1 ] ) p ( a2|k [ a2 ] ) ... p ( an|k [ an ] ) , where p ( ai|k [ ai ] ) is the probability of the ith arc in the expression starting in arc a1 .</sentence>
				<definiendum id="0">probability of each match</definiendum>
				<definiendum id="1">] )</definiendum>
				<definiens id="0">f ( k [ a1 ] ) p ( a1|k [ a1 ] ) p ( a2|k [ a2 ] ) ... p ( an|k [ an ] ) , where p ( ai|k [ ai</definiens>
				<definiens id="1">the probability of the ith arc in the expression starting in arc a1</definiens>
			</definition>
			<definition id="3">
				<sentence>We also define the normalized lattice count for the phone index as Cnormp ( q ) = ( Cp ( q ) ) 1|pron ( q ) | where |pron ( q ) | is the length of the pronunciation of query q. We then define the combined score to be Cwp ( q ) = Cw ( q ) + ŒªCnormp ( q ) where Œª is an empirically determined scaling factor .</sentence>
				<definiendum id="0">Œª</definiendum>
				<definiens id="0">the length of the pronunciation of query q. We then define the combined score to be Cwp ( q ) = Cw ( q ) + ŒªCnormp ( q ) where</definiens>
				<definiens id="1">an empirically determined scaling factor</definiens>
			</definition>
			<definition id="4">
				<sentence>System Precision Recall maxF Word 1-best 98.3 29.7 45.6 Word lattices 96.1 46.1 62.3 Word+Phone lattices 94.9 49.8 65.4 Table 6 : Results for word pair queries on Switchboard 0 20 40 60 80 1000 20 40 60 80 100 Effect of Recognition Vocabulary Size Precision Recall word ( 45k ) word ( 20k ) word ( 5k ) word+phone ( 45k ) word+phone ( 20k ) word+phone ( 5k ) Figure 4 : Comparison of various recognition vocabulary sizes for Switchboard Finally , we make a comparison of various techniques on different tasks .</sentence>
				<definiendum id="0">Vocabulary Size Precision Recall word</definiendum>
				<definiens id="0">Results for word pair queries on Switchboard 0 20 40 60 80 1000 20 40 60 80 100 Effect of Recognition</definiens>
			</definition>
</paper>

		<paper id="4028">
			<definition id="0">
				<sentence>Here , a record is an entire block of a person‚Äôs contact information , and a field is one element of that record ( e.g. COMPANYNAME ) .</sentence>
				<definiendum id="0">record</definiendum>
			</definition>
			<definition id="1">
				<sentence>Conditional random fields ( Lafferty et al. , 2001 ) are undirected graphical models to calculate the conditional probability of values on designated output nodes given values on designated input nodes .</sentence>
				<definiendum id="0">Conditional random fields</definiendum>
				<definiens id="0">Lafferty et al. , 2001 ) are undirected graphical models to calculate the conditional probability of values on designated output nodes given values on designated input nodes</definiens>
			</definition>
			<definition id="2">
				<sentence>CRFs define the conditional probability of a state sequence given an input sequence as pŒõ ( s|o ) = 1Z o exp parenleftBigg Tsummationdisplay t=1 summationdisplay k Œªkfk ( st‚àí1 , st , o , t ) parenrightBigg , ( 1 ) where Zo is a normalization factor over all state sequences , fk ( st‚àí1 , st , o , t ) is an arbitrary feature function over its arguments , and Œªk is a learned weight for each feature function .</sentence>
				<definiendum id="0">CRFs</definiendum>
				<definiendum id="1">conditional probability</definiendum>
				<definiendum id="2">Zo</definiendum>
			</definition>
			<definition id="3">
				<sentence>Because CRFs are conditional models , Viterbi finds the most likely state sequence given an observation sequence , defined as s‚àó = argmaxs pŒõ ( s|o ) .</sentence>
				<definiendum id="0">Viterbi</definiendum>
			</definition>
			<definition id="4">
				<sentence>To avoid an exponential-time search over all possible settings of s , Viterbi stores the probability of the most likely path at time t that accounts for the first t observations and ends in state si .</sentence>
				<definiendum id="0">Viterbi</definiendum>
				<definiens id="0">stores the probability of the most likely path at time t that accounts for the first t observations and ends in state si</definiens>
			</definition>
			<definition id="5">
				<sentence>The calculations of the forward values can be made to conform to C by the recursion Œ±primeq ( si ) = braceleftBiggP sprime h Œ±primeq‚àí1 ( sprime ) exp ‚ÄúP k Œªkfk ( s prime , si , o , t ) ‚Äùi if si similarequal sq 0 otherwise for all sq ‚àà C , where the operator si similarequal sq means si conforms to constraint sq .</sentence>
				<definiendum id="0">recursion Œ±primeq</definiendum>
				<definiens id="0">s prime , si , o</definiens>
			</definition>
			<definition id="6">
				<sentence>The CRF achieves an overall token accuracy of 87.32 on the testing data , with a field-level performance of F1 = 84.11 , precision = 85.43 , and recall = 82.83 .</sentence>
				<definiendum id="0">CRF</definiendum>
				<definiens id="0">achieves an overall token accuracy of 87.32 on the testing data</definiens>
			</definition>
			<definition id="7">
				<sentence>WORSTCASE is the average precision obtained by ranking all incorrect instances above all correct instances .</sentence>
				<definiendum id="0">WORSTCASE</definiendum>
				<definiens id="0">the average precision obtained by ranking all incorrect instances above all correct instances</definiens>
			</definition>
			<definition id="8">
				<sentence>In all experiments , RANDOM assigns confidence values chosen uniformly at random between 0 and 1 .</sentence>
				<definiendum id="0">RANDOM</definiendum>
				<definiens id="0">assigns confidence values chosen uniformly at random between 0 and 1</definiens>
			</definition>
			<definition id="9">
				<sentence>Rule-based extraction methods ( Thompson et al. , 1999 ) estimate confidence based on a rule‚Äôs coverage in the training data .</sentence>
				<definiendum id="0">Rule-based extraction methods</definiendum>
				<definiens id="0">Thompson et al. , 1999 ) estimate confidence based on a rule‚Äôs coverage in the training data</definiens>
			</definition>
</paper>

		<paper id="3001">
			<definition id="0">
				<sentence>The Columbia Summarizer routes to one of two multidocument summarization systems based on the similarity of the documents in the cluster .</sentence>
				<definiendum id="0">Columbia Summarizer</definiendum>
				<definiens id="0">routes to one of two multidocument summarization systems based on the similarity of the documents in the cluster</definiens>
			</definition>
</paper>

		<paper id="4023">
			<definition id="0">
				<sentence>In many cases , previous story segmentation research has focused on single stream analysis techniques , utilizing only one of the information sources present in news broadcasts : natural language , audio , image , and video ( see , for example , ( Furht et al. , 1995 ) , ( Fiscus and Doddington , 2002 ) , ( Greiff et al. , 2001 ) , ( O‚ÄôConnor et al. , 2001 ) ) .</sentence>
				<definiendum id="0">video</definiendum>
				<definiens id="0">utilizing only one of the information sources present in news broadcasts : natural language , audio , image</definiens>
			</definition>
			<definition id="1">
				<sentence>The data set contains roughly equal amounts ( 8-12 hours ) of news broadcasts from seven sources in three languages : Aljazeera ( Arabic ) , BBC America ( UK English ) , China Central TV ( Mandarin Chinese ) , CNN Headline News ( US English ) , CNN International ( US/UK English ) , Fox News ( US English ) , and Newsworld International ( US/UK English ) .</sentence>
				<definiendum id="0">BBC America</definiendum>
				<definiendum id="1">Newsworld International</definiendum>
				<definiens id="0">8-12 hours ) of news broadcasts from seven sources in three languages : Aljazeera ( Arabic ) ,</definiens>
			</definition>
			<definition id="2">
				<sentence>Closed captioning is a human-generated transcript of the spoken words that is often embedded in a broadcast video signal .</sentence>
				<definiendum id="0">Closed captioning</definiendum>
				<definiens id="0">a human-generated transcript of the spoken words that is often embedded in a broadcast video signal</definiens>
			</definition>
</paper>

		<paper id="4018">
			<definition id="0">
				<sentence>For each hypothesized word sequence , an HMM is used to estimate the posterior probability of a sentence boundary at each word boundary .</sentence>
				<definiendum id="0">HMM</definiendum>
				<definiens id="0">used to estimate the posterior probability of a sentence boundary at each word boundary</definiens>
			</definition>
			<definition id="1">
				<sentence>Confusion networks are a compacted representation of word lattices that have strictly ordered word hypothesis slots ( Mangu et al. , 2000 ) .</sentence>
				<definiendum id="0">Confusion networks</definiendum>
			</definition>
			<definition id="2">
				<sentence>The BN data consists of a set of 20 hours of news shows for training , and 3 hours ( 6 shows ) for testing .</sentence>
				<definiendum id="0">BN data</definiendum>
				<definiens id="0">consists of a set of 20 hours of news shows for training , and 3 hours ( 6 shows ) for testing</definiens>
			</definition>
			<definition id="3">
				<sentence>Test data comes from the month of February in 2001 ; training data is taken from a previous time period .</sentence>
				<definiendum id="0">Test data</definiendum>
				<definiens id="0">comes from the month of February in 2001 ; training data is taken from a previous time period</definiens>
			</definition>
			<definition id="4">
				<sentence>The prosody model is a decision tree classifier that generates the posterior probability of an SU boundary at each interword boundary given the prosodic features .</sentence>
				<definiendum id="0">prosody model</definiendum>
				<definiens id="0">a decision tree classifier that generates the posterior probability of an SU boundary at each interword boundary given the prosodic features</definiens>
			</definition>
</paper>

		<paper id="3011">
			<definition id="0">
				<sentence>‚àó MiPad is a Web based PIM application that facilitates multimodal access to personal email , calendar , and contact information .</sentence>
				<definiendum id="0">MiPad</definiendum>
				<definiens id="0">a Web based PIM application that facilitates multimodal access to personal email , calendar , and contact information</definiens>
			</definition>
			<definition id="1">
				<sentence>SLU utilizes SLM to infer user‚Äôs intention from speech .</sentence>
				<definiendum id="0">SLU</definiendum>
				<definiens id="0">utilizes SLM to infer user‚Äôs intention from speech</definiens>
			</definition>
</paper>

		<paper id="1023">
			<definition id="0">
				<sentence>This sourcechannel model treated the task of finding the probability a1a3a2a5a4a7a6a9a8a11a10 , where a4 is the translation in the target ( English ) language for a given source ( foreign ) sentence a8 , as two generative probability models : the language model a1a3a2a5a4a12a10 which is a generative probability over candidate translations and the translation model a1a3a2a5a8a13a6a14a4a12a10 which is a generative conditional probability of the source sentence given a candidate translation a4 .</sentence>
				<definiendum id="0">a4</definiendum>
			</definition>
			<definition id="1">
				<sentence>Then the set of training samples is : a68 a36a39a69a70a2 a58 a17a16a59a61a9a41 a66 a17a60a59a61a11a10a71a6a12a38 a34 a19 a34a73a72 a41a74a38 a34 a62 a34 a0a76a75a52a41 where a72 is the number of clusters and a0 is the length of ranks for each cluster .</sentence>
				<definiendum id="0">a72</definiendum>
				<definiendum id="1">a0</definiendum>
				<definiens id="0">the number of clusters</definiens>
				<definiens id="1">the length of ranks for each cluster</definiens>
			</definition>
			<definition id="2">
				<sentence>Let a77 a2 a58 a10a78a36 a57a80a79a82a81a24a58 be a linear function , where a58 is the feature vector of a translation , and a57a80a79 is a weight vector .</sentence>
				<definiendum id="0">a58</definiendum>
				<definiendum id="1">a57a80a79</definiendum>
				<definiens id="0">the feature vector of a translation</definiens>
				<definiens id="1">a weight vector</definiens>
			</definition>
			<definition id="3">
				<sentence>a83 a79 a2 a58 a20 a41a93a92a94a92a95a92 a58 a26 a10a96a36 a30a14a97 a0 a31 a2 a77 a2 a58 a20 a10a49a41a24a92a95a92a95a92a94a41 a77 a2 a58 a26 a10a53a10a49a41 where a30a14a97 a0 a31 is a function that takes a list of scores for the candidate translations computed according to the evaluation metric and returns the rank in that list .</sentence>
				<definiendum id="0">a30a14a97 a0 a31</definiendum>
				<definiens id="0">a function that takes a list of scores for the candidate translations computed according to the evaluation metric and returns the rank in that list</definiens>
			</definition>
			<definition id="4">
				<sentence>Now the complexity of a repeat iteration is a160 a2 a72 a0 a22 a32 a72 a0a139a161a70a10 , where a161 is the average number of active features in vector a58 a17a60a59a61 .</sentence>
				<definiendum id="0">a161</definiendum>
			</definition>
			<definition id="5">
				<sentence>The a165 -axis represents the number of iterations in the training .</sentence>
				<definiendum id="0">a165 -axis</definiendum>
				<definiens id="0">represents the number of iterations in the training</definiens>
			</definition>
</paper>

		<paper id="1013">
			<definition id="0">
				<sentence>The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory .</sentence>
				<definiendum id="0">XLE parser</definiendum>
			</definition>
			<definition id="1">
				<sentence>XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear ( a.k.a. maximum-entropy ) probability model ( Riezler et al. , 2002 ) .</sentence>
				<definiendum id="0">XLE</definiendum>
			</definition>
			<definition id="2">
				<sentence>XLE incorporates a sophisticated finite-state morphology and dictionary lookup component , and its time is part of the measure of XLE performance .</sentence>
				<definiendum id="0">XLE</definiendum>
				<definiens id="0">incorporates a sophisticated finite-state morphology and dictionary lookup component , and its time is part of the measure of XLE performance</definiens>
			</definition>
			<definition id="3">
				<sentence>The general form of conditional exponential models is as follows : pŒª ( x|y ) = ZŒª ( y ) ‚àí1eŒª¬∑f ( x ) where ZŒª ( y ) = summationtextx‚ààX ( y ) eŒª¬∑f ( x ) is a normalizing constant over the set X ( y ) of parses for sentence y , Œª is a vector of log-parameters , f is a vector of featurevalues , and Œª ¬∑ f ( x ) is a vector dot product denoting the ( log- ) weight of parse x. Dynamic-programming algorithms that allow the efficient estimation and searching of log-linear models from a packed parse representation without enumerating an exponential number of parses have been recently presented by Miyao and Tsujii ( 2002 ) and Geman and Johnson ( 2002 ) .</sentence>
				<definiendum id="0">f</definiendum>
				<definiens id="0">pŒª ( x|y ) = ZŒª ( y ) ‚àí1eŒª¬∑f ( x ) where ZŒª ( y ) = summationtextx‚ààX ( y ) eŒª¬∑f ( x ) is a normalizing constant over the set X ( y ) of parses for sentence y , Œª is a vector of log-parameters</definiens>
				<definiens id="1">a vector of featurevalues , and Œª ¬∑ f ( x ) is a vector dot product denoting the ( log- ) weight of parse x. Dynamic-programming algorithms that allow the efficient estimation and searching of log-linear models from a packed parse representation without enumerating an exponential number of parses</definiens>
			</definition>
			<definition id="4">
				<sentence>In the notation of Miyao and Tsujii ( 2002 ) , such a feature forest Œ¶ is defined as a tuple „ÄàC , D , r , Œ≥ , Œ¥„Äâ where C is a set of conjunctive nodes , D is a set of disjunctive nodes , r ‚àà C is the root node , Œ≥ : D ‚Üí 2C is a conjunctive daughter function , and Œ¥ : C ‚Üí 2D is a disjunctive daughter function .</sentence>
				<definiendum id="0">D</definiendum>
				<definiendum id="1">C</definiendum>
				<definiendum id="2">Œ¥</definiendum>
				<definiens id="0">a tuple „ÄàC , D , r , Œ≥ , Œ¥„Äâ where C is a set of conjunctive nodes</definiens>
				<definiens id="1">a set of disjunctive nodes , r ‚àà</definiens>
				<definiens id="2">the root node , Œ≥</definiens>
				<definiens id="3">a conjunctive daughter function , and</definiens>
			</definition>
			<definition id="5">
				<sentence>The outside weight œàc of a conjunctive node is defined as the outside weight of its disjunctive mother node ( s ) : œàc = summationdisplay { d|c‚ààŒ≥ ( d ) } œàd ( 3 ) The outside weight of a disjunctive node is the sum of the product of the outside weight ( s ) of its conjunctive mother ( s ) , the weight ( s ) of its mother ( s ) , and the inside weight ( s ) of its disjunctive sister ( s ) : œàd = summationdisplay { c|d‚ààŒ¥ ( c ) } { œàc eŒª¬∑f ( c ) productdisplay { dprime|dprime‚ààŒ¥ ( c ) , dprimenegationslash=d } œÜdprime } ( 4 ) From these formulae , the conditional expectation of a feature-function fi can be computed from a chart with root node r for a sentence y in the following way : summationdisplay x‚ààX ( y ) eŒª¬∑f ( x ) fi ( x ) ZŒª ( y ) = summationdisplay c‚ààC œÜcœàcfi ( c ) œÜr ( 5 ) Formula 5 is used in our system to compute expectations for discriminative Bayesian estimation from partially labeled data using a first-order conjugate-gradient routine .</sentence>
				<definiendum id="0">outside weight œàc of a conjunctive node</definiendum>
				<definiens id="0">the outside weight of its disjunctive mother node ( s ) : œàc = summationdisplay { d|c‚ààŒ≥ ( d ) } œàd ( 3 ) The outside weight of a disjunctive node is the sum of the product of the outside weight ( s ) of its conjunctive mother ( s ) , the weight ( s ) of its mother ( s ) , and the inside weight ( s ) of its disjunctive sister ( s ) : œàd = summationdisplay { c|d‚ààŒ¥ ( c ) } { œàc eŒª¬∑f ( c ) productdisplay { dprime|dprime‚ààŒ¥ ( c ) , dprimenegationslash=d } œÜdprime } ( 4 ) From these formulae , the conditional expectation of a feature-function fi can be computed from a chart with root node r for a sentence y in the following way : summationdisplay x‚ààX ( y ) eŒª¬∑f ( x ) fi ( x ) ZŒª ( y ) = summationdisplay c‚ààC œÜcœàcfi ( c ) œÜr ( 5 ) Formula 5 is used in our system to compute expectations for discriminative Bayesian estimation from partially labeled data using a first-order conjugate-gradient routine</definiens>
			</definition>
			<definition id="6">
				<sentence>The DEPBANK consists of dependency annotations for 700 sentences that were randomly extracted from section 23 of the UPenn Wall Street Journal ( WSJ ) treebank .</sentence>
				<definiendum id="0">DEPBANK</definiendum>
			</definition>
			<definition id="7">
				<sentence>parc.com/istl/groups/nltt/fsbank/ Function Meaning adjunct adjuncts aquant adjectival quantifiers ( many , etc. ) comp complement clauses ( that , whether ) conj conjuncts in coordinate structures focus int fronted element in interrogatives mod noun-noun modifiers number numbers modifying nouns obj objects obj theta secondary objects obl oblique obl ag demoted subject of a passive obl compar comparative than/as clauses poss possessives ( John‚Äôs book ) pron int interrogative pronouns pron rel relative pronouns quant quantifiers ( all , etc. ) subj subjects topic rel fronted element in relative clauses xcomp non-finite complements verbal and small clauses Figure 1 : Grammatical functions in DEPBANK .</sentence>
				<definiendum id="0">pron int interrogative pronouns pron rel relative pronouns quant quantifiers</definiendum>
				<definiens id="0">that , whether ) conj conjuncts in coordinate structures focus int fronted element in interrogatives mod noun-noun modifiers number numbers modifying nouns obj objects obj theta secondary objects obl oblique obl ag demoted subject of a passive obl compar comparative than/as clauses poss possessives ( John‚Äôs book</definiens>
			</definition>
			<definition id="8">
				<sentence>Feature Meaning adegree degree of adjectives and adverbs ( positive , comparative , superlative ) coord form form of a coordinating conjunction ( e.g. , and , or ) det form form of a determiner ( e.g. , the , a ) num number of nouns ( sg , pl ) number type cardinals vs. ordinals passive passive verb ( e.g. , It was eaten . )</sentence>
				<definiendum id="0">pl</definiendum>
				<definiens id="0">positive , comparative , superlative ) coord form form of a coordinating conjunction ( e.g. , and , or ) det form form of a determiner ( e.g. , the , a ) num number of nouns ( sg ,</definiens>
			</definition>
</paper>

		<paper id="2002">
			<definition id="0">
				<sentence>sp ( c ) p ( s ) ... sp ( c|s ( 2 ) Now notice that for most applications we would like to be able to vary precision/recall tradeoff by setting some threshold t and classifying each string s as a chemical only if t ) p ( s/|c ) p ( sp ( c ) scp n 1i i n 1i i &gt; = ‚àè‚àè == ) | ( ( 3 ) or '/ tp ( c ) t ) p ( s/|c ) p ( s n 1i i n 1i i = &gt; ‚àè‚àè == ( 4 ) This allows us to avoid estimation of p ( c ) ( estimating p ( c ) is hard without any labeled text ) .</sentence>
				<definiendum id="0">sp ( c ) p</definiendum>
				<definiens id="0">hard without any labeled text )</definiens>
			</definition>
			<definition id="1">
				<sentence>Distribution obtained by n ( s ) such trials can be approximated with the normal distribution with mean n ( s ) p ( c ) and variance n ( s ) p ( c ) ( 1-p ( c ) ) .</sentence>
				<definiendum id="0">Distribution</definiendum>
				<definiens id="0">obtained by n ( s ) such trials can be approximated with the normal distribution with mean n ( s ) p ( c ) and variance n ( s ) p ( c ) ( 1-p ( c ) )</definiens>
			</definition>
			<definition id="2">
				<sentence>( ) ... | ( ) , ... | ( ) ( / ) ( ) | ( ) | ( 01 01 cpsssp csssp SpcpcSpScp i ii i ii ‚àè ‚àè ‚àí ‚àí = = ( 8 ) where S is the string to be classified and si are the letters of S. The N-gram approach has been a successful modeling technique in many other applications .</sentence>
				<definiendum id="0">S</definiendum>
				<definiens id="0">the string to be classified and si are the letters of S. The N-gram approach has been a successful modeling technique in many other applications</definiens>
			</definition>
			<definition id="3">
				<sentence>Bn nsssp Bn ncsssp i Ni i Ni ii i Nic i Nic ii d d d d + +‚âà + +‚âà ‚àí +‚àí +‚àí ‚àí ‚àí +‚àí +‚àí ‚àí 1 1 1 01 1 1 1 01 ) ... | ( ) , ... | ( ( 9 ) where N is the length of the N-grams , nii-N+1 and ncii-N+1 are the number of occurrences of N-gram sisi-1 ... si-N-1 in MEDLINE and chemical list respectively , d is the smoothing parameter , and B is the number of different N-grams of length N. The smoothing parameter was tuned for each n individually using the development data ( hand annotated MEDLINE abstracts ) .</sentence>
				<definiendum id="0">N</definiendum>
				<definiendum id="1">B</definiendum>
				<definiens id="0">the length of the N-grams</definiens>
				<definiens id="1">the smoothing parameter , and</definiens>
			</definition>
</paper>

	</volume>
